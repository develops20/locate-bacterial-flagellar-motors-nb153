{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/byu-2-locate-bacterial-flagellar-motors-nb153?scriptVersionId=240994000\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install required packages\n!pip install monai scipy scikit-image wandb imageio gcsfs\n\n# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\nimport torch.optim as optim\nfrom scipy.ndimage import gaussian_filter, center_of_mass\nfrom scipy.signal import find_peaks\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport glob\nfrom IPython.display import Video, display\nimport wandb\nimport time\nimport shutil\nimport gcsfs  # Dataset is too big for kaggle - After multiple attempts of uploading, I failed miserably. \nimport gc\nimport threading\nimport logging\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport queue\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:23:57.195396Z","iopub.execute_input":"2025-05-21T09:23:57.195864Z","iopub.status.idle":"2025-05-21T09:26:03.84222Z","shell.execute_reply.started":"2025-05-21T09:23:57.195842Z","shell.execute_reply":"2025-05-21T09:26:03.841597Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\nRequirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.2)\nRequirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.18)\nRequirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\nRequirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.2)\nRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.40.1)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.20.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.18.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\nCollecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs)\n  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.70.0)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.24->monai) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, google-api-core, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 1.34.1\n    Uninstalling google-api-core-1.34.1:\n      Successfully uninstalled google-api-core-1.34.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.24.2 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-core-2.24.2 monai-1.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-05-21 09:25:49.964487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747819550.156527      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747819550.215238      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Ensure the output directory exists\nos.makedirs('/kaggle/working', exist_ok=True)\n\n# Clear any existing handlers to avoid conflicts\nlogging.getLogger().handlers = []\nlogger = logging.getLogger('training') #Root logger \nlogger.setLevel(logging.WARNING)\n# Create and configure FileHandler\nfile_handler = logging.FileHandler(\"/kaggle/working/training.log\", mode='a')  # Append mode\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n# Add handler to logger\nlogger.addHandler(file_handler)\n\n# Test logging\nlogger.info(\"Logging initialized successfully\")\n\n# Suppress console logging\nfor handler in logging.getLogger().handlers:\n    if isinstance(handler, logging.StreamHandler):\n        logging.getLogger().removeHandler(handler)\n\n# Suppress external library logs\nlogging.getLogger('gcsfs').setLevel(logging.ERROR)\nlogging.getLogger('torch').setLevel(logging.ERROR)\nlogging.getLogger('monai').setLevel(logging.ERROR)\nlogging.getLogger('numpy').setLevel(logging.ERROR)\n\n# Conditional INFO logging\ndef log_info(message, force=False):\n    if force or 'first_epoch' in globals() and first_epoch:\n        logger.info(message)\n\n# Force flush to ensure logs are written\nfile_handler.flush()\nfile_handler.stream.flush()\n\n# Brief delay to ensure file write completes\ntime.sleep(0.1)\n\n# Verify file contents\ntry:\n    with open(\"/kaggle/working/training.log\", \"r\") as f:\n        contents = f.read()\n        print(f\"Initial log file contents: {contents}\", flush=True)\nexcept FileNotFoundError:\n    print(\"training.log not found\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:27:10.162347Z","iopub.execute_input":"2025-05-21T09:27:10.162805Z","iopub.status.idle":"2025-05-21T09:27:10.27334Z","shell.execute_reply.started":"2025-05-21T09:27:10.162786Z","shell.execute_reply":"2025-05-21T09:27:10.272551Z"}},"outputs":[{"name":"stdout","text":"Initial log file contents: \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Initialize wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_token = user_secrets.get_secret(\"WANDB\")\nwandb.login(key=wb_token)\nwandb.init(\n    project=\"byu-bacterial-flagellar-motors\",\n    config={\n        \"learning_rate\": 1e-3,\n        \"epochs\": 50,\n        \"batch_size\": 4,\n        \"patch_size\": (128, 128, 128),\n        \"gaussian_sigma\": 5,\n        \"architecture\": \"3D U-Net\",\n        \"optimizer\": \"Adam\",\n        \"loss_function\": \"DiceLoss\",\n        \"beta\": 2\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fs = gcsfs.GCSFileSystem(token=\"anon\") # Initialize GCS filesystem\n\n# Define GCS path and local directory\ngcs_precomputed_path = \"gs://nb153/precomputedmasks\"\ngcs_preprocessed_path = \"gs://nb153/preprocessed\"\nlocal_dir = \"/kaggle/working/data\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:27:19.306709Z","iopub.execute_input":"2025-05-21T09:27:19.306985Z","iopub.status.idle":"2025-05-21T09:27:19.312931Z","shell.execute_reply.started":"2025-05-21T09:27:19.306964Z","shell.execute_reply":"2025-05-21T09:27:19.312234Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 1. Analyze dataset to identify tomograms with motors\ndef identify_motor_tomograms(labels_df):\n    \"\"\"Identify tomograms with valid motor annotations.\"\"\"\n    motor_tomograms = []\n    for tomo_id in labels_df[\"tomo_id\"].unique():\n        tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n        if tomo_labels[\"Number of motors\"] > 0 and tomo_labels[\"Motor axis 0\"] != -1:\n            motor_tomograms.append(tomo_id)\n    logger.info(f\"Found {len(motor_tomograms)} tomograms with motors\")\n    return motor_tomograms\n\n\n# Load labels and identify motor tomograms\nlabels_df = pd.read_csv(\"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\")\ntomo_ids = sorted(labels_df[\"tomo_id\"].unique())\nmotor_tomo_ids = identify_motor_tomograms(labels_df)  # 2. Store tomogram IDs with motors\nlogger.info(f\"Total tomograms: {len(tomo_ids)}, Motor tomograms: {len(motor_tomo_ids)}\")\n\n# Split into train/val/test (80/10/10) using only motor tomograms for training\ntrain_val_ids, test_ids = train_test_split(tomo_ids, test_size=0.1, random_state=42)\ntrain_ids, val_ids = train_test_split(train_val_ids, test_size=0.1111, random_state=42)\ntrain_ids = [tid for tid in train_ids if tid in motor_tomo_ids]  # 3. Use only motor tomograms for training\nlogger.info(f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}\")\n\n# Analyze tomograms and print motor statistics\ndef analyze_tomograms(labels_df):\n    total_tomograms = len(labels_df[\"tomo_id\"].unique())\n    motor_tomograms = len(labels_df[labels_df[\"Number of motors\"] > 0])\n    non_motor_tomograms = total_tomograms - motor_tomograms\n    print(f\"Total tomograms: {total_tomograms}\", flush=True)\n    print(f\"Tomograms with motors: {motor_tomograms}\", flush=True)\n    print(f\"Tomograms without motors: {non_motor_tomograms}\", flush=True)\n    logger.info(f\"Total tomograms: {total_tomograms}\")\n    logger.info(f\"Tomograms with motors: {motor_tomograms}\")\n    logger.info(f\"Tomograms without motors: {non_motor_tomograms}\")\n    return total_tomograms, motor_tomograms, non_motor_tomograms\n\n# Plot tomogram distribution\ndef plot_tomogram_distribution(labels_df):\n    total_tomograms = len(labels_df[\"tomo_id\"].unique())\n    motor_tomograms = len(labels_df[labels_df[\"Number of motors\"] > 0])\n    non_motor_tomograms = total_tomograms - motor_tomograms\n    plt.figure(figsize=(8, 5))\n    plt.bar([\"Total\", \"With Motors\", \"Without Motors\"], [total_tomograms, motor_tomograms, non_motor_tomograms], color=[\"blue\", \"green\", \"red\"])\n    plt.title(\"Tomogram Distribution\")\n    plt.ylabel(\"Count\")\n    for i, count in enumerate([total_tomograms, motor_tomograms, non_motor_tomograms]):\n        plt.text(i, count + 0.5, str(count), ha=\"center\")\n    plt.show()\n    plt.close()\n\n# Download functions with simultaneous downloading\ndef download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, split, local_dir):\n    \"\"\"Download tomogram and mask simultaneously using threads.\"\"\"\n    def download_npy():\n        gcs_file_path = f\"{gcs_preprocessed_path}/{split}/{tomo_id}/{tomo_id}.npy\"\n        local_file_path = os.path.join(local_dir, f\"{tomo_id}.npy\")\n        if not os.path.exists(local_file_path):\n            logger.info(f\"Downloading {gcs_file_path} to {local_file_path}\")\n            fs.get(gcs_file_path, local_file_path)\n            logger.info(f\"✅ Download complete: {tomo_id}\")\n\n    def download_mask():\n        gcs_file_path = f\"{gcs_precomputed_path}/{split}/{tomo_id}_mask.npy\"\n        local_file_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n        if not os.path.exists(local_file_path):\n            logger.info(f\"Downloading {gcs_file_path} to {local_file_path}\")\n            fs.get(gcs_file_path, local_file_path)\n            logger.info(f\"✅ Download complete: {tomo_id}_mask.npy\")\n\n    # Run downloads in parallel\n    t1 = threading.Thread(target=download_npy)\n    t2 = threading.Thread(target=download_mask)\n    t1.start(); t2.start()\n    t1.join(); t2.join()\n    logger.info(f\"Completed downloading tomogram and mask for {tomo_id}\")\n    return os.path.join(local_dir, f\"{tomo_id}.npy\"), os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n\n\n# Combined download and preprocess function\ndef download_and_preprocess(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, split, local_dir, labels_df, patches_per_volume):\n    try:\n        tomo_path, mask_path = download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, split, local_dir)\n        logger.warning(f\"Completed parallel download for tomogram {tomo_id}\")\n        volume = np.load(tomo_path)\n        mask = np.load(mask_path)\n        tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n        num_motors = tomo_labels[\"Number of motors\"]\n        log_info(f\"Tomogram {tomo_id} has {num_motors} motors\")\n        patches, mask_patches = sample_patches(tomo_id, volume, mask, labels_df, patches_per_volume=patches_per_volume)\n        del volume, mask\n        logger.warning(f\"Prepared patches for tomogram {tomo_id}\")\n        return tomo_id, patches, mask_patches\n    except Exception as e:\n        logger.error(f\"Error processing tomogram {tomo_id}: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:49:28.163468Z","iopub.execute_input":"2025-05-21T09:49:28.163775Z","iopub.status.idle":"2025-05-21T09:49:28.421189Z","shell.execute_reply.started":"2025-05-21T09:49:28.163754Z","shell.execute_reply":"2025-05-21T09:49:28.420642Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Sample patches\ndef sample_patches(tomo_id, volume, mask, labels_df, patch_size=(128, 128, 128), patches_per_volume=32):\n    shape = volume.shape\n    patches = []\n    mask_patches = []\n    tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id]\n    motor_coords = []\n    for _, row in tomo_labels.iterrows():\n        if row[\"Number of motors\"] > 0 and row[\"Motor axis 0\"] != -1:\n            z, y, x = int(row[\"Motor axis 0\"]), int(row[\"Motor axis 1\"]), int(row[\"Motor axis 2\"])\n            if 0 <= z < shape[0] and 0 <= y < shape[1] and 0 <= x < shape[2]:\n                motor_coords.append((z, y, x))\n    \n    for _ in range(patches_per_volume // 2):\n        if motor_coords:\n            zc, yc, xc = motor_coords[np.random.randint(len(motor_coords))]\n            z = np.clip(zc - patch_size[0]//2 + np.random.randint(-32, 32), 0, shape[0] - patch_size[0])\n            y = np.clip(yc - patch_size[1]//2 + np.random.randint(-32, 32), 0, shape[1] - patch_size[1])\n            x = np.clip(xc - patch_size[2]//2 + np.random.randint(-32, 32), 0, shape[2] - patch_size[2])\n        else:\n            z = np.random.randint(0, max(1, shape[0] - patch_size[0]))\n            y = np.random.randint(0, max(1, shape[1] - patch_size[1]))\n            x = np.random.randint(0, max(1, shape[2] - patch_size[2]))\n        patch = volume[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        mask_patch = mask[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        patches.append(patch)\n        mask_patches.append(mask_patch)\n    \n    for _ in range(patches_per_volume // 2):\n        z = np.random.randint(0, max(1, shape[0] - patch_size[0]))\n        y = np.random.randint(0, max(1, shape[1] - patch_size[1]))\n        x = np.random.randint(0, max(1, shape[2] - patch_size[2]))\n        patch = volume[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        mask_patch = mask[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        patches.append(patch)\n        mask_patches.append(mask_patch)\n    \n    return np.array(patches), np.array(mask_patches)\n\n# Patch dataset\nclass PatchDataset(Dataset):\n    def __init__(self, patches, mask_patches):\n        self.patches = patches\n        self.mask_patches = mask_patches\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, idx):\n        patch = self.patches[idx]\n        mask_patch = self.mask_patches[idx]\n        return torch.tensor(patch, dtype=torch.float32), torch.tensor(mask_patch, dtype=torch.float32)\n\n# Tomogram dataset\nclass TomogramDataset(Dataset):\n    def __init__(self, tomo_id, gcs_preprocessed_path, local_dir, mode=\"test\"):\n        self.tomo_id = tomo_id\n        self.gcs_preprocessed_path = gcs_preprocessed_path\n        self.gcs_precomputed_path = gcs_preprocessed_path.replace(\"preprocessed\", \"precomputedmasks\")\n        self.local_dir = local_dir\n        self.mode = mode\n        self.volume = None\n    \n    def load(self):\n        tomo_path, mask_path = download_npy_and_mask(\n            self.tomo_id, \n            self.gcs_preprocessed_path, \n            self.gcs_precomputed_path, \n            \"train\" if self.mode == \"val\" else self.mode, \n            self.local_dir\n        )\n        self.volume = np.load(tomo_path)\n        if os.path.exists(mask_path):\n            self.mask = np.load(mask_path)\n        print(f\"Loaded tomogram {self.tomo_id} (shape: {self.volume.shape})\", flush=True)\n        logger.info(f\"Loaded tomogram {self.tomo_id} (shape: {self.volume.shape})\")\n        if self.mask is not None:\n            print(f\"Loaded mask {self.tomo_id} (min/max: {self.mask.min()}/{self.mask.max()})\", flush=True)\n            logger.info(f\"Loaded mask {self.tomo_id} (min/max: {self.mask.min()}/{self.mask.max()})\")\n    \n    def clear(self):\n        if self.volume is not None:\n            del self.volume\n            self.volume = None\n            tomo_path = os.path.join(local_dir, f\"{self.tomo_id}.npy\")\n            if os.path.exists(tomo_path):\n                os.remove(tomo_path)\n            gc.collect()\n    \n    def __len__(self):\n        return 1\n    \n    def __getitem__(self, idx):\n        if self.volume is None:\n            self.load()\n        return torch.from_numpy(self.volume).float(), torch.from_numpy(self.mask).float() if self.mask is not None else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:49:40.01885Z","iopub.execute_input":"2025-05-21T09:49:40.01915Z","iopub.status.idle":"2025-05-21T09:49:40.035582Z","shell.execute_reply.started":"2025-05-21T09:49:40.019127Z","shell.execute_reply":"2025-05-21T09:49:40.035021Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize model\nmodel = UNet(\n    spatial_dims=3,\n    in_channels=1,\n    out_channels=1,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n).to(device)\nlogger.info(f\"Model device: {next(model.parameters()).device}\")\n\n# Loss and optimizer\ncriterion = DiceLoss(sigmoid=True)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5)\n\n# Load checkpoint if available\nstart_epoch = 0\nbest_val_loss = float(\"inf\")\ntrain_losses = []\nval_losses = []\nif os.path.exists(\"checkpoint.pth\"):\n    checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    best_val_loss = checkpoint['best_val_loss']\n    start_epoch = checkpoint['epoch'] + 1\n    logger.info(f\"Resumed from epoch {start_epoch}\")\nelse:\n    logger.info(\"No checkpoint found, starting from scratch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:49:46.932321Z","iopub.execute_input":"2025-05-21T09:49:46.932904Z","iopub.status.idle":"2025-05-21T09:49:47.004223Z","shell.execute_reply.started":"2025-05-21T09:49:46.932881Z","shell.execute_reply":"2025-05-21T09:49:47.00362Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# TRAINING FUNCTION LOOP \n","metadata":{}},{"cell_type":"code","source":"# Training function\n# Training function with print-based logging\ndef train_epoch(model, loader, criterion, optimizer, epoch, start_epoch, tomo_id):\n    model.train()\n    epoch_loss = 0.0\n    start = time.time()\n    for i, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Training tomo {tomo_id}\", file=sys.stdout, disable=True)):\n        batch_load_time = time.time() - start\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        # Print batch progress to console\n        print(f\"Epoch {epoch+1}, Tomo {tomo_id}, Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}\", flush=True)\n        # Detailed logs for first 2 batches of first epoch (to console and file)\n        if epoch == start_epoch and i < 2:\n            # print(f\"Tomo {tomo_id}, Batch {i} load time: {batch_load_time:.2f}s\", flush=True)\n            # print(f\"Inputs shape: {inputs.shape}, min/max: {inputs.min().item():.4f}/{inputs.max().item():.4f}\", flush=True)\n            # print(f\"Targets shape: {targets.shape}, min/max: {targets.min().item():.4f}/{targets.max().item():.4f}\", flush=True)\n            # print(f\"Outputs shape: {outputs.shape}, min/max: {outputs.min().item():.4f}/{outputs.max().item():.4f}\", flush=True)\n            # print(f\"Loss: {loss.item():.4f}\", flush=True)\n            # print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\", flush=True)\n            logger.info(f\"Tomo {tomo_id}, Batch {i} load time: {batch_load_time:.2f}s\")\n            logger.info(f\"Inputs shape: {inputs.shape}, min/max: {inputs.min().item():.4f}/{inputs.max().item():.4f}\")\n            logger.info(f\"Targets shape: {targets.shape}, min/max: {targets.min().item():.4f}/{targets.max().item():.4f}\")\n            logger.info(f\"Outputs shape: {outputs.shape}, min/max: {outputs.min().item():.4f}/{outputs.max().item():.4f}\")\n            logger.info(f\"Loss: {loss.item():.4f}\")\n            logger.info(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            torch.cuda.synchronize()\n            logger.info(torch.cuda.memory_summary())\n        sys.stdout.flush()\n        start = time.time() \n    avg_loss = epoch_loss / len(loader)\n    print(f\"Epoch {epoch+1}, Tomo {tomo_id} completed, Average Loss: {avg_loss:.4f}\", flush=True)\n    sys.stdout.flush()\n    return avg_loss\n\n# Updated validation function with logging\ndef validate(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, criterion):\n    model.eval()\n    epoch_loss = 0.0\n    patches_per_volume = 8\n    for tomo_id in tqdm(val_ids, desc=\"Validation\"):\n        logger.warning(f\"Validating tomogram {tomo_id}\")\n        try:\n            tomo_path, mask_path = download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, \"train\", local_dir)\n            volume = np.load(tomo_path)\n            mask = np.load(mask_path)\n            tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n            num_motors = tomo_labels[\"Number of motors\"]\n            log_info(f\"Tomogram {tomo_id} has {num_motors} motors\")\n            patches, mask_patches = sample_patches(tomo_id, volume, mask, labels_df, patches_per_volume=patches_per_volume)\n            dataset = PatchDataset(patches, mask_patches)\n            loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n            with torch.no_grad():\n                for i, (inputs, targets) in enumerate(loader):\n                    inputs = inputs.to(device)\n                    targets = targets.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    epoch_loss += loss.item()\n                    log_info(f\"Validation tomo {tomo_id}, Batch {i+1}, Loss: {loss.item():.4f}\")\n            del volume, mask, patches, mask_patches, dataset, loader\n            clean_memory([tomo_id], local_dir)\n        except Exception as e:\n            logger.error(f\"Error validating tomogram {tomo_id}: {str(e)}\")\n            raise\n    return epoch_loss / (len(val_ids) * patches_per_volume)\n\n\n# Clean memory after processing tomograms\ndef clean_memory(tomo_ids, local_dir):\n    \"\"\"Delete tomogram and mask files and clear GPU memory.\"\"\"\n    for tomo_id in tomo_ids:\n        tomo_path = os.path.join(local_dir, f\"{tomo_id}.npy\")\n        mask_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n        for path in [tomo_path, mask_path]:\n            if os.path.exists(path):\n                os.remove(path)\n                logger.info(f\"Deleted {path}\")\n    gc.collect()\n    torch.cuda.empty_cache()\n    logger.info(\"Cleared memory and GPU cache\")\n\n\n# Prefetch thread for producer-consumer queue\ndata_queue = queue.Queue(maxsize=2)\ndef prefetch_batches(train_ids, batch_size, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, patches_per_volume):\n    for batch_start in range(0, len(train_ids), batch_size):\n        batch_tomo_ids = train_ids[batch_start:batch_start + batch_size]\n        logger.warning(f\"Prefetching batch of tomograms: {batch_tomo_ids}\")\n        batch_data = []\n        download_start_time = time.time()\n        try:\n            with ThreadPoolExecutor(max_workers=batch_size) as executor:\n                future_to_tomo = {\n                    executor.submit(\n                        download_and_preprocess,\n                        tomo_id,\n                        gcs_preprocessed_path,\n                        gcs_precomputed_path,\n                        \"train\",\n                        local_dir,\n                        labels_df,\n                        patches_per_volume\n                    ): tomo_id for tomo_id in batch_tomo_ids\n                }\n                for future in as_completed(future_to_tomo):\n                    batch_data.append(future.result())\n            logger.warning(f\"Batch prefetch time for {batch_tomo_ids}: {time.time() - download_start_time:.2f} seconds\")\n            data_queue.put((batch_tomo_ids, batch_data))\n        except Exception as e:\n            logger.error(f\"Error prefetching batch {batch_tomo_ids}: {str(e)}\")\n            raise\n\n\n# Training loop with batch processing\nnum_epochs = 3\npatience = 10\ntrigger_times = 0\npatches_per_volume_train = 64\nbatch_size = 8  # Process 5 tomograms at a time\nmetrics_log = []  # Store metrics for concise reporting\nbest_val_loss = float(\"inf\")\ntrain_losses = []\nval_losses = []\nfirst_epoch = True\n\n# Analyze and plot tomogram distribution\ntotal_tomograms, motor_tomograms, non_motor_tomograms = analyze_tomograms(labels_df)\nplot_tomogram_distribution(labels_df)\n\n\nfor epoch in range(start_epoch, num_epochs):\n    logger.info(f\"STARTING TRAINING - Epoch {epoch+1}/{num_epochs}\")\n    print(f\"STARTING TRAINING - Epoch {epoch+1}/{num_epochs}\", flush=True)  \n    sys.stdout.flush()\n    epoch_train_loss = 0.0\n    processed_tomograms = 0\n    total_train_tomograms = len(train_ids)\n\n    # Start prefetch thread\n    prefetch_thread = threading.Thread(\n        target=prefetch_batches,\n        args=(train_ids, batch_size, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, patches_per_volume_train)\n    )\n    prefetch_thread.start()\n\n\n    # Process tomograms in batches\n    for batch_start in range(0, len(train_ids), batch_size):\n        batch_tomo_ids, batch_data = data_queue.get()\n        logger.warning(f\"Processing batch of tomograms: {batch_tomo_ids}\")\n        print(f\"Processing batch of tomograms: {batch_tomo_ids}\", flush=True)\n        sys.stdout.flush()\n\n        # Train on batch\n        for tomo_id, patches, mask_patches in batch_data:\n            try:\n                dataset = PatchDataset(patches, mask_patches)\n                loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n                logger.warning(f\"Training on tomogram {tomo_id}\")\n                print(f\"Training on tomogram {tomo_id}\", flush=True)\n                batch_loss = train_epoch(model, loader, criterion, optimizer, epoch, start_epoch, tomo_id)\n                epoch_train_loss += batch_loss\n                processed_tomograms += 1\n                logger.warning(f\"Completed training on tomogram {tomo_id}, Loss: {batch_loss:.4f}\")\n                print(f\"Completed training on tomogram {tomo_id}, Loss: {batch_loss:.4f}\", flush=True)\n                print(f\"Processed {processed_tomograms}/{total_train_tomograms} tomograms\", flush=True)\n                logger.warning(f\"Processed {processed_tomograms}/{total_train_tomograms} tomograms\")\n                del dataset, loader, patches, mask_patches\n            except Exception as e:\n                logger.error(f\"Error training tomogram {tomo_id}: {str(e)}\")\n                raise\n            sys.stdout.flush()\n\n        # Clean up\n        print(f\"Cleaning memory for batch: {batch_tomo_ids}\", flush=True)\n        logger.info(f\"Cleaning memory for batch: {batch_tomo_ids}\")\n        clean_memory(batch_tomo_ids, local_dir)\n\n    prefetch_thread.join()  # Wait for prefetching to complete\n\n    train_loss = epoch_train_loss / max(1, processed_tomograms)\n    train_losses.append(train_loss)\n\n    # Validation\n    logger.info(\"Starting validation\")\n    val_loss = validate(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, criterion)\n    val_losses.append(val_loss)\n    \n    # Log metrics\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n    logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n    #wandb.log({\n    #    \"epoch\": epoch + 1,\n    #    \"train_loss\": train_loss,\n    #    \"val_loss\": val_loss,\n    #    \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n    #})\n    metrics_log.append({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss\n    })\n    \n    # Save metrics to file\n    pd.DataFrame(metrics_log).to_csv(\"/kaggle/working/training_metrics.csv\", index=False)\n    print(\"TEMP Saved training metrics to training_metrics.csv\")\n    logger.info(\"Saved training metrics to training_metrics.csv\")\n    sys.stdout.flush()\n\n    scheduler.step(val_loss)   # Learning rate scheduling\n\n    # GPU diagnostics every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.cuda.synchronize()\n        logger.info(f\"GPU Memory after Epoch {epoch+1}:\\n{torch.cuda.memory_summary()}\")\n        nvidia_smi_output = os.popen(\"nvidia-smi\").read()\n        logger.info(f\"nvidia-smi output after Epoch {epoch+1}:\\n{nvidia_smi_output}\")\n        sys.stdout.flush()\n\n    \n    # Checkpointing and Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        trigger_times = 0\n        torch.save({               # Save full training state \n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_loss': best_val_loss\n        }, \"checkpoint.pth\")\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pth\") #Save model weights for inference / tuning \n        print(\"Saved checkpoint and best model\", flush=True)\n        logger.info(\"Saved checkpoint and best model\")\n        # Log the best model to wandb - Uncomment the below. \n        #artifact = wandb.Artifact(\"best_model\", type=\"model\")\n        #artifact.add_file(\"best_model.pth\")\n        #wandb.log_artifact(artifact)\n    else:\n        trigger_times += 1\n        if trigger_times >= patience:\n            print(\"Early stopping triggered!\", flush=True)\n            logger.info(\"Early stopping triggered!\")\n            break\n\n    # Flush logs to ensure training.log is updated\n    file_handler.flush()\n    file_handler.stream.flush()\n\n# Plot losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(val_losses, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss Over Epochs\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_plot.png\")\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:54:26.209427Z","iopub.execute_input":"2025-05-21T09:54:26.209704Z","iopub.status.idle":"2025-05-21T09:54:26.263781Z","shell.execute_reply.started":"2025-05-21T09:54:26.209682Z","shell.execute_reply":"2025-05-21T09:54:26.262797Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2246695800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Prefetch thread for producer-consumer queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mdata_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprefetch_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_preprocessed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_precomputed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches_per_volume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'queue' is not defined"],"ename":"NameError","evalue":"name 'queue' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# 1 Hyperparameter Tuning (Peak Detection Threshold)\n# 2 Predict on the validation set,\n# 3 Extract motor locations using peak detection\n# 4 Tune the threshold to maximize the Fβ-score (β=2).\n# 5 Using competition's metric\n\n# Metric implementation (from provided notebook)\ndef distance_metric(solution, submission, thresh_ratio, min_radius):\n    coordinate_cols = ['Motor axis 0', 'Motor axis 1', 'Motor axis 2']\n    label_tensor = solution[coordinate_cols].values.reshape(len(solution), -1, len(coordinate_cols))\n    predicted_tensor = submission[coordinate_cols].values.reshape(len(submission), -1, len(coordinate_cols))\n    solution['distance'] = np.linalg.norm(label_tensor - predicted_tensor, axis=2).min(axis=1)\n    solution['thresholds'] = solution['Voxel spacing'].apply(lambda x: (min_radius * thresh_ratio) / x)\n    solution['predictions'] = submission['Has motor'].values\n    solution.loc[(solution['distance'] > solution['thresholds']) & (solution['Has motor'] == 1) & (submission['Has motor'] == 1), 'predictions'] = 0\n    return solution['predictions'].values\n\ndef score(solution, submission, min_radius, beta):\n    solution = solution.sort_values('tomo_id').reset_index(drop=True)\n    submission = submission.sort_values('tomo_id').reset_index(drop=True)\n    if not solution['tomo_id'].eq(submission['tomo_id']).all():\n        raise ValueError('Submitted tomo_id values do not match')\n    submission['Has motor'] = 1\n    select = (submission[['Motor axis 0', 'Motor axis 1', 'Motor axis 2']] == -1).any(axis='columns')\n    submission.loc[select, 'Has motor'] = 0\n    predictions = distance_metric(solution, submission, thresh_ratio=1.0, min_radius=min_radius)\n    return sklearn.metrics.fbeta_score(solution['Has motor'].values, predictions, beta=beta)\n\n# Predict on full volume with sliding window\ndef predict_full_volume(model, volume, patch_size=(128, 128, 128), stride=64):\n    model.eval()\n    volume = volume.to(device)\n    z_size, y_size, x_size = volume.shape[2:]\n    pz, py, px = patch_size\n    output = torch.zeros_like(volume)\n    counts = torch.zeros_like(volume)\n    \n    with torch.no_grad():\n        for z in range(0, z_size, stride):\n            for y in range(0, y_size, stride):\n                for x in range(0, x_size, stride):\n                    z_end, y_end, x_end = z+pz, y+py, x+px\n                    patch = volume[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)]\n                    pad_z, pad_y, pad_x = max(0, z_end-z_size), max(0, y_end-y_size), max(0, x_end-x_size)\n                    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n                        patch = torch.nn.functional.pad(patch, (0, pad_x, 0, pad_y, 0, pad_z))\n                    out_patch = torch.sigmoid(model(patch))\n                    output[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)] += out_patch[:, :, :pz-pad_z, :py-pad_y, :px-pad_x]\n                    counts[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)] += 1\n    output = output / (counts + 1e-8)\n    return output.cpu().numpy()\n\n# Extract motor location from predicted mask\ndef extract_motor_location(mask, threshold):\n    mask = mask.squeeze()\n    if mask.max() < threshold:\n        return -1, -1, -1, 0  # No motor\n    # Find the strongest peak\n    z, y, x = np.unravel_index(np.argmax(mask), mask.shape)\n    # Refine with center of mass for sub-voxel accuracy\n    region = mask[max(0, z-5):z+6, max(0, y-5):y+6, max(0, x-5):x+6]\n    if region.size == 0:\n        return z, y, x, 1\n    z_offset, y_offset, x_offset = center_of_mass(region)\n    z, y, x = z + z_offset - 5, y + y_offset - 5, x + x_offset - 5\n    return z, y, x, 1\n\n# Tune peak detection threshold\ndef tune_threshold(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, thresholds=np.linspace(0.1, 0.9, 9)):\n    best_model_path = \"/kaggle/working/best_model.pth\"\n    if os.path.exists(best_model_path):\n        model.load_state_dict(torch.load(best_model_path, weights_only=True))\n        print(\"Loaded best_model.pth\", flush=True)\n        logger.info(\"Loaded best_model.pth\")\n    else:\n        print(\"Warning: best_model.pth not found, using current model state\", flush=True)\n        logger.warning(\"best_model.pth not found, using current model state\")\n    model.eval()\n    best_threshold = 0.5\n    best_fbeta = 0.0\n    # Store thresholds and Fβ Scrores \n    thresholds_list = []\n    fbeta_scores = []\n    \n    for threshold in thresholds:\n        predictions = []\n        for tomo_id in tqdm(val_ids, desc=f\"Tuning threshold {threshold:.2f}\"):\n            dataset = TomogramDataset(tomo_id, gcs_preprocessed_path, local_dir, mode=\"val\")  # Fixed typo\n            dataset.load()\n            volume, _ = dataset[0]\n            pred_mask = predict_full_volume(model, volume)\n            z, y, x, has_motor = extract_motor_location(pred_mask, threshold)\n            \n            # Visualize prediction for the first tomogram\n            if len(predictions) == 0 and tomo_id == val_ids[0]:\n                mask_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n                if not os.path.exists(mask_path):\n                    _, mask_path = download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, \"train\", local_dir)\n                gt_mask = np.load(mask_path)\n                slice_idx = volume.shape[2] // 2\n                tomo_slice = volume[0, 0, slice_idx, :, :].numpy()\n                pred_slice = pred_mask[0, 0, slice_idx, :, :]\n                gt_slice = gt_mask[slice_idx, :, :]\n                gt_row = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n                gt_z, gt_y, gt_x = gt_row[\"Motor axis 0\"], gt_row[\"Motor axis 1\"], gt_row[\"Motor axis 2\"]\n\n                plt.figure(figsize=(15, 5))\n                plt.subplot(1, 3, 1)\n                plt.imshow(tomo_slice, cmap=\"gray\")\n                plt.title(f\"Tomogram Slice (z={slice_idx})\")\n                plt.axis(\"off\")\n                plt.subplot(1, 3, 2)\n                plt.imshow(pred_slice, cmap=\"hot\")\n                if has_motor:\n                    plt.scatter(pred_x, pred_y, c=\"blue\", marker=\"x\", s=100, label=\"Predicted\")\n                if gt_z != -1:\n                    plt.scatter(gt_x, gt_y, c=\"green\", marker=\"o\", s=100, label=\"Ground Truth\")\n                plt.title(\"Predicted Mask with Motor Locations\")\n                plt.legend()\n                plt.axis(\"off\")\n                plt.subplot(1, 3, 3)\n                plt.imshow(gt_slice, cmap=\"hot\")\n                if gt_z != -1:\n                    plt.scatter(gt_x, gt_y, c=\"green\", marker=\"o\", s=100, label=\"Ground Truth\")\n                plt.title(\"Ground Truth Mask\")\n                plt.legend()\n                plt.axis(\"off\")\n                plt.tight_layout()\n                plt.show()\n                os.remove(mask_path)  # Clean up\n                break  # Visualize only one tomogram\n            predictions.append({\"tomo_id\": tomo_id, \"Motor axis 0\": z, \"Motor axis 1\": y, \"Motor axis 2\": x, \"Has motor\": has_motor})\n            dataset.clear()\n        \n        # Create submission DataFrame\n        submission_df = pd.DataFrame(predictions)\n        # Create solution DataFrame\n        solution_data = []\n        for tomo_id in val_ids:\n            tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n            solution_data.append({\n                \"tomo_id\": tomo_id,\n                \"Motor axis 0\": tomo_labels[\"Motor axis 0\"],\n                \"Motor axis 1\": tomo_labels[\"Motor axis 1\"],\n                \"Motor axis 2\": tomo_labels[\"Motor axis 2\"],\n                \"Voxel spacing\": tomo_labels[\"Voxel spacing\"],\n                \"Has motor\": 1 if tomo_labels[\"Number of motors\"] > 0 else 0\n            })\n        solution_df = pd.DataFrame(solution_data)\n        fbeta = score(solution_df, submission_df, min_radius=1000, beta=2) # Compute Fβ-score\n        print(f\"Threshold {threshold:.2f}, Fβ-score: {fbeta:.4f}\", flush=True)\n        logger.info(f\"Threshold {threshold:.2f}, Fβ-score: {fbeta:.4f}\")\n        thresholds_list.append(threshold)  # Append for plotting \n        fbeta_scores.append(fbeta)\n        # WANDB Log Fβ-score for this threshold\n        #wandb.log({\"threshold\": threshold,\"fbeta_score\": fbeta})\n        \n        if fbeta > best_fbeta:\n            best_fbeta = fbeta\n            best_threshold = threshold\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds_list, fbeta_scores, marker=\"o\")\n    plt.xlabel(\"Threshold\")\n    plt.ylabel(\"Fβ-score (β=2)\")\n    plt.title(\"Fβ-score vs. Peak Detection Threshold\")\n    plt.grid(True)\n    plt.show()\n    plt.close()\n    print(f\"Best threshold: {best_threshold:.2f}, Best Fβ-score: {best_fbeta:.4f}\", flush=True)\n    logger.info(f\"Best threshold: {best_threshold:.2f}, Best Fβ-score: {best_fbeta:.4f}\")\n    #wandb.log({\"best_threshold\": best_threshold, \"best_fbeta_score\": best_fbeta})\n    return best_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:32:16.745366Z","iopub.execute_input":"2025-05-21T09:32:16.746036Z","iopub.status.idle":"2025-05-21T09:32:16.772692Z","shell.execute_reply.started":"2025-05-21T09:32:16.746013Z","shell.execute_reply":"2025-05-21T09:32:16.771549Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Test prediction\ndef predict_test(model, test_ids, gcs_preprocessed_path, local_dir, threshold):\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    predictions = []\n    for tomo_id in tqdm(test_ids, desc=\"Predicting on test set\"):\n        dataset = TomogramDataset(tomo_id, gcs_preprocessed_path, local_dir, mode=\"test\")\n        dataset.load()\n        volume, _ = dataset[0]\n        pred_mask = predict_full_volume(model, volume)\n        z, y, x, has_motor = extract_motor_location(pred_mask, threshold)\n        predictions.append({\"tomo_id\": tomo_id, \"Motor axis 0\": z, \"Motor axis 1\": y, \"Motor axis 2\": x})\n        dataset.clear()\n    return pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:38:06.754861Z","iopub.execute_input":"2025-05-21T09:38:06.755201Z","iopub.status.idle":"2025-05-21T09:38:06.7612Z","shell.execute_reply.started":"2025-05-21T09:38:06.75518Z","shell.execute_reply":"2025-05-21T09:38:06.760486Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Run pipeline\nprint(\"Starting hyperparameter tuning...\")\nlogger.info(\"Starting hyperparameter tuning...\")\nbest_threshold = tune_threshold(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df)\nprint(\"Generating test predictions...\")\nlogger.info(\"Generating test predictions...\")\nsubmission_df = predict_test(model, test_ids, gcs_preprocessed_path, local_dir, best_threshold)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created: submission.csv\")\nlogger.info(\"Submission file created: submission.csv\")\n# Finish the wandb run\n#wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:38:09.069802Z","iopub.execute_input":"2025-05-21T09:38:09.070112Z","iopub.status.idle":"2025-05-21T09:38:45.984472Z","shell.execute_reply.started":"2025-05-21T09:38:09.070062Z","shell.execute_reply":"2025-05-21T09:38:45.98337Z"}},"outputs":[{"name":"stdout","text":"Starting hyperparameter tuning...\nWarning: best_model.pth not found, using current model state\n","output_type":"stream"},{"name":"stderr","text":"Tuning threshold 0.10:   0%|          | 0/65 [00:00<?, ?it/s]Exception in thread Thread-47 (download_mask):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_35/4107285653.py\", line 68, in download_mask\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 118, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 103, in sync\n    raise return_result\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 56, in _runner\n    result[0] = await coro\n                ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 681, in _get\n    return await _run_coros_in_chunks(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 280, in _run_coros_in_chunks\n    result, k = await done.pop()\n                ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 257, in _run_coro\n    return await asyncio.wait_for(coro, timeout=timeout), i\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 452, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/callbacks.py\", line 81, in func\n    return await fn(path1, path2, callback=child, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/core.py\", line 1620, in _get_file\n    await self._get_file_request(u2, lpath, callback=callback, **kwargs)\n  File \"<decorator-gen-120>\", line 2, in _get_file_request\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/retry.py\", line 135, in retry_request\n    return await func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/core.py\", line 1607, in _get_file_request\n    f2.write(data)\nOSError: [Errno 28] No space left on device\nException in thread Thread-46 (download_npy):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_35/4107285653.py\", line 60, in download_npy\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 118, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 103, in sync\n    raise return_result\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 56, in _runner\n    result[0] = await coro\n                ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 681, in _get\n    return await _run_coros_in_chunks(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 280, in _run_coros_in_chunks\n    result, k = await done.pop()\n                ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\", line 257, in _run_coro\n    return await asyncio.wait_for(coro, timeout=timeout), i\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 452, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fsspec/callbacks.py\", line 81, in func\n    return await fn(path1, path2, callback=child, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/core.py\", line 1620, in _get_file\n    await self._get_file_request(u2, lpath, callback=callback, **kwargs)\n  File \"<decorator-gen-120>\", line 2, in _get_file_request\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/retry.py\", line 135, in retry_request\n    return await func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gcsfs/core.py\", line 1607, in _get_file_request\n    f2.write(data)\nOSError: [Errno 28] No space left on device\nTuning threshold 0.10:   0%|          | 0/65 [00:36<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/807396157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting hyperparameter tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting hyperparameter tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_preprocessed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_precomputed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating test predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating test predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2675285236.py\u001b[0m in \u001b[0;36mtune_threshold\u001b[0;34m(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, thresholds)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtomo_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Tuning threshold {threshold:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTomogramDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtomo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_preprocessed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fixed typo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mvolume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mpred_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_full_volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3082621155.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtomo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 321668064 into shape (800,928,960)"],"ename":"ValueError","evalue":"cannot reshape array of size 321668064 into shape (800,928,960)","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}