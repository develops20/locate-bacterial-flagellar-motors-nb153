{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/byu-2-locate-bacterial-flagellar-motors-nb153?scriptVersionId=237933996\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install required packages\n!pip install monai scipy scikit-image wandb imageio gcsfs\n\n# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\nimport torch.optim as optim\nfrom scipy.ndimage import gaussian_filter, center_of_mass\nfrom scipy.signal import find_peaks\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport glob\nfrom IPython.display import Video, display\nimport wandb\nimport time\nimport shutil\nimport gcsfs  # Dataset is too big for kaggle - After multiple attempts of uploading, I failed miserably. \nimport gc\nimport threading\nimport logging\nimport sys\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:27:22.097164Z","iopub.execute_input":"2025-05-05T09:27:22.097979Z","iopub.status.idle":"2025-05-05T09:29:35.810699Z","shell.execute_reply.started":"2025-05-05T09:27:22.097939Z","shell.execute_reply":"2025-05-05T09:29:35.80989Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.1)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\nRequirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2024.10.0)\nRequirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.5.1+cu124)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.1.10)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.16)\nRequirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\nCollecting fsspec==2024.10.0 (from gcsfs)\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.27.0)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.14.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.19.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.18.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.34.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.6.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.67.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.24->monai) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.10.0 monai-1.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"2025-05-05 09:29:16.199690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746437356.434759      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746437356.510754      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Ensure the output directory exists\nos.makedirs('/kaggle/working', exist_ok=True)\n\n# Clear any existing handlers to avoid conflicts\nlogging.getLogger().handlers = []\nlogger = logging.getLogger() #Root logger \nlogger.setLevel(logging.INFO)\n\n# Create and configure FileHandler\nfile_handler = logging.FileHandler(\"/kaggle/working/training.log\", mode='a')  # Append mode\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n\n# Add handler to logger\nlogger.addHandler(file_handler)\n\n# Test logging\nlogger.info(\"Logging initialized successfully\")\n\n# Force flush to ensure logs are written\nfile_handler.flush()\nfile_handler.stream.flush()\n\n# Brief delay to ensure file write completes\ntime.sleep(0.1)\n\n# Verify file contents\ntry:\n    with open(\"/kaggle/working/training.log\", \"r\") as f:\n        contents = f.read()\n        print(f\"Initial log file contents: {contents}\", flush=True)\nexcept FileNotFoundError:\n    print(\"training.log not found\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:32:00.697872Z","iopub.execute_input":"2025-05-05T09:32:00.700764Z","iopub.status.idle":"2025-05-05T09:32:00.819194Z","shell.execute_reply.started":"2025-05-05T09:32:00.700692Z","shell.execute_reply":"2025-05-05T09:32:00.817884Z"}},"outputs":[{"name":"stdout","text":"Initial log file contents: 2025-05-05 09:32:00,713 - INFO - Logging initialized successfully\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Initialize wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_token = user_secrets.get_secret(\"WANDB\")\nwandb.login(key=wb_token)\nwandb.init(\n    project=\"byu-bacterial-flagellar-motors\",\n    config={\n        \"learning_rate\": 1e-3,\n        \"epochs\": 50,\n        \"batch_size\": 4,\n        \"patch_size\": (128, 128, 128),\n        \"gaussian_sigma\": 5,\n        \"architecture\": \"3D U-Net\",\n        \"optimizer\": \"Adam\",\n        \"loss_function\": \"DiceLoss\",\n        \"beta\": 2\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fs = gcsfs.GCSFileSystem(token=\"anon\") # Initialize GCS filesystem\n\n# Define GCS path and local directory\ngcs_precomputed_path = \"gs://nb153/precomputedmasks\"\ngcs_preprocessed_path = \"gs://nb153/preprocessed\"\nlocal_dir = \"/kaggle/working/data\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:41:16.628513Z","iopub.execute_input":"2025-05-05T09:41:16.629199Z","iopub.status.idle":"2025-05-05T09:41:16.641408Z","shell.execute_reply.started":"2025-05-05T09:41:16.629161Z","shell.execute_reply":"2025-05-05T09:41:16.640295Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 1. Analyze dataset to identify tomograms with motors\ndef identify_motor_tomograms(labels_df):\n    \"\"\"Identify tomograms with valid motor annotations.\"\"\"\n    motor_tomograms = []\n    for tomo_id in labels_df[\"tomo_id\"].unique():\n        tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n        if tomo_labels[\"Number of motors\"] > 0 and tomo_labels[\"Motor axis 0\"] != -1:\n            motor_tomograms.append(tomo_id)\n    logger.info(f\"Found {len(motor_tomograms)} tomograms with motors\")\n    return motor_tomograms\n\n\n# Load labels and identify motor tomograms\nlabels_df = pd.read_csv(\"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\")\ntomo_ids = sorted(labels_df[\"tomo_id\"].unique())\nmotor_tomo_ids = identify_motor_tomograms(labels_df)  # 2. Store tomogram IDs with motors\nlogger.info(f\"Total tomograms: {len(tomo_ids)}, Motor tomograms: {len(motor_tomo_ids)}\")\n\n# Split into train/val/test (80/10/10) using only motor tomograms for training\ntrain_val_ids, test_ids = train_test_split(tomo_ids, test_size=0.1, random_state=42)\ntrain_ids, val_ids = train_test_split(train_val_ids, test_size=0.1111, random_state=42)\ntrain_ids = [tid for tid in train_ids if tid in motor_tomo_ids]  # 3. Use only motor tomograms for training\nlogger.info(f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}\")\n\n# Analyze tomograms and print motor statistics\ndef analyze_tomograms(labels_df):\n    total_tomograms = len(labels_df[\"tomo_id\"].unique())\n    motor_tomograms = len(labels_df[labels_df[\"Number of motors\"] > 0])\n    non_motor_tomograms = total_tomograms - motor_tomograms\n    print(f\"Total tomograms: {total_tomograms}\", flush=True)\n    print(f\"Tomograms with motors: {motor_tomograms}\", flush=True)\n    print(f\"Tomograms without motors: {non_motor_tomograms}\", flush=True)\n    logger.info(f\"Total tomograms: {total_tomograms}\")\n    logger.info(f\"Tomograms with motors: {motor_tomograms}\")\n    logger.info(f\"Tomograms without motors: {non_motor_tomograms}\")\n    return total_tomograms, motor_tomograms, non_motor_tomograms\n\n# Plot tomogram distribution\ndef plot_tomogram_distribution(labels_df):\n    total_tomograms = len(labels_df[\"tomo_id\"].unique())\n    motor_tomograms = len(labels_df[labels_df[\"Number of motors\"] > 0])\n    non_motor_tomograms = total_tomograms - motor_tomograms\n    plt.figure(figsize=(8, 5))\n    plt.bar([\"Total\", \"With Motors\", \"Without Motors\"], [total_tomograms, motor_tomograms, non_motor_tomograms], color=[\"blue\", \"green\", \"red\"])\n    plt.title(\"Tomogram Distribution\")\n    plt.ylabel(\"Count\")\n    for i, count in enumerate([total_tomograms, motor_tomograms, non_motor_tomograms]):\n        plt.text(i, count + 0.5, str(count), ha=\"center\")\n    plt.show()\n    plt.close()\n\n# Download functions with simultaneous downloading\ndef download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, split, local_dir):\n    \"\"\"Download tomogram and mask simultaneously using threads.\"\"\"\n    def download_npy():\n        gcs_file_path = f\"{gcs_preprocessed_path}/{split}/{tomo_id}/{tomo_id}.npy\"\n        local_file_path = os.path.join(local_dir, f\"{tomo_id}.npy\")\n        if not os.path.exists(local_file_path):\n            logger.info(f\"Downloading {gcs_file_path} to {local_file_path}\")\n            fs.get(gcs_file_path, local_file_path)\n            logger.info(f\"✅ Download complete: {tomo_id}\")\n\n    def download_mask():\n        gcs_file_path = f\"{gcs_precomputed_path}/{split}/{tomo_id}_mask.npy\"\n        local_file_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n        if not os.path.exists(local_file_path):\n            logger.info(f\"Downloading {gcs_file_path} to {local_file_path}\")\n            fs.get(gcs_file_path, local_file_path)\n            logger.info(f\"✅ Download complete: {tomo_id}_mask.npy\")\n\n    # Run downloads in parallel\n    t1 = threading.Thread(target=download_npy)\n    t2 = threading.Thread(target=download_mask)\n    t1.start(); t2.start()\n    t1.join(); t2.join()\n    logger.info(f\"Completed downloading tomogram and mask for {tomo_id}\")\n    return os.path.join(local_dir, f\"{tomo_id}.npy\"), os.path.join(local_dir, f\"{tomo_id}_mask.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:41:20.288961Z","iopub.execute_input":"2025-05-05T09:41:20.290061Z","iopub.status.idle":"2025-05-05T09:41:20.644163Z","shell.execute_reply.started":"2025-05-05T09:41:20.29002Z","shell.execute_reply":"2025-05-05T09:41:20.643092Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Sample patches\ndef sample_patches(tomo_id, volume, mask, labels_df, patch_size=(128, 128, 128), patches_per_volume=32):\n    shape = volume.shape\n    patches = []\n    mask_patches = []\n    tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id]\n    motor_coords = []\n    for _, row in tomo_labels.iterrows():\n        if row[\"Number of motors\"] > 0 and row[\"Motor axis 0\"] != -1:\n            z, y, x = int(row[\"Motor axis 0\"]), int(row[\"Motor axis 1\"]), int(row[\"Motor axis 2\"])\n            if 0 <= z < shape[0] and 0 <= y < shape[1] and 0 <= x < shape[2]:\n                motor_coords.append((z, y, x))\n    \n    for _ in range(patches_per_volume // 2):\n        if motor_coords:\n            zc, yc, xc = motor_coords[np.random.randint(len(motor_coords))]\n            z = np.clip(zc - patch_size[0]//2 + np.random.randint(-32, 32), 0, shape[0] - patch_size[0])\n            y = np.clip(yc - patch_size[1]//2 + np.random.randint(-32, 32), 0, shape[1] - patch_size[1])\n            x = np.clip(xc - patch_size[2]//2 + np.random.randint(-32, 32), 0, shape[2] - patch_size[2])\n        else:\n            z = np.random.randint(0, max(1, shape[0] - patch_size[0]))\n            y = np.random.randint(0, max(1, shape[1] - patch_size[1]))\n            x = np.random.randint(0, max(1, shape[2] - patch_size[2]))\n        patch = volume[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        mask_patch = mask[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        patches.append(patch)\n        mask_patches.append(mask_patch)\n    \n    for _ in range(patches_per_volume // 2):\n        z = np.random.randint(0, max(1, shape[0] - patch_size[0]))\n        y = np.random.randint(0, max(1, shape[1] - patch_size[1]))\n        x = np.random.randint(0, max(1, shape[2] - patch_size[2]))\n        patch = volume[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        mask_patch = mask[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]][np.newaxis, ...]\n        patches.append(patch)\n        mask_patches.append(mask_patch)\n    \n    return np.array(patches), np.array(mask_patches)\n\n# Patch dataset\nclass PatchDataset(Dataset):\n    def __init__(self, patches, mask_patches):\n        self.patches = patches\n        self.mask_patches = mask_patches\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, idx):\n        patch = self.patches[idx]\n        mask_patch = self.mask_patches[idx]\n        return torch.tensor(patch, dtype=torch.float32), torch.tensor(mask_patch, dtype=torch.float32)\n\n# Tomogram dataset\nclass TomogramDataset(Dataset):\n    def __init__(self, tomo_id, gcs_preprocessed_path, local_dir, mode=\"test\"):\n        self.tomo_id = tomo_id\n        self.gcs_preprocessed_path = gcs_preprocessed_path\n        self.gcs_precomputed_path = gcs_preprocessed_path.replace(\"preprocessed\", \"precomputedmasks\")\n        self.local_dir = local_dir\n        self.mode = mode\n        self.volume = None\n    \n    def load(self):\n        tomo_path, mask_path = download_npy_and_mask(\n            self.tomo_id, \n            self.gcs_preprocessed_path, \n            self.gcs_precomputed_path, \n            \"train\" if self.mode == \"val\" else self.mode, \n            self.local_dir\n        )\n        self.volume = np.load(tomo_path)\n        if os.path.exists(mask_path):\n            self.mask = np.load(mask_path)\n        print(f\"Loaded tomogram {self.tomo_id} (shape: {self.volume.shape})\", flush=True)\n        logger.info(f\"Loaded tomogram {self.tomo_id} (shape: {self.volume.shape})\")\n        if self.mask is not None:\n            print(f\"Loaded mask {self.tomo_id} (min/max: {self.mask.min()}/{self.mask.max()})\", flush=True)\n            logger.info(f\"Loaded mask {self.tomo_id} (min/max: {self.mask.min()}/{self.mask.max()})\")\n    \n    def clear(self):\n        if self.volume is not None:\n            del self.volume\n            self.volume = None\n            tomo_path = os.path.join(local_dir, f\"{self.tomo_id}.npy\")\n            if os.path.exists(tomo_path):\n                os.remove(tomo_path)\n            gc.collect()\n    \n    def __len__(self):\n        return 1\n    \n    def __getitem__(self, idx):\n        if self.volume is None:\n            self.load()\n        return torch.from_numpy(self.volume).float(), torch.from_numpy(self.mask).float() if self.mask is not None else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:43:38.982704Z","iopub.execute_input":"2025-05-05T07:43:38.983386Z","iopub.status.idle":"2025-05-05T07:43:39.004583Z","shell.execute_reply.started":"2025-05-05T07:43:38.983351Z","shell.execute_reply":"2025-05-05T07:43:39.003896Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Initialize model\nmodel = UNet(\n    spatial_dims=3,\n    in_channels=1,\n    out_channels=1,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n).to(device)\nlogger.info(f\"Model device: {next(model.parameters()).device}\")\n\n# Loss and optimizer\ncriterion = DiceLoss(sigmoid=True)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5)\n\n# Load checkpoint if available\nstart_epoch = 0\nbest_val_loss = float(\"inf\")\ntrain_losses = []\nval_losses = []\nif os.path.exists(\"checkpoint.pth\"):\n    checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    best_val_loss = checkpoint['best_val_loss']\n    start_epoch = checkpoint['epoch'] + 1\n    logger.info(f\"Resumed from epoch {start_epoch}\")\nelse:\n    logger.info(\"No checkpoint found, starting from scratch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:43:42.307877Z","iopub.execute_input":"2025-05-05T07:43:42.308413Z","iopub.status.idle":"2025-05-05T07:43:42.52793Z","shell.execute_reply.started":"2025-05-05T07:43:42.308389Z","shell.execute_reply":"2025-05-05T07:43:42.527283Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# TRAINING FUNCTION LOOP \n","metadata":{}},{"cell_type":"code","source":"# Training function\n# Training function with print-based logging\ndef train_epoch(model, loader, criterion, optimizer, epoch, start_epoch, tomo_id):\n    model.train()\n    epoch_loss = 0.0\n    start = time.time()\n    for i, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Training tomo {tomo_id}\", file=sys.stdout, disable=True)):\n        batch_load_time = time.time() - start\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        # Print batch progress to console\n        print(f\"Epoch {epoch+1}, Tomo {tomo_id}, Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}\", flush=True)\n        # Detailed logs for first 2 batches of first epoch (to console and file)\n        if epoch == start_epoch and i < 2:\n            # print(f\"Tomo {tomo_id}, Batch {i} load time: {batch_load_time:.2f}s\", flush=True)\n            # print(f\"Inputs shape: {inputs.shape}, min/max: {inputs.min().item():.4f}/{inputs.max().item():.4f}\", flush=True)\n            # print(f\"Targets shape: {targets.shape}, min/max: {targets.min().item():.4f}/{targets.max().item():.4f}\", flush=True)\n            # print(f\"Outputs shape: {outputs.shape}, min/max: {outputs.min().item():.4f}/{outputs.max().item():.4f}\", flush=True)\n            # print(f\"Loss: {loss.item():.4f}\", flush=True)\n            # print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\", flush=True)\n            logger.info(f\"Tomo {tomo_id}, Batch {i} load time: {batch_load_time:.2f}s\")\n            logger.info(f\"Inputs shape: {inputs.shape}, min/max: {inputs.min().item():.4f}/{inputs.max().item():.4f}\")\n            logger.info(f\"Targets shape: {targets.shape}, min/max: {targets.min().item():.4f}/{targets.max().item():.4f}\")\n            logger.info(f\"Outputs shape: {outputs.shape}, min/max: {outputs.min().item():.4f}/{outputs.max().item():.4f}\")\n            logger.info(f\"Loss: {loss.item():.4f}\")\n            logger.info(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            torch.cuda.synchronize()\n            logger.info(torch.cuda.memory_summary())\n        sys.stdout.flush()\n        start = time.time() \n    avg_loss = epoch_loss / len(loader)\n    print(f\"Epoch {epoch+1}, Tomo {tomo_id} completed, Average Loss: {avg_loss:.4f}\", flush=True)\n    sys.stdout.flush()\n    return avg_loss\n\n# Validation function (unchanged)\ndef validate(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, criterion):\n    model.eval()\n    epoch_loss = 0.0\n    patches_per_volume = 8\n    for tomo_id in tqdm(val_ids, desc=\"Validation\"):\n        tomo_path, mask_path = download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, \"train\", local_dir)\n        volume = np.load(tomo_path)\n        mask = np.load(mask_path)\n        patches, mask_patches = sample_patches(tomo_id, volume, mask, labels_df, patches_per_volume=patches_per_volume)\n        dataset = PatchDataset(patches, mask_patches)\n        loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n        with torch.no_grad():\n            for inputs, targets in loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                epoch_loss += loss.item()\n        del volume, mask, patches, mask_patches, dataset, loader\n        os.remove(tomo_path)\n        os.remove(mask_path)\n        gc.collect()\n    return epoch_loss / (len(val_ids) * patches_per_volume)\n\n# Clean memory after processing tomograms\ndef clean_memory(tomo_ids, local_dir):\n    \"\"\"Delete tomogram and mask files and clear GPU memory.\"\"\"\n    for tomo_id in tomo_ids:\n        tomo_path = os.path.join(local_dir, f\"{tomo_id}.npy\")\n        mask_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n        for path in [tomo_path, mask_path]:\n            if os.path.exists(path):\n                os.remove(path)\n                logger.info(f\"Deleted {path}\")\n    gc.collect()\n    torch.cuda.empty_cache()\n    logger.info(\"Cleared memory and GPU cache\")\n\n# Training loop with batch processing\nnum_epochs = 50\npatience = 10\ntrigger_times = 0\npatches_per_volume_train = 64\nbatch_size = 5  # Process 5 tomograms at a time\nmetrics_log = []  # Store metrics for concise reporting\nbest_val_loss = float(\"inf\")\ntrain_losses = []\nval_losses = []\n\n# Analyze and plot tomogram distribution\ntotal_tomograms, motor_tomograms, non_motor_tomograms = analyze_tomograms(labels_df)\nplot_tomogram_distribution(labels_df)\n\n\nfor epoch in range(start_epoch, num_epochs):\n    logger.info(f\"STARTING TRAINING - Epoch {epoch+1}/{num_epochs}\")\n    print(f\"STARTING TRAINING - Epoch {epoch+1}/{num_epochs}\", flush=True)  \n    sys.stdout.flush()\n    epoch_train_loss = 0.0\n    processed_tomograms = 0\n    total_train_tomograms = len(train_ids)\n\n    # Process tomograms in batches of 5\n    for batch_start in range(0, len(train_ids), batch_size):\n        batch_tomo_ids = train_ids[batch_start:batch_start + batch_size]\n        print(f\"Processing batch of tomograms: {batch_tomo_ids}\", flush=True)\n        logger.info(f\"Processing batch of tomograms: {batch_tomo_ids}\")\n        sys.stdout.flush()\n\n        # Download tomograms and masks in batches of 5 IN PARALLEL to fix under utilization of CPU \n        batch_data = []\n        download_results[]\n        try: \n            with ThreadPoolExecutor(max_workers=batch_size) as executor:\n                future_to_tomo = {\n                    executir.submit(\n                        download_npy_and_mask,\n                        tomo_id,\n                        gcs_preprocessed_path,\n                        gcs_precomputed_path,\n                        \"train\",\n                        local_dir\n                    ): tomo_id for tomo_id in batch_tomo_ids\n                }\n                #collecting restults as they complete \n                for future in as_completed(future_to_tomo): \n                    tomo_id = future_to_tomo[future]\n                    try: \n                        tomo_path, mask_path = future.result()\n                        print(f\"TEMP Completed parallel download for tomogram {tomo_id}\")\n                        logger.info(f\"Completed parallel download for tomogram {tomo_id}\")\n                        #Loading and processing immediately to avoid buildup. \n                        volume = np.load(tomo_path)\n                        mask = np.load(mask_path)\n                        tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n                        num_motors = tomo_labels[\"Number of motors\"]\n                        print(f\"TEMP Tomogram {tomo_id} has {num_motors} motors\", flush=True)\n                        logger.info(f\"Tomogram {tomo_id} has {num_motors} motors\")\n                        patches, mask_patches = sample_patches(tomo_id, volume, mask, labels_df, patches_per_volume=patches_per_volume_train)\n                        batch_data.append((tomo_id, patches, mask_patches))\n                        del volume, mask\n                        logger.info(f\"Prepared patches for tomogram {tomo_id}\")\n                    except Exception as e:\n                        logger.error(f\"Error downloading or processing tomogram {tomo_id}: {str(e)}\")\n                        raise \n        except Exception as e:\n            logger.error(f\"Error in parallel download for batch {batch_tomo_ids}: {str(e)}\")\n            raise\n        # Minimal console output\n        print(f\"Completed downloading and processing batch: {batch_tomo_ids}\")\n\n        # Train on batch\n        for tomo_id, patches, mask_patches in batch_data:\n            try: \n                dataset = PatchDataset(patches, mask_patches)\n                loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n                print(f\"Training on tomogram {tomo_id}\", flush=True)\n                logger.info(f\"Training on tomogram {tomo_id}\")\n                batch_loss = train_epoch(model, loader, criterion, optimizer, epoch, start_epoch, tomo_id)\n                epoch_train_loss += batch_loss\n                processed_tomograms += 1\n                print(f\"Completed training on tomogram {tomo_id}, Loss: {batch_loss:.4f}\", flush=True)\n                print(f\"Processed {processed_tomograms}/{total_train_tomograms} tomograms\", flush=True)\n                logger.info(f\"Completed training on tomogram {tomo_id}, Loss: {batch_loss:.4f}\")\n                logger.info(f\"Processed {processed_tomograms}/{total_train_tomograms} tomograms\")\n                del dataset, loader, patches, mask_patches\n            except Exception as e:\n                logger.error(f\"Error training tomogram {tomo_id}: {str(e)}\")\n                raise\n                sys.stdout.flush()\n\n        # Clean up\n        print(f\"Cleaning memory for batch: {batch_tomo_ids}\", flush=True)\n        logger.info(f\"Cleaning memory for batch: {batch_tomo_ids}\")\n        clean_memory(batch_tomo_ids, local_dir)\n    \n\n    train_loss = epoch_train_loss / max(1, processed_tomograms)\n    train_losses.append(train_loss)\n\n    # Validation\n    logger.info(\"Starting validation\")\n    val_loss = validate(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, criterion)\n    val_losses.append(val_loss)\n    \n    # Log metrics\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n    logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n    #wandb.log({\n    #    \"epoch\": epoch + 1,\n    #    \"train_loss\": train_loss,\n    #    \"val_loss\": val_loss,\n    #    \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n    #})\n    metrics_log.append({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss\n    })\n    \n    # Save metrics to file\n    pd.DataFrame(metrics_log).to_csv(\"/kaggle/working/training_metrics.csv\", index=False)\n    print(\"TEMP Saved training metrics to training_metrics.csv\")\n    logger.info(\"Saved training metrics to training_metrics.csv\")\n    sys.stdout.flush()\n\n    scheduler.step(val_loss)   # Learning rate scheduling\n\n    # GPU diagnostics every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.cuda.synchronize()\n        logger.info(f\"GPU Memory after Epoch {epoch+1}:\\n{torch.cuda.memory_summary()}\")\n        nvidia_smi_output = os.popen(\"nvidia-smi\").read()\n        logger.info(f\"nvidia-smi output after Epoch {epoch+1}:\\n{nvidia_smi_output}\")\n        sys.stdout.flush()\n\n    \n    # Checkpointing and Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        trigger_times = 0\n        torch.save({               # Save full training state \n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_loss': best_val_loss\n        }, \"checkpoint.pth\")\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pth\") #Save model weights for inference / tuning \n        print(\"Saved checkpoint and best model\", flush=True)\n        logger.info(\"Saved checkpoint and best model\")\n        # Log the best model to wandb - Uncomment the below. \n        #artifact = wandb.Artifact(\"best_model\", type=\"model\")\n        #artifact.add_file(\"best_model.pth\")\n        #wandb.log_artifact(artifact)\n    else:\n        trigger_times += 1\n        if trigger_times >= patience:\n            print(\"Early stopping triggered!\", flush=True)\n            logger.info(\"Early stopping triggered!\")\n            break\n\n    # Flush logs to ensure training.log is updated\n    file_handler.flush()\n    file_handler.stream.flush()\n\n# Plot losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(val_losses, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss Over Epochs\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_plot.png\")\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:43:45.29203Z","iopub.execute_input":"2025-05-05T07:43:45.292519Z"}},"outputs":[{"name":"stdout","text":"Total tomograms: 648\nTomograms with motors: 451\nTomograms without motors: 197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"STARTING TRAINING - Epoch 1/50\nProcessing batch of tomograms: ['tomo_3c6038', 'tomo_369cce', 'tomo_cff77a', 'tomo_60d478', 'tomo_1c38fd']\nTomogram tomo_3c6038 has 1 motors\nDownloading data for tomogram tomo_3c6038\nPrepared patches for tomogram tomo_3c6038\nTomogram tomo_369cce has 1 motors\nDownloading data for tomogram tomo_369cce\nPrepared patches for tomogram tomo_369cce\nTomogram tomo_cff77a has 1 motors\nDownloading data for tomogram tomo_cff77a\nPrepared patches for tomogram tomo_cff77a\nTomogram tomo_60d478 has 1 motors\nDownloading data for tomogram tomo_60d478\nPrepared patches for tomogram tomo_60d478\nTomogram tomo_1c38fd has 1 motors\nDownloading data for tomogram tomo_1c38fd\nPrepared patches for tomogram tomo_1c38fd\nTraining on tomogram tomo_3c6038\nTraining tomo tomo_3c6038:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_3c6038, Batch 1/16, Loss: 0.9986\nTomo tomo_3c6038, Batch 0 load time: 0.18s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.6318/9.5059\nLoss: 0.9986\nGPU Memory: 0.17 GB\nTraining tomo tomo_3c6038:   6%|▋         | 1/16 [00:01<00:16,  1.07s/it]Epoch 1, Tomo tomo_3c6038, Batch 2/16, Loss: 0.9991\nTomo tomo_3c6038, Batch 1 load time: 0.11s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.6683/10.5203\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_3c6038:  12%|█▎        | 2/16 [00:01<00:08,  1.57it/s]Epoch 1, Tomo tomo_3c6038, Batch 3/16, Loss: 0.9986\nTraining tomo tomo_3c6038:  19%|█▉        | 3/16 [00:01<00:06,  2.10it/s]Epoch 1, Tomo tomo_3c6038, Batch 4/16, Loss: 0.9990\nTraining tomo tomo_3c6038:  25%|██▌       | 4/16 [00:01<00:04,  2.54it/s]Epoch 1, Tomo tomo_3c6038, Batch 5/16, Loss: 1.0000\nTraining tomo tomo_3c6038:  31%|███▏      | 5/16 [00:02<00:03,  2.85it/s]Epoch 1, Tomo tomo_3c6038, Batch 6/16, Loss: 0.9986\nTraining tomo tomo_3c6038:  38%|███▊      | 6/16 [00:02<00:03,  3.11it/s]Epoch 1, Tomo tomo_3c6038, Batch 7/16, Loss: 0.9989\nTraining tomo tomo_3c6038:  44%|████▍     | 7/16 [00:02<00:02,  3.26it/s]Epoch 1, Tomo tomo_3c6038, Batch 8/16, Loss: 0.9989\nTraining tomo tomo_3c6038:  50%|█████     | 8/16 [00:03<00:02,  3.40it/s]Epoch 1, Tomo tomo_3c6038, Batch 9/16, Loss: 0.9983\nTraining tomo tomo_3c6038:  56%|█████▋    | 9/16 [00:03<00:02,  3.48it/s]Epoch 1, Tomo tomo_3c6038, Batch 10/16, Loss: 0.9988\nTraining tomo tomo_3c6038:  62%|██████▎   | 10/16 [00:03<00:01,  3.56it/s]Epoch 1, Tomo tomo_3c6038, Batch 11/16, Loss: 0.9993\nTraining tomo tomo_3c6038:  69%|██████▉   | 11/16 [00:03<00:01,  3.57it/s]Epoch 1, Tomo tomo_3c6038, Batch 12/16, Loss: 0.9986\nTraining tomo tomo_3c6038:  75%|███████▌  | 12/16 [00:04<00:01,  3.62it/s]Epoch 1, Tomo tomo_3c6038, Batch 13/16, Loss: 0.9979\nTraining tomo tomo_3c6038:  81%|████████▏ | 13/16 [00:04<00:00,  3.64it/s]Epoch 1, Tomo tomo_3c6038, Batch 14/16, Loss: 0.9985\nTraining tomo tomo_3c6038:  88%|████████▊ | 14/16 [00:04<00:00,  3.62it/s]Epoch 1, Tomo tomo_3c6038, Batch 15/16, Loss: 0.9992\nTraining tomo tomo_3c6038:  94%|█████████▍| 15/16 [00:04<00:00,  3.63it/s]Epoch 1, Tomo tomo_3c6038, Batch 16/16, Loss: 0.9992\nTraining tomo tomo_3c6038: 100%|██████████| 16/16 [00:05<00:00,  3.06it/s]\nEpoch 1, Tomo tomo_3c6038 completed, Average Loss: 0.9988\nCompleted training on tomogram tomo_3c6038, Loss: 0.9988\nProcessed 1/287 tomograms\nTraining on tomogram tomo_369cce\nTraining tomo tomo_369cce:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_369cce, Batch 1/16, Loss: 0.9991\nTomo tomo_369cce, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.9950/13.5319\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_369cce:   6%|▋         | 1/16 [00:00<00:04,  3.64it/s]Epoch 1, Tomo tomo_369cce, Batch 2/16, Loss: 0.9991\nTomo tomo_369cce, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.0630/11.1758\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_369cce:  12%|█▎        | 2/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_369cce, Batch 3/16, Loss: 1.0000\nTraining tomo tomo_369cce:  19%|█▉        | 3/16 [00:00<00:03,  3.61it/s]Epoch 1, Tomo tomo_369cce, Batch 4/16, Loss: 0.9973\nTraining tomo tomo_369cce:  25%|██▌       | 4/16 [00:01<00:03,  3.63it/s]Epoch 1, Tomo tomo_369cce, Batch 5/16, Loss: 0.9986\nTraining tomo tomo_369cce:  31%|███▏      | 5/16 [00:01<00:03,  3.61it/s]Epoch 1, Tomo tomo_369cce, Batch 6/16, Loss: 0.9985\nTraining tomo tomo_369cce:  38%|███▊      | 6/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_369cce, Batch 7/16, Loss: 0.9984\nTraining tomo tomo_369cce:  44%|████▍     | 7/16 [00:01<00:02,  3.59it/s]Epoch 1, Tomo tomo_369cce, Batch 8/16, Loss: 0.9978\nTraining tomo tomo_369cce:  50%|█████     | 8/16 [00:02<00:02,  3.64it/s]Epoch 1, Tomo tomo_369cce, Batch 9/16, Loss: 0.9977\nTraining tomo tomo_369cce:  56%|█████▋    | 9/16 [00:02<00:01,  3.63it/s]Epoch 1, Tomo tomo_369cce, Batch 10/16, Loss: 0.9969\nTraining tomo tomo_369cce:  62%|██████▎   | 10/16 [00:02<00:01,  3.64it/s]Epoch 1, Tomo tomo_369cce, Batch 11/16, Loss: 0.9984\nTraining tomo tomo_369cce:  69%|██████▉   | 11/16 [00:03<00:01,  3.60it/s]Epoch 1, Tomo tomo_369cce, Batch 12/16, Loss: 0.9984\nTraining tomo tomo_369cce:  75%|███████▌  | 12/16 [00:03<00:01,  3.62it/s]Epoch 1, Tomo tomo_369cce, Batch 13/16, Loss: 0.9992\nTraining tomo tomo_369cce:  81%|████████▏ | 13/16 [00:03<00:00,  3.62it/s]Epoch 1, Tomo tomo_369cce, Batch 14/16, Loss: 1.0000\nTraining tomo tomo_369cce:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_369cce, Batch 15/16, Loss: 0.9991\nTraining tomo tomo_369cce:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_369cce, Batch 16/16, Loss: 0.9984\nTraining tomo tomo_369cce: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_369cce completed, Average Loss: 0.9986\nCompleted training on tomogram tomo_369cce, Loss: 0.9986\nProcessed 2/287 tomograms\nTraining on tomogram tomo_cff77a\nTraining tomo tomo_cff77a:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_cff77a, Batch 1/16, Loss: 0.9980\nTomo tomo_cff77a, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.5011/18.5208\nLoss: 0.9980\nGPU Memory: 0.17 GB\nTraining tomo tomo_cff77a:   6%|▋         | 1/16 [00:00<00:04,  3.55it/s]Epoch 1, Tomo tomo_cff77a, Batch 2/16, Loss: 0.9984\nTomo tomo_cff77a, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.6457/10.6817\nLoss: 0.9984\nGPU Memory: 0.17 GB\nTraining tomo tomo_cff77a:  12%|█▎        | 2/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_cff77a, Batch 3/16, Loss: 0.9975\nTraining tomo tomo_cff77a:  19%|█▉        | 3/16 [00:00<00:03,  3.60it/s]Epoch 1, Tomo tomo_cff77a, Batch 4/16, Loss: 0.9992\nTraining tomo tomo_cff77a:  25%|██▌       | 4/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_cff77a, Batch 5/16, Loss: 0.9981\nTraining tomo tomo_cff77a:  31%|███▏      | 5/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_cff77a, Batch 6/16, Loss: 1.0000\nTraining tomo tomo_cff77a:  38%|███▊      | 6/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_cff77a, Batch 7/16, Loss: 0.9975\nTraining tomo tomo_cff77a:  44%|████▍     | 7/16 [00:01<00:02,  3.63it/s]Epoch 1, Tomo tomo_cff77a, Batch 8/16, Loss: 0.9992\nTraining tomo tomo_cff77a:  50%|█████     | 8/16 [00:02<00:02,  3.67it/s]Epoch 1, Tomo tomo_cff77a, Batch 9/16, Loss: 0.9966\nTraining tomo tomo_cff77a:  56%|█████▋    | 9/16 [00:02<00:01,  3.64it/s]Epoch 1, Tomo tomo_cff77a, Batch 10/16, Loss: 0.9974\nTraining tomo tomo_cff77a:  62%|██████▎   | 10/16 [00:02<00:01,  3.68it/s]Epoch 1, Tomo tomo_cff77a, Batch 11/16, Loss: 0.9979\nTraining tomo tomo_cff77a:  69%|██████▉   | 11/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_cff77a, Batch 12/16, Loss: 0.9991\nTraining tomo tomo_cff77a:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_cff77a, Batch 13/16, Loss: 0.9982\nTraining tomo tomo_cff77a:  81%|████████▏ | 13/16 [00:03<00:00,  3.62it/s]Epoch 1, Tomo tomo_cff77a, Batch 14/16, Loss: 0.9982\nTraining tomo tomo_cff77a:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_cff77a, Batch 15/16, Loss: 1.0000\nTraining tomo tomo_cff77a:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_cff77a, Batch 16/16, Loss: 0.9991\nTraining tomo tomo_cff77a: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_cff77a completed, Average Loss: 0.9984\nCompleted training on tomogram tomo_cff77a, Loss: 0.9984\nProcessed 3/287 tomograms\nTraining on tomogram tomo_60d478\nTraining tomo tomo_60d478:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_60d478, Batch 1/16, Loss: 0.9991\nTomo tomo_60d478, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.1337/18.8293\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_60d478:   6%|▋         | 1/16 [00:00<00:04,  3.65it/s]Epoch 1, Tomo tomo_60d478, Batch 2/16, Loss: 0.9991\nTomo tomo_60d478, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.2489/16.1497\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_60d478:  12%|█▎        | 2/16 [00:00<00:03,  3.63it/s]Epoch 1, Tomo tomo_60d478, Batch 3/16, Loss: 0.9992\nTraining tomo tomo_60d478:  19%|█▉        | 3/16 [00:00<00:03,  3.61it/s]Epoch 1, Tomo tomo_60d478, Batch 4/16, Loss: 0.9988\nTraining tomo tomo_60d478:  25%|██▌       | 4/16 [00:01<00:03,  3.63it/s]Epoch 1, Tomo tomo_60d478, Batch 5/16, Loss: 0.9988\nTraining tomo tomo_60d478:  31%|███▏      | 5/16 [00:01<00:03,  3.65it/s]Epoch 1, Tomo tomo_60d478, Batch 6/16, Loss: 0.9976\nTraining tomo tomo_60d478:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_60d478, Batch 7/16, Loss: 0.9975\nTraining tomo tomo_60d478:  44%|████▍     | 7/16 [00:01<00:02,  3.64it/s]Epoch 1, Tomo tomo_60d478, Batch 8/16, Loss: 0.9975\nTraining tomo tomo_60d478:  50%|█████     | 8/16 [00:02<00:02,  3.66it/s]Epoch 1, Tomo tomo_60d478, Batch 9/16, Loss: 0.9982\nTraining tomo tomo_60d478:  56%|█████▋    | 9/16 [00:02<00:01,  3.62it/s]Epoch 1, Tomo tomo_60d478, Batch 10/16, Loss: 0.9974\nTraining tomo tomo_60d478:  62%|██████▎   | 10/16 [00:02<00:01,  3.64it/s]Epoch 1, Tomo tomo_60d478, Batch 11/16, Loss: 0.9982\nTraining tomo tomo_60d478:  69%|██████▉   | 11/16 [00:03<00:01,  3.62it/s]Epoch 1, Tomo tomo_60d478, Batch 12/16, Loss: 0.9991\nTraining tomo tomo_60d478:  75%|███████▌  | 12/16 [00:03<00:01,  3.64it/s]Epoch 1, Tomo tomo_60d478, Batch 13/16, Loss: 0.9991\nTraining tomo tomo_60d478:  81%|████████▏ | 13/16 [00:03<00:00,  3.63it/s]Epoch 1, Tomo tomo_60d478, Batch 14/16, Loss: 0.9995\nTraining tomo tomo_60d478:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_60d478, Batch 15/16, Loss: 0.9991\nTraining tomo tomo_60d478:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_60d478, Batch 16/16, Loss: 0.9973\nTraining tomo tomo_60d478: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_60d478 completed, Average Loss: 0.9985\nCompleted training on tomogram tomo_60d478, Loss: 0.9985\nProcessed 4/287 tomograms\nTraining on tomogram tomo_1c38fd\nTraining tomo tomo_1c38fd:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_1c38fd, Batch 1/16, Loss: 0.9974\nTomo tomo_1c38fd, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.7756/13.5860\nLoss: 0.9974\nGPU Memory: 0.17 GB\nTraining tomo tomo_1c38fd:   6%|▋         | 1/16 [00:00<00:04,  3.64it/s]Epoch 1, Tomo tomo_1c38fd, Batch 2/16, Loss: 0.9995\nTomo tomo_1c38fd, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.3370/14.7082\nLoss: 0.9995\nGPU Memory: 0.17 GB\nTraining tomo tomo_1c38fd:  12%|█▎        | 2/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_1c38fd, Batch 3/16, Loss: 0.9982\nTraining tomo tomo_1c38fd:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_1c38fd, Batch 4/16, Loss: 0.9982\nTraining tomo tomo_1c38fd:  25%|██▌       | 4/16 [00:01<00:03,  3.62it/s]Epoch 1, Tomo tomo_1c38fd, Batch 5/16, Loss: 0.9989\nTraining tomo tomo_1c38fd:  31%|███▏      | 5/16 [00:01<00:03,  3.64it/s]Epoch 1, Tomo tomo_1c38fd, Batch 6/16, Loss: 0.9964\nTraining tomo tomo_1c38fd:  38%|███▊      | 6/16 [00:01<00:02,  3.60it/s]Epoch 1, Tomo tomo_1c38fd, Batch 7/16, Loss: 0.9991\nTraining tomo tomo_1c38fd:  44%|████▍     | 7/16 [00:01<00:02,  3.65it/s]Epoch 1, Tomo tomo_1c38fd, Batch 8/16, Loss: 0.9982\nTraining tomo tomo_1c38fd:  50%|█████     | 8/16 [00:02<00:02,  3.63it/s]Epoch 1, Tomo tomo_1c38fd, Batch 9/16, Loss: 0.9991\nTraining tomo tomo_1c38fd:  56%|█████▋    | 9/16 [00:02<00:01,  3.65it/s]Epoch 1, Tomo tomo_1c38fd, Batch 10/16, Loss: 0.9963\nTraining tomo tomo_1c38fd:  62%|██████▎   | 10/16 [00:02<00:01,  3.65it/s]Epoch 1, Tomo tomo_1c38fd, Batch 11/16, Loss: 0.9982\nTraining tomo tomo_1c38fd:  69%|██████▉   | 11/16 [00:03<00:01,  3.63it/s]Epoch 1, Tomo tomo_1c38fd, Batch 12/16, Loss: 0.9981\nTraining tomo tomo_1c38fd:  75%|███████▌  | 12/16 [00:03<00:01,  3.65it/s]Epoch 1, Tomo tomo_1c38fd, Batch 13/16, Loss: 0.9991\nTraining tomo tomo_1c38fd:  81%|████████▏ | 13/16 [00:03<00:00,  3.65it/s]Epoch 1, Tomo tomo_1c38fd, Batch 14/16, Loss: 0.9981\nTraining tomo tomo_1c38fd:  88%|████████▊ | 14/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_1c38fd, Batch 15/16, Loss: 0.9972\nTraining tomo tomo_1c38fd:  94%|█████████▍| 15/16 [00:04<00:00,  3.64it/s]Epoch 1, Tomo tomo_1c38fd, Batch 16/16, Loss: 0.9991\nTraining tomo tomo_1c38fd: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\nEpoch 1, Tomo tomo_1c38fd completed, Average Loss: 0.9982\nCompleted training on tomogram tomo_1c38fd, Loss: 0.9982\nProcessed 5/287 tomograms\nCleaning memory for batch: ['tomo_3c6038', 'tomo_369cce', 'tomo_cff77a', 'tomo_60d478', 'tomo_1c38fd']\nProcessing batch of tomograms: ['tomo_98d455', 'tomo_4e38b8', 'tomo_b7d014', 'tomo_e57baf', 'tomo_4f379f']\nTomogram tomo_98d455 has 1 motors\nDownloading data for tomogram tomo_98d455\nPrepared patches for tomogram tomo_98d455\nTomogram tomo_4e38b8 has 1 motors\nDownloading data for tomogram tomo_4e38b8\nPrepared patches for tomogram tomo_4e38b8\nTomogram tomo_b7d014 has 2 motors\nDownloading data for tomogram tomo_b7d014\nPrepared patches for tomogram tomo_b7d014\nTomogram tomo_e57baf has 1 motors\nDownloading data for tomogram tomo_e57baf\nPrepared patches for tomogram tomo_e57baf\nTomogram tomo_4f379f has 1 motors\nDownloading data for tomogram tomo_4f379f\nPrepared patches for tomogram tomo_4f379f\nTraining on tomogram tomo_98d455\nTraining tomo tomo_98d455:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_98d455, Batch 1/16, Loss: 0.9996\nTomo tomo_98d455, Batch 0 load time: 0.07s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.2024/28.3582\nLoss: 0.9996\nGPU Memory: 0.17 GB\nTraining tomo tomo_98d455:   6%|▋         | 1/16 [00:00<00:04,  3.22it/s]Epoch 1, Tomo tomo_98d455, Batch 2/16, Loss: 0.9991\nTomo tomo_98d455, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.3123/27.1066\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_98d455:  12%|█▎        | 2/16 [00:00<00:04,  3.43it/s]Epoch 1, Tomo tomo_98d455, Batch 3/16, Loss: 0.9992\nTraining tomo tomo_98d455:  19%|█▉        | 3/16 [00:00<00:03,  3.49it/s]Epoch 1, Tomo tomo_98d455, Batch 4/16, Loss: 0.9979\nTraining tomo tomo_98d455:  25%|██▌       | 4/16 [00:01<00:03,  3.54it/s]Epoch 1, Tomo tomo_98d455, Batch 5/16, Loss: 0.9981\nTraining tomo tomo_98d455:  31%|███▏      | 5/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 6/16, Loss: 0.9982\nTraining tomo tomo_98d455:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 7/16, Loss: 0.9990\nTraining tomo tomo_98d455:  44%|████▍     | 7/16 [00:01<00:02,  3.64it/s]Epoch 1, Tomo tomo_98d455, Batch 8/16, Loss: 0.9992\nTraining tomo tomo_98d455:  50%|█████     | 8/16 [00:02<00:02,  3.62it/s]Epoch 1, Tomo tomo_98d455, Batch 9/16, Loss: 0.9989\nTraining tomo tomo_98d455:  56%|█████▋    | 9/16 [00:02<00:01,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 10/16, Loss: 0.9996\nTraining tomo tomo_98d455:  62%|██████▎   | 10/16 [00:02<00:01,  3.64it/s]Epoch 1, Tomo tomo_98d455, Batch 11/16, Loss: 0.9984\nTraining tomo tomo_98d455:  69%|██████▉   | 11/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_98d455, Batch 12/16, Loss: 0.9991\nTraining tomo tomo_98d455:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 13/16, Loss: 0.9976\nTraining tomo tomo_98d455:  81%|████████▏ | 13/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 14/16, Loss: 0.9991\nTraining tomo tomo_98d455:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 15/16, Loss: 0.9974\nTraining tomo tomo_98d455:  94%|█████████▍| 15/16 [00:04<00:00,  3.66it/s]Epoch 1, Tomo tomo_98d455, Batch 16/16, Loss: 0.9982\nTraining tomo tomo_98d455: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\nEpoch 1, Tomo tomo_98d455 completed, Average Loss: 0.9987\nCompleted training on tomogram tomo_98d455, Loss: 0.9987\nProcessed 6/287 tomograms\nTraining on tomogram tomo_4e38b8\nTraining tomo tomo_4e38b8:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_4e38b8, Batch 1/16, Loss: 0.9991\nTomo tomo_4e38b8, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.4305/15.1497\nLoss: 0.9991\nGPU Memory: 0.17 GB\nTraining tomo tomo_4e38b8:   6%|▋         | 1/16 [00:00<00:04,  3.64it/s]Epoch 1, Tomo tomo_4e38b8, Batch 2/16, Loss: 0.9969\nTomo tomo_4e38b8, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.0503/16.5951\nLoss: 0.9969\nGPU Memory: 0.17 GB\nTraining tomo tomo_4e38b8:  12%|█▎        | 2/16 [00:00<00:03,  3.61it/s]Epoch 1, Tomo tomo_4e38b8, Batch 3/16, Loss: 0.9965\nTraining tomo tomo_4e38b8:  19%|█▉        | 3/16 [00:00<00:03,  3.60it/s]Epoch 1, Tomo tomo_4e38b8, Batch 4/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  25%|██▌       | 4/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_4e38b8, Batch 5/16, Loss: 0.9973\nTraining tomo tomo_4e38b8:  31%|███▏      | 5/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_4e38b8, Batch 6/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  38%|███▊      | 6/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_4e38b8, Batch 7/16, Loss: 0.9966\nTraining tomo tomo_4e38b8:  44%|████▍     | 7/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_4e38b8, Batch 8/16, Loss: 0.9973\nTraining tomo tomo_4e38b8:  50%|█████     | 8/16 [00:02<00:02,  3.71it/s]Epoch 1, Tomo tomo_4e38b8, Batch 9/16, Loss: 0.9983\nTraining tomo tomo_4e38b8:  56%|█████▋    | 9/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_4e38b8, Batch 10/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  62%|██████▎   | 10/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_4e38b8, Batch 11/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  69%|██████▉   | 11/16 [00:02<00:01,  3.74it/s]Epoch 1, Tomo tomo_4e38b8, Batch 12/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  75%|███████▌  | 12/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_4e38b8, Batch 13/16, Loss: 1.0000\nTraining tomo tomo_4e38b8:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_4e38b8, Batch 14/16, Loss: 0.9991\nTraining tomo tomo_4e38b8:  88%|████████▊ | 14/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_4e38b8, Batch 15/16, Loss: 0.9981\nTraining tomo tomo_4e38b8:  94%|█████████▍| 15/16 [00:04<00:00,  3.72it/s]Epoch 1, Tomo tomo_4e38b8, Batch 16/16, Loss: 0.9972\nTraining tomo tomo_4e38b8: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\nEpoch 1, Tomo tomo_4e38b8 completed, Average Loss: 0.9982\nCompleted training on tomogram tomo_4e38b8, Loss: 0.9982\nProcessed 7/287 tomograms\nTraining on tomogram tomo_b7d014\nTraining tomo tomo_b7d014:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_b7d014, Batch 1/16, Loss: 1.0000\nTomo tomo_b7d014, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/0.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.0230/14.5808\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_b7d014:   6%|▋         | 1/16 [00:00<00:04,  3.56it/s]Epoch 1, Tomo tomo_b7d014, Batch 2/16, Loss: 0.9986\nTomo tomo_b7d014, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.0320/12.3515\nLoss: 0.9986\nGPU Memory: 0.17 GB\nTraining tomo tomo_b7d014:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_b7d014, Batch 3/16, Loss: 0.9984\nTraining tomo tomo_b7d014:  19%|█▉        | 3/16 [00:00<00:03,  3.58it/s]Epoch 1, Tomo tomo_b7d014, Batch 4/16, Loss: 0.9991\nTraining tomo tomo_b7d014:  25%|██▌       | 4/16 [00:01<00:03,  3.71it/s]Epoch 1, Tomo tomo_b7d014, Batch 5/16, Loss: 0.9976\nTraining tomo tomo_b7d014:  31%|███▏      | 5/16 [00:01<00:02,  3.76it/s]Epoch 1, Tomo tomo_b7d014, Batch 6/16, Loss: 0.9987\nTraining tomo tomo_b7d014:  38%|███▊      | 6/16 [00:01<00:02,  3.75it/s]Epoch 1, Tomo tomo_b7d014, Batch 7/16, Loss: 0.9974\nTraining tomo tomo_b7d014:  44%|████▍     | 7/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_b7d014, Batch 8/16, Loss: 0.9991\nTraining tomo tomo_b7d014:  50%|█████     | 8/16 [00:02<00:02,  3.72it/s]Epoch 1, Tomo tomo_b7d014, Batch 9/16, Loss: 0.9982\nTraining tomo tomo_b7d014:  56%|█████▋    | 9/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_b7d014, Batch 10/16, Loss: 0.9979\nTraining tomo tomo_b7d014:  62%|██████▎   | 10/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_b7d014, Batch 11/16, Loss: 0.9973\nTraining tomo tomo_b7d014:  69%|██████▉   | 11/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_b7d014, Batch 12/16, Loss: 0.9991\nTraining tomo tomo_b7d014:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_b7d014, Batch 13/16, Loss: 0.9991\nTraining tomo tomo_b7d014:  81%|████████▏ | 13/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_b7d014, Batch 14/16, Loss: 0.9965\nTraining tomo tomo_b7d014:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_b7d014, Batch 15/16, Loss: 0.9973\nTraining tomo tomo_b7d014:  94%|█████████▍| 15/16 [00:04<00:00,  3.71it/s]Epoch 1, Tomo tomo_b7d014, Batch 16/16, Loss: 0.9991\nTraining tomo tomo_b7d014: 100%|██████████| 16/16 [00:04<00:00,  3.70it/s]\nEpoch 1, Tomo tomo_b7d014 completed, Average Loss: 0.9984\nCompleted training on tomogram tomo_b7d014, Loss: 0.9984\nProcessed 8/287 tomograms\nTraining on tomogram tomo_e57baf\nTraining tomo tomo_e57baf:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_e57baf, Batch 1/16, Loss: 0.9981\nTomo tomo_e57baf, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.8581/15.5699\nLoss: 0.9981\nGPU Memory: 0.17 GB\nTraining tomo tomo_e57baf:   6%|▋         | 1/16 [00:00<00:04,  3.65it/s]Epoch 1, Tomo tomo_e57baf, Batch 2/16, Loss: 0.9981\nTomo tomo_e57baf, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.8145/12.4173\nLoss: 0.9981\nGPU Memory: 0.17 GB\nTraining tomo tomo_e57baf:  12%|█▎        | 2/16 [00:00<00:03,  3.63it/s]Epoch 1, Tomo tomo_e57baf, Batch 3/16, Loss: 0.9991\nTraining tomo tomo_e57baf:  19%|█▉        | 3/16 [00:00<00:03,  3.65it/s]Epoch 1, Tomo tomo_e57baf, Batch 4/16, Loss: 0.9971\nTraining tomo tomo_e57baf:  25%|██▌       | 4/16 [00:01<00:03,  3.69it/s]Epoch 1, Tomo tomo_e57baf, Batch 5/16, Loss: 0.9981\nTraining tomo tomo_e57baf:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_e57baf, Batch 6/16, Loss: 0.9962\nTraining tomo tomo_e57baf:  38%|███▊      | 6/16 [00:01<00:02,  3.64it/s]Epoch 1, Tomo tomo_e57baf, Batch 7/16, Loss: 0.9971\nTraining tomo tomo_e57baf:  44%|████▍     | 7/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_e57baf, Batch 8/16, Loss: 0.9971\nTraining tomo tomo_e57baf:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_e57baf, Batch 9/16, Loss: 0.9971\nTraining tomo tomo_e57baf:  56%|█████▋    | 9/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_e57baf, Batch 10/16, Loss: 0.9961\nTraining tomo tomo_e57baf:  62%|██████▎   | 10/16 [00:02<00:01,  3.68it/s]Epoch 1, Tomo tomo_e57baf, Batch 11/16, Loss: 0.9990\nTraining tomo tomo_e57baf:  69%|██████▉   | 11/16 [00:03<00:01,  3.61it/s]Epoch 1, Tomo tomo_e57baf, Batch 12/16, Loss: 1.0000\nTraining tomo tomo_e57baf:  75%|███████▌  | 12/16 [00:03<00:01,  3.64it/s]Epoch 1, Tomo tomo_e57baf, Batch 13/16, Loss: 0.9980\nTraining tomo tomo_e57baf:  81%|████████▏ | 13/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_e57baf, Batch 14/16, Loss: 1.0000\nTraining tomo tomo_e57baf:  88%|████████▊ | 14/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_e57baf, Batch 15/16, Loss: 1.0000\nTraining tomo tomo_e57baf:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_e57baf, Batch 16/16, Loss: 0.9980\nTraining tomo tomo_e57baf: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\nEpoch 1, Tomo tomo_e57baf completed, Average Loss: 0.9981\nCompleted training on tomogram tomo_e57baf, Loss: 0.9981\nProcessed 9/287 tomograms\nTraining on tomogram tomo_4f379f\nTraining tomo tomo_4f379f:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_4f379f, Batch 1/16, Loss: 0.9980\nTomo tomo_4f379f, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.6923/18.6899\nLoss: 0.9980\nGPU Memory: 0.17 GB\nTraining tomo tomo_4f379f:   6%|▋         | 1/16 [00:00<00:04,  3.64it/s]Epoch 1, Tomo tomo_4f379f, Batch 2/16, Loss: 0.9969\nTomo tomo_4f379f, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.6402/24.3906\nLoss: 0.9969\nGPU Memory: 0.17 GB\nTraining tomo tomo_4f379f:  12%|█▎        | 2/16 [00:00<00:03,  3.57it/s]Epoch 1, Tomo tomo_4f379f, Batch 3/16, Loss: 0.9980\nTraining tomo tomo_4f379f:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_4f379f, Batch 4/16, Loss: 0.9969\nTraining tomo tomo_4f379f:  25%|██▌       | 4/16 [00:01<00:03,  3.65it/s]Epoch 1, Tomo tomo_4f379f, Batch 5/16, Loss: 0.9969\nTraining tomo tomo_4f379f:  31%|███▏      | 5/16 [00:01<00:02,  3.74it/s]Epoch 1, Tomo tomo_4f379f, Batch 6/16, Loss: 0.9979\nTraining tomo tomo_4f379f:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_4f379f, Batch 7/16, Loss: 0.9990\nTraining tomo tomo_4f379f:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_4f379f, Batch 8/16, Loss: 1.0000\nTraining tomo tomo_4f379f:  50%|█████     | 8/16 [00:02<00:02,  3.70it/s]Epoch 1, Tomo tomo_4f379f, Batch 9/16, Loss: 0.9969\nTraining tomo tomo_4f379f:  56%|█████▋    | 9/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_4f379f, Batch 10/16, Loss: 0.9979\nTraining tomo tomo_4f379f:  62%|██████▎   | 10/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_4f379f, Batch 11/16, Loss: 0.9968\nTraining tomo tomo_4f379f:  69%|██████▉   | 11/16 [00:02<00:01,  3.74it/s]Epoch 1, Tomo tomo_4f379f, Batch 12/16, Loss: 0.9979\nTraining tomo tomo_4f379f:  75%|███████▌  | 12/16 [00:03<00:01,  3.75it/s]Epoch 1, Tomo tomo_4f379f, Batch 13/16, Loss: 0.9990\nTraining tomo tomo_4f379f:  81%|████████▏ | 13/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_4f379f, Batch 14/16, Loss: 0.9979\nTraining tomo tomo_4f379f:  88%|████████▊ | 14/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_4f379f, Batch 15/16, Loss: 0.9989\nTraining tomo tomo_4f379f:  94%|█████████▍| 15/16 [00:04<00:00,  3.74it/s]Epoch 1, Tomo tomo_4f379f, Batch 16/16, Loss: 0.9979\nTraining tomo tomo_4f379f: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\nEpoch 1, Tomo tomo_4f379f completed, Average Loss: 0.9979\nCompleted training on tomogram tomo_4f379f, Loss: 0.9979\nProcessed 10/287 tomograms\nCleaning memory for batch: ['tomo_98d455', 'tomo_4e38b8', 'tomo_b7d014', 'tomo_e57baf', 'tomo_4f379f']\nProcessing batch of tomograms: ['tomo_6e237a', 'tomo_fadbe2', 'tomo_6cf2df', 'tomo_eb4fd4', 'tomo_89d156']\nTomogram tomo_6e237a has 1 motors\nDownloading data for tomogram tomo_6e237a\nPrepared patches for tomogram tomo_6e237a\nTomogram tomo_fadbe2 has 1 motors\nDownloading data for tomogram tomo_fadbe2\nPrepared patches for tomogram tomo_fadbe2\nTomogram tomo_6cf2df has 2 motors\nDownloading data for tomogram tomo_6cf2df\nPrepared patches for tomogram tomo_6cf2df\nTomogram tomo_eb4fd4 has 1 motors\nDownloading data for tomogram tomo_eb4fd4\nPrepared patches for tomogram tomo_eb4fd4\nTomogram tomo_89d156 has 1 motors\nDownloading data for tomogram tomo_89d156\nPrepared patches for tomogram tomo_89d156\nTraining on tomogram tomo_6e237a\nTraining tomo tomo_6e237a:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_6e237a, Batch 1/16, Loss: 0.9969\nTomo tomo_6e237a, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.9300/22.0324\nLoss: 0.9969\nGPU Memory: 0.17 GB\nTraining tomo tomo_6e237a:   6%|▋         | 1/16 [00:00<00:04,  3.35it/s]Epoch 1, Tomo tomo_6e237a, Batch 2/16, Loss: 0.9990\nTomo tomo_6e237a, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.7959/23.6900\nLoss: 0.9990\nGPU Memory: 0.17 GB\nTraining tomo tomo_6e237a:  12%|█▎        | 2/16 [00:00<00:04,  3.46it/s]Epoch 1, Tomo tomo_6e237a, Batch 3/16, Loss: 0.9989\nTraining tomo tomo_6e237a:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_6e237a, Batch 4/16, Loss: 0.9979\nTraining tomo tomo_6e237a:  25%|██▌       | 4/16 [00:01<00:03,  3.68it/s]Epoch 1, Tomo tomo_6e237a, Batch 5/16, Loss: 0.9979\nTraining tomo tomo_6e237a:  31%|███▏      | 5/16 [00:01<00:02,  3.74it/s]Epoch 1, Tomo tomo_6e237a, Batch 6/16, Loss: 0.9969\nTraining tomo tomo_6e237a:  38%|███▊      | 6/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_6e237a, Batch 7/16, Loss: 0.9978\nTraining tomo tomo_6e237a:  44%|████▍     | 7/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_6e237a, Batch 8/16, Loss: 0.9979\nTraining tomo tomo_6e237a:  50%|█████     | 8/16 [00:02<00:02,  3.69it/s]Epoch 1, Tomo tomo_6e237a, Batch 9/16, Loss: 0.9989\nTraining tomo tomo_6e237a:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_6e237a, Batch 10/16, Loss: 0.9966\nTraining tomo tomo_6e237a:  62%|██████▎   | 10/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_6e237a, Batch 11/16, Loss: 0.9967\nTraining tomo tomo_6e237a:  69%|██████▉   | 11/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_6e237a, Batch 12/16, Loss: 0.9978\nTraining tomo tomo_6e237a:  75%|███████▌  | 12/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_6e237a, Batch 13/16, Loss: 0.9978\nTraining tomo tomo_6e237a:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_6e237a, Batch 14/16, Loss: 0.9978\nTraining tomo tomo_6e237a:  88%|████████▊ | 14/16 [00:03<00:00,  3.73it/s]Epoch 1, Tomo tomo_6e237a, Batch 15/16, Loss: 0.9978\nTraining tomo tomo_6e237a:  94%|█████████▍| 15/16 [00:04<00:00,  3.74it/s]Epoch 1, Tomo tomo_6e237a, Batch 16/16, Loss: 0.9989\nTraining tomo tomo_6e237a: 100%|██████████| 16/16 [00:04<00:00,  3.69it/s]\nEpoch 1, Tomo tomo_6e237a completed, Average Loss: 0.9978\nCompleted training on tomogram tomo_6e237a, Loss: 0.9978\nProcessed 11/287 tomograms\nTraining on tomogram tomo_fadbe2\nTraining tomo tomo_fadbe2:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_fadbe2, Batch 1/16, Loss: 0.9996\nTomo tomo_fadbe2, Batch 0 load time: 0.04s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.2071/37.7076\nLoss: 0.9996\nGPU Memory: 0.17 GB\nTraining tomo tomo_fadbe2:   6%|▋         | 1/16 [00:00<00:03,  3.81it/s]Epoch 1, Tomo tomo_fadbe2, Batch 2/16, Loss: 0.9983\nTomo tomo_fadbe2, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -1.8308/28.9234\nLoss: 0.9983\nGPU Memory: 0.17 GB\nTraining tomo tomo_fadbe2:  12%|█▎        | 2/16 [00:00<00:03,  3.69it/s]Epoch 1, Tomo tomo_fadbe2, Batch 3/16, Loss: 0.9989\nTraining tomo tomo_fadbe2:  19%|█▉        | 3/16 [00:00<00:03,  3.71it/s]Epoch 1, Tomo tomo_fadbe2, Batch 4/16, Loss: 0.9989\nTraining tomo tomo_fadbe2:  25%|██▌       | 4/16 [00:01<00:03,  3.67it/s]Epoch 1, Tomo tomo_fadbe2, Batch 5/16, Loss: 0.9968\nTraining tomo tomo_fadbe2:  31%|███▏      | 5/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_fadbe2, Batch 6/16, Loss: 0.9980\nTraining tomo tomo_fadbe2:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_fadbe2, Batch 7/16, Loss: 0.9992\nTraining tomo tomo_fadbe2:  44%|████▍     | 7/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_fadbe2, Batch 8/16, Loss: 0.9967\nTraining tomo tomo_fadbe2:  50%|█████     | 8/16 [00:02<00:02,  3.72it/s]Epoch 1, Tomo tomo_fadbe2, Batch 9/16, Loss: 0.9992\nTraining tomo tomo_fadbe2:  56%|█████▋    | 9/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_fadbe2, Batch 10/16, Loss: 0.9967\nTraining tomo tomo_fadbe2:  62%|██████▎   | 10/16 [00:02<00:01,  3.74it/s]Epoch 1, Tomo tomo_fadbe2, Batch 11/16, Loss: 0.9976\nTraining tomo tomo_fadbe2:  69%|██████▉   | 11/16 [00:02<00:01,  3.74it/s]Epoch 1, Tomo tomo_fadbe2, Batch 12/16, Loss: 0.9978\nTraining tomo tomo_fadbe2:  75%|███████▌  | 12/16 [00:03<00:01,  3.74it/s]Epoch 1, Tomo tomo_fadbe2, Batch 13/16, Loss: 0.9966\nTraining tomo tomo_fadbe2:  81%|████████▏ | 13/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_fadbe2, Batch 14/16, Loss: 0.9978\nTraining tomo tomo_fadbe2:  88%|████████▊ | 14/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_fadbe2, Batch 15/16, Loss: 0.9971\nTraining tomo tomo_fadbe2:  94%|█████████▍| 15/16 [00:04<00:00,  3.75it/s]Epoch 1, Tomo tomo_fadbe2, Batch 16/16, Loss: 0.9977\nTraining tomo tomo_fadbe2: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\nEpoch 1, Tomo tomo_fadbe2 completed, Average Loss: 0.9979\nCompleted training on tomogram tomo_fadbe2, Loss: 0.9979\nProcessed 12/287 tomograms\nTraining on tomogram tomo_6cf2df\nTraining tomo tomo_6cf2df:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_6cf2df, Batch 1/16, Loss: 0.9967\nTomo tomo_6cf2df, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.1459/21.3238\nLoss: 0.9967\nGPU Memory: 0.17 GB\nTraining tomo tomo_6cf2df:   6%|▋         | 1/16 [00:00<00:04,  3.56it/s]Epoch 1, Tomo tomo_6cf2df, Batch 2/16, Loss: 1.0000\nTomo tomo_6cf2df, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/0.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.8574/23.4406\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_6cf2df:  12%|█▎        | 2/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_6cf2df, Batch 3/16, Loss: 0.9975\nTraining tomo tomo_6cf2df:  19%|█▉        | 3/16 [00:00<00:03,  3.60it/s]Epoch 1, Tomo tomo_6cf2df, Batch 4/16, Loss: 0.9985\nTraining tomo tomo_6cf2df:  25%|██▌       | 4/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_6cf2df, Batch 5/16, Loss: 0.9981\nTraining tomo tomo_6cf2df:  31%|███▏      | 5/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_6cf2df, Batch 6/16, Loss: 0.9978\nTraining tomo tomo_6cf2df:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_6cf2df, Batch 7/16, Loss: 0.9977\nTraining tomo tomo_6cf2df:  44%|████▍     | 7/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_6cf2df, Batch 8/16, Loss: 0.9978\nTraining tomo tomo_6cf2df:  50%|█████     | 8/16 [00:02<00:02,  3.70it/s]Epoch 1, Tomo tomo_6cf2df, Batch 9/16, Loss: 0.9981\nTraining tomo tomo_6cf2df:  56%|█████▋    | 9/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_6cf2df, Batch 10/16, Loss: 0.9980\nTraining tomo tomo_6cf2df:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_6cf2df, Batch 11/16, Loss: 0.9989\nTraining tomo tomo_6cf2df:  69%|██████▉   | 11/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_6cf2df, Batch 12/16, Loss: 0.9989\nTraining tomo tomo_6cf2df:  75%|███████▌  | 12/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_6cf2df, Batch 13/16, Loss: 0.9978\nTraining tomo tomo_6cf2df:  81%|████████▏ | 13/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_6cf2df, Batch 14/16, Loss: 0.9968\nTraining tomo tomo_6cf2df:  88%|████████▊ | 14/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_6cf2df, Batch 15/16, Loss: 0.9966\nTraining tomo tomo_6cf2df:  94%|█████████▍| 15/16 [00:04<00:00,  3.70it/s]Epoch 1, Tomo tomo_6cf2df, Batch 16/16, Loss: 0.9979\nTraining tomo tomo_6cf2df: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_6cf2df completed, Average Loss: 0.9979\nCompleted training on tomogram tomo_6cf2df, Loss: 0.9979\nProcessed 13/287 tomograms\nTraining on tomogram tomo_eb4fd4\nTraining tomo tomo_eb4fd4:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 1/16, Loss: 0.9984\nTomo tomo_eb4fd4, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.6947/26.8206\nLoss: 0.9984\nGPU Memory: 0.17 GB\nTraining tomo tomo_eb4fd4:   6%|▋         | 1/16 [00:00<00:04,  3.56it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 2/16, Loss: 0.9989\nTomo tomo_eb4fd4, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.7788/22.6890\nLoss: 0.9989\nGPU Memory: 0.17 GB\nTraining tomo tomo_eb4fd4:  12%|█▎        | 2/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 3/16, Loss: 0.9967\nTraining tomo tomo_eb4fd4:  19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 4/16, Loss: 0.9966\nTraining tomo tomo_eb4fd4:  25%|██▌       | 4/16 [00:01<00:03,  3.60it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 5/16, Loss: 0.9977\nTraining tomo tomo_eb4fd4:  31%|███▏      | 5/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 6/16, Loss: 0.9989\nTraining tomo tomo_eb4fd4:  38%|███▊      | 6/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 7/16, Loss: 0.9977\nTraining tomo tomo_eb4fd4:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 8/16, Loss: 0.9966\nTraining tomo tomo_eb4fd4:  50%|█████     | 8/16 [00:02<00:02,  3.66it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 9/16, Loss: 0.9977\nTraining tomo tomo_eb4fd4:  56%|█████▋    | 9/16 [00:02<00:01,  3.66it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 10/16, Loss: 0.9976\nTraining tomo tomo_eb4fd4:  62%|██████▎   | 10/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 11/16, Loss: 0.9976\nTraining tomo tomo_eb4fd4:  69%|██████▉   | 11/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 12/16, Loss: 0.9988\nTraining tomo tomo_eb4fd4:  75%|███████▌  | 12/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 13/16, Loss: 0.9963\nTraining tomo tomo_eb4fd4:  81%|████████▏ | 13/16 [00:03<00:00,  3.73it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 14/16, Loss: 0.9988\nTraining tomo tomo_eb4fd4:  88%|████████▊ | 14/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 15/16, Loss: 0.9975\nTraining tomo tomo_eb4fd4:  94%|█████████▍| 15/16 [00:04<00:00,  3.74it/s]Epoch 1, Tomo tomo_eb4fd4, Batch 16/16, Loss: 0.9975\nTraining tomo tomo_eb4fd4: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_eb4fd4 completed, Average Loss: 0.9977\nCompleted training on tomogram tomo_eb4fd4, Loss: 0.9977\nProcessed 14/287 tomograms\nTraining on tomogram tomo_89d156\nTraining tomo tomo_89d156:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_89d156, Batch 1/16, Loss: 0.9995\nTomo tomo_89d156, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.8994/25.3093\nLoss: 0.9995\nGPU Memory: 0.17 GB\nTraining tomo tomo_89d156:   6%|▋         | 1/16 [00:00<00:04,  3.51it/s]Epoch 1, Tomo tomo_89d156, Batch 2/16, Loss: 0.9992\nTomo tomo_89d156, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.8082/27.6794\nLoss: 0.9992\nGPU Memory: 0.17 GB\nTraining tomo tomo_89d156:  12%|█▎        | 2/16 [00:00<00:03,  3.51it/s]Epoch 1, Tomo tomo_89d156, Batch 3/16, Loss: 0.9991\nTraining tomo tomo_89d156:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_89d156, Batch 4/16, Loss: 0.9977\nTraining tomo tomo_89d156:  25%|██▌       | 4/16 [00:01<00:03,  3.65it/s]Epoch 1, Tomo tomo_89d156, Batch 5/16, Loss: 0.9997\nTraining tomo tomo_89d156:  31%|███▏      | 5/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_89d156, Batch 6/16, Loss: 0.9964\nTraining tomo tomo_89d156:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_89d156, Batch 7/16, Loss: 0.9963\nTraining tomo tomo_89d156:  44%|████▍     | 7/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_89d156, Batch 8/16, Loss: 0.9988\nTraining tomo tomo_89d156:  50%|█████     | 8/16 [00:02<00:02,  3.71it/s]Epoch 1, Tomo tomo_89d156, Batch 9/16, Loss: 0.9963\nTraining tomo tomo_89d156:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_89d156, Batch 10/16, Loss: 0.9978\nTraining tomo tomo_89d156:  62%|██████▎   | 10/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_89d156, Batch 11/16, Loss: 0.9987\nTraining tomo tomo_89d156:  69%|██████▉   | 11/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_89d156, Batch 12/16, Loss: 1.0000\nTraining tomo tomo_89d156:  75%|███████▌  | 12/16 [00:03<00:01,  3.74it/s]Epoch 1, Tomo tomo_89d156, Batch 13/16, Loss: 0.9977\nTraining tomo tomo_89d156:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_89d156, Batch 14/16, Loss: 0.9975\nTraining tomo tomo_89d156:  88%|████████▊ | 14/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_89d156, Batch 15/16, Loss: 0.9950\nTraining tomo tomo_89d156:  94%|█████████▍| 15/16 [00:04<00:00,  3.69it/s]Epoch 1, Tomo tomo_89d156, Batch 16/16, Loss: 0.9974\nTraining tomo tomo_89d156: 100%|██████████| 16/16 [00:04<00:00,  3.69it/s]\nEpoch 1, Tomo tomo_89d156 completed, Average Loss: 0.9980\nCompleted training on tomogram tomo_89d156, Loss: 0.9980\nProcessed 15/287 tomograms\nCleaning memory for batch: ['tomo_6e237a', 'tomo_fadbe2', 'tomo_6cf2df', 'tomo_eb4fd4', 'tomo_89d156']\nProcessing batch of tomograms: ['tomo_dcb9b4', 'tomo_4077d8', 'tomo_568537', 'tomo_507b7a', 'tomo_db656f']\nTomogram tomo_dcb9b4 has 1 motors\nDownloading data for tomogram tomo_dcb9b4\nPrepared patches for tomogram tomo_dcb9b4\nTomogram tomo_4077d8 has 1 motors\nDownloading data for tomogram tomo_4077d8\nPrepared patches for tomogram tomo_4077d8\nTomogram tomo_568537 has 1 motors\nDownloading data for tomogram tomo_568537\nPrepared patches for tomogram tomo_568537\nTomogram tomo_507b7a has 2 motors\nDownloading data for tomogram tomo_507b7a\nPrepared patches for tomogram tomo_507b7a\nTomogram tomo_db656f has 1 motors\nDownloading data for tomogram tomo_db656f\nPrepared patches for tomogram tomo_db656f\nTraining on tomogram tomo_dcb9b4\nTraining tomo tomo_dcb9b4:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 1/16, Loss: 0.9974\nTomo tomo_dcb9b4, Batch 0 load time: 0.07s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.8310/23.6321\nLoss: 0.9974\nGPU Memory: 0.17 GB\nTraining tomo tomo_dcb9b4:   6%|▋         | 1/16 [00:00<00:04,  3.12it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 2/16, Loss: 0.9961\nTomo tomo_dcb9b4, Batch 1 load time: 0.08s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.3421/29.6758\nLoss: 0.9961\nGPU Memory: 0.17 GB\nTraining tomo tomo_dcb9b4:  12%|█▎        | 2/16 [00:00<00:04,  3.20it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 3/16, Loss: 0.9960\nTraining tomo tomo_dcb9b4:  19%|█▉        | 3/16 [00:00<00:03,  3.35it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 4/16, Loss: 0.9975\nTraining tomo tomo_dcb9b4:  25%|██▌       | 4/16 [00:01<00:03,  3.50it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 5/16, Loss: 0.9973\nTraining tomo tomo_dcb9b4:  31%|███▏      | 5/16 [00:01<00:03,  3.62it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 6/16, Loss: 0.9973\nTraining tomo tomo_dcb9b4:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 7/16, Loss: 0.9986\nTraining tomo tomo_dcb9b4:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 8/16, Loss: 0.9979\nTraining tomo tomo_dcb9b4:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 9/16, Loss: 0.9972\nTraining tomo tomo_dcb9b4:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 10/16, Loss: 0.9972\nTraining tomo tomo_dcb9b4:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 11/16, Loss: 0.9972\nTraining tomo tomo_dcb9b4:  69%|██████▉   | 11/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 12/16, Loss: 0.9972\nTraining tomo tomo_dcb9b4:  75%|███████▌  | 12/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 13/16, Loss: 0.9972\nTraining tomo tomo_dcb9b4:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 14/16, Loss: 0.9986\nTraining tomo tomo_dcb9b4:  88%|████████▊ | 14/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 15/16, Loss: 0.9986\nTraining tomo tomo_dcb9b4:  94%|█████████▍| 15/16 [00:04<00:00,  3.72it/s]Epoch 1, Tomo tomo_dcb9b4, Batch 16/16, Loss: 0.9957\nTraining tomo tomo_dcb9b4: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\nEpoch 1, Tomo tomo_dcb9b4 completed, Average Loss: 0.9973\nCompleted training on tomogram tomo_dcb9b4, Loss: 0.9973\nProcessed 16/287 tomograms\nTraining on tomogram tomo_4077d8\nTraining tomo tomo_4077d8:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_4077d8, Batch 1/16, Loss: 0.9976\nTomo tomo_4077d8, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.7369/36.4624\nLoss: 0.9976\nGPU Memory: 0.17 GB\nTraining tomo tomo_4077d8:   6%|▋         | 1/16 [00:00<00:04,  3.51it/s]Epoch 1, Tomo tomo_4077d8, Batch 2/16, Loss: 0.9986\nTomo tomo_4077d8, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -2.9404/41.0907\nLoss: 0.9986\nGPU Memory: 0.17 GB\nTraining tomo tomo_4077d8:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_4077d8, Batch 3/16, Loss: 0.9956\nTraining tomo tomo_4077d8:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_4077d8, Batch 4/16, Loss: 0.9985\nTraining tomo tomo_4077d8:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_4077d8, Batch 5/16, Loss: 0.9941\nTraining tomo tomo_4077d8:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_4077d8, Batch 6/16, Loss: 0.9971\nTraining tomo tomo_4077d8:  38%|███▊      | 6/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_4077d8, Batch 7/16, Loss: 0.9955\nTraining tomo tomo_4077d8:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_4077d8, Batch 8/16, Loss: 0.9971\nTraining tomo tomo_4077d8:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_4077d8, Batch 9/16, Loss: 0.9969\nTraining tomo tomo_4077d8:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_4077d8, Batch 10/16, Loss: 0.9953\nTraining tomo tomo_4077d8:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_4077d8, Batch 11/16, Loss: 0.9985\nTraining tomo tomo_4077d8:  69%|██████▉   | 11/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_4077d8, Batch 12/16, Loss: 0.9984\nTraining tomo tomo_4077d8:  75%|███████▌  | 12/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_4077d8, Batch 13/16, Loss: 0.9968\nTraining tomo tomo_4077d8:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_4077d8, Batch 14/16, Loss: 0.9969\nTraining tomo tomo_4077d8:  88%|████████▊ | 14/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_4077d8, Batch 15/16, Loss: 0.9968\nTraining tomo tomo_4077d8:  94%|█████████▍| 15/16 [00:04<00:00,  3.69it/s]Epoch 1, Tomo tomo_4077d8, Batch 16/16, Loss: 0.9984\nTraining tomo tomo_4077d8: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\nEpoch 1, Tomo tomo_4077d8 completed, Average Loss: 0.9970\nCompleted training on tomogram tomo_4077d8, Loss: 0.9970\nProcessed 17/287 tomograms\nTraining on tomogram tomo_568537\nTraining tomo tomo_568537:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_568537, Batch 1/16, Loss: 0.9988\nTomo tomo_568537, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.0652/41.9292\nLoss: 0.9988\nGPU Memory: 0.17 GB\nTraining tomo tomo_568537:   6%|▋         | 1/16 [00:00<00:04,  3.63it/s]Epoch 1, Tomo tomo_568537, Batch 2/16, Loss: 0.9984\nTomo tomo_568537, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.5748/34.8882\nLoss: 0.9984\nGPU Memory: 0.17 GB\nTraining tomo tomo_568537:  12%|█▎        | 2/16 [00:00<00:03,  3.55it/s]Epoch 1, Tomo tomo_568537, Batch 3/16, Loss: 0.9985\nTraining tomo tomo_568537:  19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_568537, Batch 4/16, Loss: 0.9969\nTraining tomo tomo_568537:  25%|██▌       | 4/16 [00:01<00:03,  3.64it/s]Epoch 1, Tomo tomo_568537, Batch 5/16, Loss: 0.9955\nTraining tomo tomo_568537:  31%|███▏      | 5/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_568537, Batch 6/16, Loss: 0.9970\nTraining tomo tomo_568537:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_568537, Batch 7/16, Loss: 0.9969\nTraining tomo tomo_568537:  44%|████▍     | 7/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_568537, Batch 8/16, Loss: 1.0000\nTraining tomo tomo_568537:  50%|█████     | 8/16 [00:02<00:02,  3.69it/s]Epoch 1, Tomo tomo_568537, Batch 9/16, Loss: 0.9937\nTraining tomo tomo_568537:  56%|█████▋    | 9/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_568537, Batch 10/16, Loss: 0.9936\nTraining tomo tomo_568537:  62%|██████▎   | 10/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_568537, Batch 11/16, Loss: 0.9969\nTraining tomo tomo_568537:  69%|██████▉   | 11/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_568537, Batch 12/16, Loss: 0.9984\nTraining tomo tomo_568537:  75%|███████▌  | 12/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_568537, Batch 13/16, Loss: 0.9967\nTraining tomo tomo_568537:  81%|████████▏ | 13/16 [00:03<00:00,  3.73it/s]Epoch 1, Tomo tomo_568537, Batch 14/16, Loss: 0.9951\nTraining tomo tomo_568537:  88%|████████▊ | 14/16 [00:03<00:00,  3.73it/s]Epoch 1, Tomo tomo_568537, Batch 15/16, Loss: 0.9966\nTraining tomo tomo_568537:  94%|█████████▍| 15/16 [00:04<00:00,  3.74it/s]Epoch 1, Tomo tomo_568537, Batch 16/16, Loss: 0.9966\nTraining tomo tomo_568537: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_568537 completed, Average Loss: 0.9969\nCompleted training on tomogram tomo_568537, Loss: 0.9969\nProcessed 18/287 tomograms\nTraining on tomogram tomo_507b7a\nTraining tomo tomo_507b7a:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_507b7a, Batch 1/16, Loss: 0.9969\nTomo tomo_507b7a, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.8693/34.9160\nLoss: 0.9969\nGPU Memory: 0.17 GB\nTraining tomo tomo_507b7a:   6%|▋         | 1/16 [00:00<00:04,  3.62it/s]Epoch 1, Tomo tomo_507b7a, Batch 2/16, Loss: 0.9961\nTomo tomo_507b7a, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -3.5240/30.9043\nLoss: 0.9961\nGPU Memory: 0.17 GB\nTraining tomo tomo_507b7a:  12%|█▎        | 2/16 [00:00<00:03,  3.58it/s]Epoch 1, Tomo tomo_507b7a, Batch 3/16, Loss: 0.9953\nTraining tomo tomo_507b7a:  19%|█▉        | 3/16 [00:00<00:03,  3.60it/s]Epoch 1, Tomo tomo_507b7a, Batch 4/16, Loss: 0.9950\nTraining tomo tomo_507b7a:  25%|██▌       | 4/16 [00:01<00:03,  3.74it/s]Epoch 1, Tomo tomo_507b7a, Batch 5/16, Loss: 1.0000\nTraining tomo tomo_507b7a:  31%|███▏      | 5/16 [00:01<00:02,  3.78it/s]Epoch 1, Tomo tomo_507b7a, Batch 6/16, Loss: 0.9938\nTraining tomo tomo_507b7a:  38%|███▊      | 6/16 [00:01<00:02,  3.76it/s]Epoch 1, Tomo tomo_507b7a, Batch 7/16, Loss: 0.9969\nTraining tomo tomo_507b7a:  44%|████▍     | 7/16 [00:01<00:02,  3.74it/s]Epoch 1, Tomo tomo_507b7a, Batch 8/16, Loss: 0.9967\nTraining tomo tomo_507b7a:  50%|█████     | 8/16 [00:02<00:02,  3.70it/s]Epoch 1, Tomo tomo_507b7a, Batch 9/16, Loss: 0.9967\nTraining tomo tomo_507b7a:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_507b7a, Batch 10/16, Loss: 0.9967\nTraining tomo tomo_507b7a:  62%|██████▎   | 10/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_507b7a, Batch 11/16, Loss: 0.9983\nTraining tomo tomo_507b7a:  69%|██████▉   | 11/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_507b7a, Batch 12/16, Loss: 0.9965\nTraining tomo tomo_507b7a:  75%|███████▌  | 12/16 [00:03<00:01,  3.74it/s]Epoch 1, Tomo tomo_507b7a, Batch 13/16, Loss: 0.9965\nTraining tomo tomo_507b7a:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_507b7a, Batch 14/16, Loss: 0.9966\nTraining tomo tomo_507b7a:  88%|████████▊ | 14/16 [00:03<00:00,  3.73it/s]Epoch 1, Tomo tomo_507b7a, Batch 15/16, Loss: 1.0000\nTraining tomo tomo_507b7a:  94%|█████████▍| 15/16 [00:04<00:00,  3.73it/s]Epoch 1, Tomo tomo_507b7a, Batch 16/16, Loss: 0.9964\nTraining tomo tomo_507b7a: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\nEpoch 1, Tomo tomo_507b7a completed, Average Loss: 0.9968\nCompleted training on tomogram tomo_507b7a, Loss: 0.9968\nProcessed 19/287 tomograms\nTraining on tomogram tomo_db656f\nTraining tomo tomo_db656f:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_db656f, Batch 1/16, Loss: 0.9992\nTomo tomo_db656f, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.0777/39.2772\nLoss: 0.9992\nGPU Memory: 0.17 GB\nTraining tomo tomo_db656f:   6%|▋         | 1/16 [00:00<00:04,  3.54it/s]Epoch 1, Tomo tomo_db656f, Batch 2/16, Loss: 0.9964\nTomo tomo_db656f, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.4905/39.5783\nLoss: 0.9964\nGPU Memory: 0.17 GB\nTraining tomo tomo_db656f:  12%|█▎        | 2/16 [00:00<00:04,  3.50it/s]Epoch 1, Tomo tomo_db656f, Batch 3/16, Loss: 0.9965\nTraining tomo tomo_db656f:  19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]Epoch 1, Tomo tomo_db656f, Batch 4/16, Loss: 0.9948\nTraining tomo tomo_db656f:  25%|██▌       | 4/16 [00:01<00:03,  3.63it/s]Epoch 1, Tomo tomo_db656f, Batch 5/16, Loss: 0.9982\nTraining tomo tomo_db656f:  31%|███▏      | 5/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_db656f, Batch 6/16, Loss: 0.9947\nTraining tomo tomo_db656f:  38%|███▊      | 6/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_db656f, Batch 7/16, Loss: 0.9964\nTraining tomo tomo_db656f:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_db656f, Batch 8/16, Loss: 0.9974\nTraining tomo tomo_db656f:  50%|█████     | 8/16 [00:02<00:02,  3.70it/s]Epoch 1, Tomo tomo_db656f, Batch 9/16, Loss: 0.9982\nTraining tomo tomo_db656f:  56%|█████▋    | 9/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_db656f, Batch 10/16, Loss: 0.9963\nTraining tomo tomo_db656f:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_db656f, Batch 11/16, Loss: 0.9963\nTraining tomo tomo_db656f:  69%|██████▉   | 11/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_db656f, Batch 12/16, Loss: 0.9945\nTraining tomo tomo_db656f:  75%|███████▌  | 12/16 [00:03<00:01,  3.73it/s]Epoch 1, Tomo tomo_db656f, Batch 13/16, Loss: 0.9962\nTraining tomo tomo_db656f:  81%|████████▏ | 13/16 [00:03<00:00,  3.74it/s]Epoch 1, Tomo tomo_db656f, Batch 14/16, Loss: 0.9943\nTraining tomo tomo_db656f:  88%|████████▊ | 14/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_db656f, Batch 15/16, Loss: 0.9980\nTraining tomo tomo_db656f:  94%|█████████▍| 15/16 [00:04<00:00,  3.71it/s]Epoch 1, Tomo tomo_db656f, Batch 16/16, Loss: 0.9961\nTraining tomo tomo_db656f: 100%|██████████| 16/16 [00:04<00:00,  3.69it/s]\nEpoch 1, Tomo tomo_db656f completed, Average Loss: 0.9965\nCompleted training on tomogram tomo_db656f, Loss: 0.9965\nProcessed 20/287 tomograms\nCleaning memory for batch: ['tomo_dcb9b4', 'tomo_4077d8', 'tomo_568537', 'tomo_507b7a', 'tomo_db656f']\nProcessing batch of tomograms: ['tomo_2daaee', 'tomo_d2b1bc', 'tomo_05df8a', 'tomo_6df2d6', 'tomo_0c3a99']\nTomogram tomo_2daaee has 4 motors\nDownloading data for tomogram tomo_2daaee\nPrepared patches for tomogram tomo_2daaee\nTomogram tomo_d2b1bc has 1 motors\nDownloading data for tomogram tomo_d2b1bc\nPrepared patches for tomogram tomo_d2b1bc\nTomogram tomo_05df8a has 1 motors\nDownloading data for tomogram tomo_05df8a\nPrepared patches for tomogram tomo_05df8a\nTomogram tomo_6df2d6 has 1 motors\nDownloading data for tomogram tomo_6df2d6\nPrepared patches for tomogram tomo_6df2d6\nTomogram tomo_0c3a99 has 1 motors\nDownloading data for tomogram tomo_0c3a99\nPrepared patches for tomogram tomo_0c3a99\nTraining on tomogram tomo_2daaee\nTraining tomo tomo_2daaee:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_2daaee, Batch 1/16, Loss: 0.9933\nTomo tomo_2daaee, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.2066/55.1662\nLoss: 0.9933\nGPU Memory: 0.17 GB\nTraining tomo tomo_2daaee:   6%|▋         | 1/16 [00:00<00:04,  3.44it/s]Epoch 1, Tomo tomo_2daaee, Batch 2/16, Loss: 0.9895\nTomo tomo_2daaee, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.5812/53.2687\nLoss: 0.9895\nGPU Memory: 0.17 GB\nTraining tomo tomo_2daaee:  12%|█▎        | 2/16 [00:00<00:04,  3.48it/s]Epoch 1, Tomo tomo_2daaee, Batch 3/16, Loss: 0.9844\nTraining tomo tomo_2daaee:  19%|█▉        | 3/16 [00:00<00:03,  3.51it/s]Epoch 1, Tomo tomo_2daaee, Batch 4/16, Loss: 1.0000\nTraining tomo tomo_2daaee:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_2daaee, Batch 5/16, Loss: 0.9842\nTraining tomo tomo_2daaee:  31%|███▏      | 5/16 [00:01<00:03,  3.65it/s]Epoch 1, Tomo tomo_2daaee, Batch 6/16, Loss: 0.9918\nTraining tomo tomo_2daaee:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_2daaee, Batch 7/16, Loss: 0.9879\nTraining tomo tomo_2daaee:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_2daaee, Batch 8/16, Loss: 0.9979\nTraining tomo tomo_2daaee:  50%|█████     | 8/16 [00:02<00:02,  3.63it/s]Epoch 1, Tomo tomo_2daaee, Batch 9/16, Loss: 0.9964\nTraining tomo tomo_2daaee:  56%|█████▋    | 9/16 [00:02<00:01,  3.66it/s]Epoch 1, Tomo tomo_2daaee, Batch 10/16, Loss: 0.9911\nTraining tomo tomo_2daaee:  62%|██████▎   | 10/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_2daaee, Batch 11/16, Loss: 0.9870\nTraining tomo tomo_2daaee:  69%|██████▉   | 11/16 [00:03<00:01,  3.63it/s]Epoch 1, Tomo tomo_2daaee, Batch 12/16, Loss: 0.9911\nTraining tomo tomo_2daaee:  75%|███████▌  | 12/16 [00:03<00:01,  3.65it/s]Epoch 1, Tomo tomo_2daaee, Batch 13/16, Loss: 0.9807\nTraining tomo tomo_2daaee:  81%|████████▏ | 13/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_2daaee, Batch 14/16, Loss: 0.9833\nTraining tomo tomo_2daaee:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_2daaee, Batch 15/16, Loss: 0.9920\nTraining tomo tomo_2daaee:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_2daaee, Batch 16/16, Loss: 0.9964\nTraining tomo tomo_2daaee: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\nEpoch 1, Tomo tomo_2daaee completed, Average Loss: 0.9904\nCompleted training on tomogram tomo_2daaee, Loss: 0.9904\nProcessed 21/287 tomograms\nTraining on tomogram tomo_d2b1bc\nTraining tomo tomo_d2b1bc:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 1/16, Loss: 0.9990\nTomo tomo_d2b1bc, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.9054/68.4916\nLoss: 0.9990\nGPU Memory: 0.17 GB\nTraining tomo tomo_d2b1bc:   6%|▋         | 1/16 [00:00<00:04,  3.57it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 2/16, Loss: 0.9980\nTomo tomo_d2b1bc, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.8564/75.5131\nLoss: 0.9980\nGPU Memory: 0.17 GB\nTraining tomo tomo_d2b1bc:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 3/16, Loss: 0.9960\nTraining tomo tomo_d2b1bc:  19%|█▉        | 3/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 4/16, Loss: 0.9959\nTraining tomo tomo_d2b1bc:  25%|██▌       | 4/16 [00:01<00:03,  3.60it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 5/16, Loss: 0.9987\nTraining tomo tomo_d2b1bc:  31%|███▏      | 5/16 [00:01<00:03,  3.61it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 6/16, Loss: 0.9970\nTraining tomo tomo_d2b1bc:  38%|███▊      | 6/16 [00:01<00:02,  3.64it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 7/16, Loss: 0.9983\nTraining tomo tomo_d2b1bc:  44%|████▍     | 7/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 8/16, Loss: 0.9960\nTraining tomo tomo_d2b1bc:  50%|█████     | 8/16 [00:02<00:02,  3.63it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 9/16, Loss: 0.9941\nTraining tomo tomo_d2b1bc:  56%|█████▋    | 9/16 [00:02<00:01,  3.63it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 10/16, Loss: 0.9981\nTraining tomo tomo_d2b1bc:  62%|██████▎   | 10/16 [00:02<00:01,  3.65it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 11/16, Loss: 0.9942\nTraining tomo tomo_d2b1bc:  69%|██████▉   | 11/16 [00:03<00:01,  3.63it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 12/16, Loss: 0.9980\nTraining tomo tomo_d2b1bc:  75%|███████▌  | 12/16 [00:03<00:01,  3.65it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 13/16, Loss: 0.9940\nTraining tomo tomo_d2b1bc:  81%|████████▏ | 13/16 [00:03<00:00,  3.64it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 14/16, Loss: 0.9958\nTraining tomo tomo_d2b1bc:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 15/16, Loss: 0.9978\nTraining tomo tomo_d2b1bc:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_d2b1bc, Batch 16/16, Loss: 0.9978\nTraining tomo tomo_d2b1bc: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_d2b1bc completed, Average Loss: 0.9968\nCompleted training on tomogram tomo_d2b1bc, Loss: 0.9968\nProcessed 22/287 tomograms\nTraining on tomogram tomo_05df8a\nTraining tomo tomo_05df8a:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_05df8a, Batch 1/16, Loss: 0.9948\nTomo tomo_05df8a, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.3522/106.4552\nLoss: 0.9948\nGPU Memory: 0.17 GB\nTraining tomo tomo_05df8a:   6%|▋         | 1/16 [00:00<00:04,  3.55it/s]Epoch 1, Tomo tomo_05df8a, Batch 2/16, Loss: 0.9977\nTomo tomo_05df8a, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.2481/77.1315\nLoss: 0.9977\nGPU Memory: 0.17 GB\nTraining tomo tomo_05df8a:  12%|█▎        | 2/16 [00:00<00:03,  3.50it/s]Epoch 1, Tomo tomo_05df8a, Batch 3/16, Loss: 0.9985\nTraining tomo tomo_05df8a:  19%|█▉        | 3/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_05df8a, Batch 4/16, Loss: 0.9953\nTraining tomo tomo_05df8a:  25%|██▌       | 4/16 [00:01<00:03,  3.61it/s]Epoch 1, Tomo tomo_05df8a, Batch 5/16, Loss: 0.9932\nTraining tomo tomo_05df8a:  31%|███▏      | 5/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_05df8a, Batch 6/16, Loss: 0.9954\nTraining tomo tomo_05df8a:  38%|███▊      | 6/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_05df8a, Batch 7/16, Loss: 0.9938\nTraining tomo tomo_05df8a:  44%|████▍     | 7/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_05df8a, Batch 8/16, Loss: 0.9955\nTraining tomo tomo_05df8a:  50%|█████     | 8/16 [00:02<00:02,  3.69it/s]Epoch 1, Tomo tomo_05df8a, Batch 9/16, Loss: 0.9960\nTraining tomo tomo_05df8a:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_05df8a, Batch 10/16, Loss: 0.9934\nTraining tomo tomo_05df8a:  62%|██████▎   | 10/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_05df8a, Batch 11/16, Loss: 0.9935\nTraining tomo tomo_05df8a:  69%|██████▉   | 11/16 [00:03<00:01,  3.70it/s]Epoch 1, Tomo tomo_05df8a, Batch 12/16, Loss: 0.9975\nTraining tomo tomo_05df8a:  75%|███████▌  | 12/16 [00:03<00:01,  3.70it/s]Epoch 1, Tomo tomo_05df8a, Batch 13/16, Loss: 0.9975\nTraining tomo tomo_05df8a:  81%|████████▏ | 13/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_05df8a, Batch 14/16, Loss: 0.9952\nTraining tomo tomo_05df8a:  88%|████████▊ | 14/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_05df8a, Batch 15/16, Loss: 0.9953\nTraining tomo tomo_05df8a:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_05df8a, Batch 16/16, Loss: 0.9960\nTraining tomo tomo_05df8a: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\nEpoch 1, Tomo tomo_05df8a completed, Average Loss: 0.9955\nCompleted training on tomogram tomo_05df8a, Loss: 0.9955\nProcessed 23/287 tomograms\nTraining on tomogram tomo_6df2d6\nTraining tomo tomo_6df2d6:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_6df2d6, Batch 1/16, Loss: 0.9978\nTomo tomo_6df2d6, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.6794/66.5051\nLoss: 0.9978\nGPU Memory: 0.17 GB\nTraining tomo tomo_6df2d6:   6%|▋         | 1/16 [00:00<00:04,  3.48it/s]Epoch 1, Tomo tomo_6df2d6, Batch 2/16, Loss: 0.9953\nTomo tomo_6df2d6, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.1280/77.3707\nLoss: 0.9953\nGPU Memory: 0.17 GB\nTraining tomo tomo_6df2d6:  12%|█▎        | 2/16 [00:00<00:04,  3.50it/s]Epoch 1, Tomo tomo_6df2d6, Batch 3/16, Loss: 0.9934\nTraining tomo tomo_6df2d6:  19%|█▉        | 3/16 [00:00<00:03,  3.49it/s]Epoch 1, Tomo tomo_6df2d6, Batch 4/16, Loss: 0.9969\nTraining tomo tomo_6df2d6:  25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]Epoch 1, Tomo tomo_6df2d6, Batch 5/16, Loss: 0.9931\nTraining tomo tomo_6df2d6:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_6df2d6, Batch 6/16, Loss: 0.9976\nTraining tomo tomo_6df2d6:  38%|███▊      | 6/16 [00:01<00:02,  3.65it/s]Epoch 1, Tomo tomo_6df2d6, Batch 7/16, Loss: 0.9930\nTraining tomo tomo_6df2d6:  44%|████▍     | 7/16 [00:01<00:02,  3.65it/s]Epoch 1, Tomo tomo_6df2d6, Batch 8/16, Loss: 0.9953\nTraining tomo tomo_6df2d6:  50%|█████     | 8/16 [00:02<00:02,  3.64it/s]Epoch 1, Tomo tomo_6df2d6, Batch 9/16, Loss: 0.9952\nTraining tomo tomo_6df2d6:  56%|█████▋    | 9/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_6df2d6, Batch 10/16, Loss: 0.9906\nTraining tomo tomo_6df2d6:  62%|██████▎   | 10/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_6df2d6, Batch 11/16, Loss: 0.9952\nTraining tomo tomo_6df2d6:  69%|██████▉   | 11/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_6df2d6, Batch 12/16, Loss: 0.9952\nTraining tomo tomo_6df2d6:  75%|███████▌  | 12/16 [00:03<00:01,  3.68it/s]Epoch 1, Tomo tomo_6df2d6, Batch 13/16, Loss: 0.9963\nTraining tomo tomo_6df2d6:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_6df2d6, Batch 14/16, Loss: 1.0000\nTraining tomo tomo_6df2d6:  88%|████████▊ | 14/16 [00:03<00:00,  3.65it/s]Epoch 1, Tomo tomo_6df2d6, Batch 15/16, Loss: 0.9949\nTraining tomo tomo_6df2d6:  94%|█████████▍| 15/16 [00:04<00:00,  3.65it/s]Epoch 1, Tomo tomo_6df2d6, Batch 16/16, Loss: 0.9975\nTraining tomo tomo_6df2d6: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_6df2d6 completed, Average Loss: 0.9955\nCompleted training on tomogram tomo_6df2d6, Loss: 0.9955\nProcessed 24/287 tomograms\nTraining on tomogram tomo_0c3a99\nTraining tomo tomo_0c3a99:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_0c3a99, Batch 1/16, Loss: 1.0000\nTomo tomo_0c3a99, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/0.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.8907/63.5070\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_0c3a99:   6%|▋         | 1/16 [00:00<00:04,  3.53it/s]Epoch 1, Tomo tomo_0c3a99, Batch 2/16, Loss: 0.9900\nTomo tomo_0c3a99, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.3367/67.8537\nLoss: 0.9900\nGPU Memory: 0.17 GB\nTraining tomo tomo_0c3a99:  12%|█▎        | 2/16 [00:00<00:03,  3.52it/s]Epoch 1, Tomo tomo_0c3a99, Batch 3/16, Loss: 0.9903\nTraining tomo tomo_0c3a99:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_0c3a99, Batch 4/16, Loss: 0.9954\nTraining tomo tomo_0c3a99:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_0c3a99, Batch 5/16, Loss: 0.9973\nTraining tomo tomo_0c3a99:  31%|███▏      | 5/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_0c3a99, Batch 6/16, Loss: 0.9919\nTraining tomo tomo_0c3a99:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_0c3a99, Batch 7/16, Loss: 0.9918\nTraining tomo tomo_0c3a99:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_0c3a99, Batch 8/16, Loss: 0.9921\nTraining tomo tomo_0c3a99:  50%|█████     | 8/16 [00:02<00:02,  3.66it/s]Epoch 1, Tomo tomo_0c3a99, Batch 9/16, Loss: 0.9973\nTraining tomo tomo_0c3a99:  56%|█████▋    | 9/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_0c3a99, Batch 10/16, Loss: 0.9946\nTraining tomo tomo_0c3a99:  62%|██████▎   | 10/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_0c3a99, Batch 11/16, Loss: 0.9972\nTraining tomo tomo_0c3a99:  69%|██████▉   | 11/16 [00:03<00:01,  3.68it/s]Epoch 1, Tomo tomo_0c3a99, Batch 12/16, Loss: 0.9946\nTraining tomo tomo_0c3a99:  75%|███████▌  | 12/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_0c3a99, Batch 13/16, Loss: 0.9973\nTraining tomo tomo_0c3a99:  81%|████████▏ | 13/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_0c3a99, Batch 14/16, Loss: 0.9971\nTraining tomo tomo_0c3a99:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_0c3a99, Batch 15/16, Loss: 0.9971\nTraining tomo tomo_0c3a99:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_0c3a99, Batch 16/16, Loss: 0.9913\nTraining tomo tomo_0c3a99: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\nEpoch 1, Tomo tomo_0c3a99 completed, Average Loss: 0.9947\nCompleted training on tomogram tomo_0c3a99, Loss: 0.9947\nProcessed 25/287 tomograms\nCleaning memory for batch: ['tomo_2daaee', 'tomo_d2b1bc', 'tomo_05df8a', 'tomo_6df2d6', 'tomo_0c3a99']\nProcessing batch of tomograms: ['tomo_acadd7', 'tomo_a0cb00', 'tomo_93c0b4', 'tomo_0f9df0', 'tomo_84997e']\nTomogram tomo_acadd7 has 1 motors\nDownloading data for tomogram tomo_acadd7\nPrepared patches for tomogram tomo_acadd7\nTomogram tomo_a0cb00 has 1 motors\nDownloading data for tomogram tomo_a0cb00\nPrepared patches for tomogram tomo_a0cb00\nTomogram tomo_93c0b4 has 1 motors\nDownloading data for tomogram tomo_93c0b4\nPrepared patches for tomogram tomo_93c0b4\nTomogram tomo_0f9df0 has 1 motors\nDownloading data for tomogram tomo_0f9df0\nPrepared patches for tomogram tomo_0f9df0\nTomogram tomo_84997e has 1 motors\nDownloading data for tomogram tomo_84997e\nPrepared patches for tomogram tomo_84997e\nTraining on tomogram tomo_acadd7\nTraining tomo tomo_acadd7:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_acadd7, Batch 1/16, Loss: 0.9930\nTomo tomo_acadd7, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -4.7512/86.0299\nLoss: 0.9930\nGPU Memory: 0.17 GB\nTraining tomo tomo_acadd7:   6%|▋         | 1/16 [00:00<00:04,  3.33it/s]Epoch 1, Tomo tomo_acadd7, Batch 2/16, Loss: 0.9930\nTomo tomo_acadd7, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.1854/75.6099\nLoss: 0.9930\nGPU Memory: 0.17 GB\nTraining tomo tomo_acadd7:  12%|█▎        | 2/16 [00:00<00:04,  3.44it/s]Epoch 1, Tomo tomo_acadd7, Batch 3/16, Loss: 0.9990\nTraining tomo tomo_acadd7:  19%|█▉        | 3/16 [00:00<00:03,  3.50it/s]Epoch 1, Tomo tomo_acadd7, Batch 4/16, Loss: 0.9950\nTraining tomo tomo_acadd7:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_acadd7, Batch 5/16, Loss: 0.9948\nTraining tomo tomo_acadd7:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_acadd7, Batch 6/16, Loss: 1.0000\nTraining tomo tomo_acadd7:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_acadd7, Batch 7/16, Loss: 0.9949\nTraining tomo tomo_acadd7:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_acadd7, Batch 8/16, Loss: 0.9930\nTraining tomo tomo_acadd7:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_acadd7, Batch 9/16, Loss: 1.0000\nTraining tomo tomo_acadd7:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_acadd7, Batch 10/16, Loss: 0.9956\nTraining tomo tomo_acadd7:  62%|██████▎   | 10/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_acadd7, Batch 11/16, Loss: 0.9951\nTraining tomo tomo_acadd7:  69%|██████▉   | 11/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_acadd7, Batch 12/16, Loss: 0.9945\nTraining tomo tomo_acadd7:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_acadd7, Batch 13/16, Loss: 0.9934\nTraining tomo tomo_acadd7:  81%|████████▏ | 13/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_acadd7, Batch 14/16, Loss: 0.9945\nTraining tomo tomo_acadd7:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_acadd7, Batch 15/16, Loss: 0.9973\nTraining tomo tomo_acadd7:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_acadd7, Batch 16/16, Loss: 0.9918\nTraining tomo tomo_acadd7: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_acadd7 completed, Average Loss: 0.9953\nCompleted training on tomogram tomo_acadd7, Loss: 0.9953\nProcessed 26/287 tomograms\nTraining on tomogram tomo_a0cb00\nTraining tomo tomo_a0cb00:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_a0cb00, Batch 1/16, Loss: 0.9974\nTomo tomo_a0cb00, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -8.2855/84.3788\nLoss: 0.9974\nGPU Memory: 0.17 GB\nTraining tomo tomo_a0cb00:   6%|▋         | 1/16 [00:00<00:04,  3.58it/s]Epoch 1, Tomo tomo_a0cb00, Batch 2/16, Loss: 0.9973\nTomo tomo_a0cb00, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -8.2167/94.8923\nLoss: 0.9973\nGPU Memory: 0.17 GB\nTraining tomo tomo_a0cb00:  12%|█▎        | 2/16 [00:00<00:03,  3.52it/s]Epoch 1, Tomo tomo_a0cb00, Batch 3/16, Loss: 0.9975\nTraining tomo tomo_a0cb00:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_a0cb00, Batch 4/16, Loss: 0.9976\nTraining tomo tomo_a0cb00:  25%|██▌       | 4/16 [00:01<00:03,  3.61it/s]Epoch 1, Tomo tomo_a0cb00, Batch 5/16, Loss: 0.9955\nTraining tomo tomo_a0cb00:  31%|███▏      | 5/16 [00:01<00:03,  3.67it/s]Epoch 1, Tomo tomo_a0cb00, Batch 6/16, Loss: 0.9945\nTraining tomo tomo_a0cb00:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_a0cb00, Batch 7/16, Loss: 0.9915\nTraining tomo tomo_a0cb00:  44%|████▍     | 7/16 [00:01<00:02,  3.62it/s]Epoch 1, Tomo tomo_a0cb00, Batch 8/16, Loss: 1.0000\nTraining tomo tomo_a0cb00:  50%|█████     | 8/16 [00:02<00:02,  3.55it/s]Epoch 1, Tomo tomo_a0cb00, Batch 9/16, Loss: 0.9910\nTraining tomo tomo_a0cb00:  56%|█████▋    | 9/16 [00:02<00:01,  3.61it/s]Epoch 1, Tomo tomo_a0cb00, Batch 10/16, Loss: 0.9882\nTraining tomo tomo_a0cb00:  62%|██████▎   | 10/16 [00:02<00:01,  3.59it/s]Epoch 1, Tomo tomo_a0cb00, Batch 11/16, Loss: 0.9941\nTraining tomo tomo_a0cb00:  69%|██████▉   | 11/16 [00:03<00:01,  3.64it/s]Epoch 1, Tomo tomo_a0cb00, Batch 12/16, Loss: 0.9937\nTraining tomo tomo_a0cb00:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_a0cb00, Batch 13/16, Loss: 0.9936\nTraining tomo tomo_a0cb00:  81%|████████▏ | 13/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_a0cb00, Batch 14/16, Loss: 0.9905\nTraining tomo tomo_a0cb00:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_a0cb00, Batch 15/16, Loss: 0.9904\nTraining tomo tomo_a0cb00:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_a0cb00, Batch 16/16, Loss: 0.9938\nTraining tomo tomo_a0cb00: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\nEpoch 1, Tomo tomo_a0cb00 completed, Average Loss: 0.9942\nCompleted training on tomogram tomo_a0cb00, Loss: 0.9942\nProcessed 27/287 tomograms\nTraining on tomogram tomo_93c0b4\nTraining tomo tomo_93c0b4:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_93c0b4, Batch 1/16, Loss: 0.9967\nTomo tomo_93c0b4, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.1734/175.9253\nLoss: 0.9967\nGPU Memory: 0.17 GB\nTraining tomo tomo_93c0b4:   6%|▋         | 1/16 [00:00<00:04,  3.60it/s]Epoch 1, Tomo tomo_93c0b4, Batch 2/16, Loss: 0.9967\nTomo tomo_93c0b4, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.7739/122.2346\nLoss: 0.9967\nGPU Memory: 0.17 GB\nTraining tomo tomo_93c0b4:  12%|█▎        | 2/16 [00:00<00:03,  3.57it/s]Epoch 1, Tomo tomo_93c0b4, Batch 3/16, Loss: 0.9903\nTraining tomo tomo_93c0b4:  19%|█▉        | 3/16 [00:00<00:03,  3.62it/s]Epoch 1, Tomo tomo_93c0b4, Batch 4/16, Loss: 0.9999\nTraining tomo tomo_93c0b4:  25%|██▌       | 4/16 [00:01<00:03,  3.68it/s]Epoch 1, Tomo tomo_93c0b4, Batch 5/16, Loss: 0.9934\nTraining tomo tomo_93c0b4:  31%|███▏      | 5/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_93c0b4, Batch 6/16, Loss: 0.9967\nTraining tomo tomo_93c0b4:  38%|███▊      | 6/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_93c0b4, Batch 7/16, Loss: 0.9968\nTraining tomo tomo_93c0b4:  44%|████▍     | 7/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_93c0b4, Batch 8/16, Loss: 0.9935\nTraining tomo tomo_93c0b4:  50%|█████     | 8/16 [00:02<00:02,  3.71it/s]Epoch 1, Tomo tomo_93c0b4, Batch 9/16, Loss: 0.9896\nTraining tomo tomo_93c0b4:  56%|█████▋    | 9/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_93c0b4, Batch 10/16, Loss: 0.9958\nTraining tomo tomo_93c0b4:  62%|██████▎   | 10/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_93c0b4, Batch 11/16, Loss: 0.9895\nTraining tomo tomo_93c0b4:  69%|██████▉   | 11/16 [00:02<00:01,  3.73it/s]Epoch 1, Tomo tomo_93c0b4, Batch 12/16, Loss: 0.9932\nTraining tomo tomo_93c0b4:  75%|███████▌  | 12/16 [00:03<00:01,  3.73it/s]Epoch 1, Tomo tomo_93c0b4, Batch 13/16, Loss: 0.9929\nTraining tomo tomo_93c0b4:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_93c0b4, Batch 14/16, Loss: 1.0000\nTraining tomo tomo_93c0b4:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_93c0b4, Batch 15/16, Loss: 0.9932\nTraining tomo tomo_93c0b4:  94%|█████████▍| 15/16 [00:04<00:00,  3.70it/s]Epoch 1, Tomo tomo_93c0b4, Batch 16/16, Loss: 0.9927\nTraining tomo tomo_93c0b4: 100%|██████████| 16/16 [00:04<00:00,  3.70it/s]\nEpoch 1, Tomo tomo_93c0b4 completed, Average Loss: 0.9944\nCompleted training on tomogram tomo_93c0b4, Loss: 0.9944\nProcessed 28/287 tomograms\nTraining on tomogram tomo_0f9df0\nTraining tomo tomo_0f9df0:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_0f9df0, Batch 1/16, Loss: 0.9981\nTomo tomo_0f9df0, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.0998/116.2064\nLoss: 0.9981\nGPU Memory: 0.17 GB\nTraining tomo tomo_0f9df0:   6%|▋         | 1/16 [00:00<00:04,  3.59it/s]Epoch 1, Tomo tomo_0f9df0, Batch 2/16, Loss: 0.9997\nTomo tomo_0f9df0, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.6871/130.3517\nLoss: 0.9997\nGPU Memory: 0.17 GB\nTraining tomo tomo_0f9df0:  12%|█▎        | 2/16 [00:00<00:03,  3.52it/s]Epoch 1, Tomo tomo_0f9df0, Batch 3/16, Loss: 0.9963\nTraining tomo tomo_0f9df0:  19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_0f9df0, Batch 4/16, Loss: 0.9998\nTraining tomo tomo_0f9df0:  25%|██▌       | 4/16 [00:01<00:03,  3.63it/s]Epoch 1, Tomo tomo_0f9df0, Batch 5/16, Loss: 0.9967\nTraining tomo tomo_0f9df0:  31%|███▏      | 5/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_0f9df0, Batch 6/16, Loss: 0.9952\nTraining tomo tomo_0f9df0:  38%|███▊      | 6/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_0f9df0, Batch 7/16, Loss: 0.9991\nTraining tomo tomo_0f9df0:  44%|████▍     | 7/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_0f9df0, Batch 8/16, Loss: 1.0000\nTraining tomo tomo_0f9df0:  50%|█████     | 8/16 [00:02<00:02,  3.71it/s]Epoch 1, Tomo tomo_0f9df0, Batch 9/16, Loss: 0.9973\nTraining tomo tomo_0f9df0:  56%|█████▋    | 9/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_0f9df0, Batch 10/16, Loss: 0.9947\nTraining tomo tomo_0f9df0:  62%|██████▎   | 10/16 [00:02<00:01,  3.66it/s]Epoch 1, Tomo tomo_0f9df0, Batch 11/16, Loss: 1.0000\nTraining tomo tomo_0f9df0:  69%|██████▉   | 11/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_0f9df0, Batch 12/16, Loss: 0.9913\nTraining tomo tomo_0f9df0:  75%|███████▌  | 12/16 [00:03<00:01,  3.60it/s]Epoch 1, Tomo tomo_0f9df0, Batch 13/16, Loss: 0.9967\nTraining tomo tomo_0f9df0:  81%|████████▏ | 13/16 [00:03<00:00,  3.54it/s]Epoch 1, Tomo tomo_0f9df0, Batch 14/16, Loss: 0.9895\nTraining tomo tomo_0f9df0:  88%|████████▊ | 14/16 [00:03<00:00,  3.58it/s]Epoch 1, Tomo tomo_0f9df0, Batch 15/16, Loss: 0.9997\nTraining tomo tomo_0f9df0:  94%|█████████▍| 15/16 [00:04<00:00,  3.58it/s]Epoch 1, Tomo tomo_0f9df0, Batch 16/16, Loss: 0.9941\nTraining tomo tomo_0f9df0: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_0f9df0 completed, Average Loss: 0.9968\nCompleted training on tomogram tomo_0f9df0, Loss: 0.9968\nProcessed 29/287 tomograms\nTraining on tomogram tomo_84997e\nTraining tomo tomo_84997e:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_84997e, Batch 1/16, Loss: 0.9940\nTomo tomo_84997e, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.9125/95.0399\nLoss: 0.9940\nGPU Memory: 0.17 GB\nTraining tomo tomo_84997e:   6%|▋         | 1/16 [00:00<00:04,  3.61it/s]Epoch 1, Tomo tomo_84997e, Batch 2/16, Loss: 0.9969\nTomo tomo_84997e, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.0830/134.1472\nLoss: 0.9969\nGPU Memory: 0.17 GB\nTraining tomo tomo_84997e:  12%|█▎        | 2/16 [00:00<00:03,  3.58it/s]Epoch 1, Tomo tomo_84997e, Batch 3/16, Loss: 0.9939\nTraining tomo tomo_84997e:  19%|█▉        | 3/16 [00:00<00:03,  3.62it/s]Epoch 1, Tomo tomo_84997e, Batch 4/16, Loss: 1.0000\nTraining tomo tomo_84997e:  25%|██▌       | 4/16 [00:01<00:03,  3.67it/s]Epoch 1, Tomo tomo_84997e, Batch 5/16, Loss: 0.9933\nTraining tomo tomo_84997e:  31%|███▏      | 5/16 [00:01<00:02,  3.74it/s]Epoch 1, Tomo tomo_84997e, Batch 6/16, Loss: 0.9934\nTraining tomo tomo_84997e:  38%|███▊      | 6/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_84997e, Batch 7/16, Loss: 0.9896\nTraining tomo tomo_84997e:  44%|████▍     | 7/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_84997e, Batch 8/16, Loss: 0.9924\nTraining tomo tomo_84997e:  50%|█████     | 8/16 [00:02<00:02,  3.69it/s]Epoch 1, Tomo tomo_84997e, Batch 9/16, Loss: 0.9926\nTraining tomo tomo_84997e:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_84997e, Batch 10/16, Loss: 0.9966\nTraining tomo tomo_84997e:  62%|██████▎   | 10/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_84997e, Batch 11/16, Loss: 0.9928\nTraining tomo tomo_84997e:  69%|██████▉   | 11/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_84997e, Batch 12/16, Loss: 0.9881\nTraining tomo tomo_84997e:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_84997e, Batch 13/16, Loss: 0.9886\nTraining tomo tomo_84997e:  81%|████████▏ | 13/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_84997e, Batch 14/16, Loss: 0.9960\nTraining tomo tomo_84997e:  88%|████████▊ | 14/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_84997e, Batch 15/16, Loss: 0.9883\nTraining tomo tomo_84997e:  94%|█████████▍| 15/16 [00:04<00:00,  3.69it/s]Epoch 1, Tomo tomo_84997e, Batch 16/16, Loss: 0.9889\nTraining tomo tomo_84997e: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_84997e completed, Average Loss: 0.9928\nCompleted training on tomogram tomo_84997e, Loss: 0.9928\nProcessed 30/287 tomograms\nCleaning memory for batch: ['tomo_acadd7', 'tomo_a0cb00', 'tomo_93c0b4', 'tomo_0f9df0', 'tomo_84997e']\nProcessing batch of tomograms: ['tomo_a8073d', 'tomo_82d780', 'tomo_fc5ae4', 'tomo_6f83d4', 'tomo_210371']\nTomogram tomo_a8073d has 3 motors\nDownloading data for tomogram tomo_a8073d\nPrepared patches for tomogram tomo_a8073d\nTomogram tomo_82d780 has 1 motors\nDownloading data for tomogram tomo_82d780\nPrepared patches for tomogram tomo_82d780\nTomogram tomo_fc5ae4 has 1 motors\nDownloading data for tomogram tomo_fc5ae4\nPrepared patches for tomogram tomo_fc5ae4\nTomogram tomo_6f83d4 has 1 motors\nDownloading data for tomogram tomo_6f83d4\nPrepared patches for tomogram tomo_6f83d4\nTomogram tomo_210371 has 1 motors\nDownloading data for tomogram tomo_210371\nPrepared patches for tomogram tomo_210371\nTraining on tomogram tomo_a8073d\nTraining tomo tomo_a8073d:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_a8073d, Batch 1/16, Loss: 0.9998\nTomo tomo_a8073d, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.8661/123.3637\nLoss: 0.9998\nGPU Memory: 0.17 GB\nTraining tomo tomo_a8073d:   6%|▋         | 1/16 [00:00<00:04,  3.36it/s]Epoch 1, Tomo tomo_a8073d, Batch 2/16, Loss: 0.9935\nTomo tomo_a8073d, Batch 1 load time: 0.07s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.9637/119.9690\nLoss: 0.9935\nGPU Memory: 0.17 GB\nTraining tomo tomo_a8073d:  12%|█▎        | 2/16 [00:00<00:04,  3.37it/s]Epoch 1, Tomo tomo_a8073d, Batch 3/16, Loss: 0.9927\nTraining tomo tomo_a8073d:  19%|█▉        | 3/16 [00:00<00:03,  3.47it/s]Epoch 1, Tomo tomo_a8073d, Batch 4/16, Loss: 0.9930\nTraining tomo tomo_a8073d:  25%|██▌       | 4/16 [00:01<00:03,  3.52it/s]Epoch 1, Tomo tomo_a8073d, Batch 5/16, Loss: 0.9950\nTraining tomo tomo_a8073d:  31%|███▏      | 5/16 [00:01<00:03,  3.61it/s]Epoch 1, Tomo tomo_a8073d, Batch 6/16, Loss: 0.9989\nTraining tomo tomo_a8073d:  38%|███▊      | 6/16 [00:01<00:02,  3.60it/s]Epoch 1, Tomo tomo_a8073d, Batch 7/16, Loss: 0.9968\nTraining tomo tomo_a8073d:  44%|████▍     | 7/16 [00:01<00:02,  3.63it/s]Epoch 1, Tomo tomo_a8073d, Batch 8/16, Loss: 0.9941\nTraining tomo tomo_a8073d:  50%|█████     | 8/16 [00:02<00:02,  3.65it/s]Epoch 1, Tomo tomo_a8073d, Batch 9/16, Loss: 0.9942\nTraining tomo tomo_a8073d:  56%|█████▋    | 9/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_a8073d, Batch 10/16, Loss: 0.9947\nTraining tomo tomo_a8073d:  62%|██████▎   | 10/16 [00:02<00:01,  3.66it/s]Epoch 1, Tomo tomo_a8073d, Batch 11/16, Loss: 0.9931\nTraining tomo tomo_a8073d:  69%|██████▉   | 11/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_a8073d, Batch 12/16, Loss: 0.9945\nTraining tomo tomo_a8073d:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_a8073d, Batch 13/16, Loss: 0.9909\nTraining tomo tomo_a8073d:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_a8073d, Batch 14/16, Loss: 0.9940\nTraining tomo tomo_a8073d:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_a8073d, Batch 15/16, Loss: 0.9936\nTraining tomo tomo_a8073d:  94%|█████████▍| 15/16 [00:04<00:00,  3.69it/s]Epoch 1, Tomo tomo_a8073d, Batch 16/16, Loss: 0.9968\nTraining tomo tomo_a8073d: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_a8073d completed, Average Loss: 0.9947\nCompleted training on tomogram tomo_a8073d, Loss: 0.9947\nProcessed 31/287 tomograms\nTraining on tomogram tomo_82d780\nTraining tomo tomo_82d780:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_82d780, Batch 1/16, Loss: 0.9980\nTomo tomo_82d780, Batch 0 load time: 0.07s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.6001/117.1755\nLoss: 0.9980\nGPU Memory: 0.17 GB\nTraining tomo tomo_82d780:   6%|▋         | 1/16 [00:00<00:04,  3.43it/s]Epoch 1, Tomo tomo_82d780, Batch 2/16, Loss: 0.9945\nTomo tomo_82d780, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.1174/136.0215\nLoss: 0.9945\nGPU Memory: 0.17 GB\nTraining tomo tomo_82d780:  12%|█▎        | 2/16 [00:00<00:04,  3.49it/s]Epoch 1, Tomo tomo_82d780, Batch 3/16, Loss: 0.9926\nTraining tomo tomo_82d780:  19%|█▉        | 3/16 [00:00<00:03,  3.51it/s]Epoch 1, Tomo tomo_82d780, Batch 4/16, Loss: 0.9965\nTraining tomo tomo_82d780:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_82d780, Batch 5/16, Loss: 0.9883\nTraining tomo tomo_82d780:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_82d780, Batch 6/16, Loss: 0.9960\nTraining tomo tomo_82d780:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_82d780, Batch 7/16, Loss: 0.9919\nTraining tomo tomo_82d780:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_82d780, Batch 8/16, Loss: 0.9879\nTraining tomo tomo_82d780:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_82d780, Batch 9/16, Loss: 0.9883\nTraining tomo tomo_82d780:  56%|█████▋    | 9/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_82d780, Batch 10/16, Loss: 0.9917\nTraining tomo tomo_82d780:  62%|██████▎   | 10/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_82d780, Batch 11/16, Loss: 0.9875\nTraining tomo tomo_82d780:  69%|██████▉   | 11/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_82d780, Batch 12/16, Loss: 0.9919\nTraining tomo tomo_82d780:  75%|███████▌  | 12/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_82d780, Batch 13/16, Loss: 0.9916\nTraining tomo tomo_82d780:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_82d780, Batch 14/16, Loss: 0.9913\nTraining tomo tomo_82d780:  88%|████████▊ | 14/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_82d780, Batch 15/16, Loss: 0.9912\nTraining tomo tomo_82d780:  94%|█████████▍| 15/16 [00:04<00:00,  3.63it/s]Epoch 1, Tomo tomo_82d780, Batch 16/16, Loss: 0.9955\nTraining tomo tomo_82d780: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_82d780 completed, Average Loss: 0.9922\nCompleted training on tomogram tomo_82d780, Loss: 0.9922\nProcessed 32/287 tomograms\nTraining on tomogram tomo_fc5ae4\nTraining tomo tomo_fc5ae4:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 1/16, Loss: 0.9946\nTomo tomo_fc5ae4, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.2702/163.1321\nLoss: 0.9946\nGPU Memory: 0.17 GB\nTraining tomo tomo_fc5ae4:   6%|▋         | 1/16 [00:00<00:04,  3.55it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 2/16, Loss: 0.9911\nTomo tomo_fc5ae4, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -5.9949/117.7761\nLoss: 0.9911\nGPU Memory: 0.17 GB\nTraining tomo tomo_fc5ae4:  12%|█▎        | 2/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 3/16, Loss: 0.9923\nTraining tomo tomo_fc5ae4:  19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 4/16, Loss: 0.9866\nTraining tomo tomo_fc5ae4:  25%|██▌       | 4/16 [00:01<00:03,  3.69it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 5/16, Loss: 0.9952\nTraining tomo tomo_fc5ae4:  31%|███▏      | 5/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 6/16, Loss: 0.9901\nTraining tomo tomo_fc5ae4:  38%|███▊      | 6/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 7/16, Loss: 0.9920\nTraining tomo tomo_fc5ae4:  44%|████▍     | 7/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 8/16, Loss: 1.0000\nTraining tomo tomo_fc5ae4:  50%|█████     | 8/16 [00:02<00:02,  3.64it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 9/16, Loss: 0.9946\nTraining tomo tomo_fc5ae4:  56%|█████▋    | 9/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 10/16, Loss: 0.9917\nTraining tomo tomo_fc5ae4:  62%|██████▎   | 10/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 11/16, Loss: 0.9903\nTraining tomo tomo_fc5ae4:  69%|██████▉   | 11/16 [00:03<00:01,  3.68it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 12/16, Loss: 0.9918\nTraining tomo tomo_fc5ae4:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 13/16, Loss: 1.0000\nTraining tomo tomo_fc5ae4:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 14/16, Loss: 0.9876\nTraining tomo tomo_fc5ae4:  88%|████████▊ | 14/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 15/16, Loss: 0.9879\nTraining tomo tomo_fc5ae4:  94%|█████████▍| 15/16 [00:04<00:00,  3.66it/s]Epoch 1, Tomo tomo_fc5ae4, Batch 16/16, Loss: 0.9912\nTraining tomo tomo_fc5ae4: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\nEpoch 1, Tomo tomo_fc5ae4 completed, Average Loss: 0.9923\nCompleted training on tomogram tomo_fc5ae4, Loss: 0.9923\nProcessed 33/287 tomograms\nTraining on tomogram tomo_6f83d4\nTraining tomo tomo_6f83d4:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_6f83d4, Batch 1/16, Loss: 0.9956\nTomo tomo_6f83d4, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -7.5436/142.4371\nLoss: 0.9956\nGPU Memory: 0.17 GB\nTraining tomo tomo_6f83d4:   6%|▋         | 1/16 [00:00<00:04,  3.57it/s]Epoch 1, Tomo tomo_6f83d4, Batch 2/16, Loss: 0.9998\nTomo tomo_6f83d4, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -8.9577/166.4500\nLoss: 0.9998\nGPU Memory: 0.17 GB\nTraining tomo tomo_6f83d4:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_6f83d4, Batch 3/16, Loss: 0.9999\nTraining tomo tomo_6f83d4:  19%|█▉        | 3/16 [00:00<00:03,  3.57it/s]Epoch 1, Tomo tomo_6f83d4, Batch 4/16, Loss: 0.9997\nTraining tomo tomo_6f83d4:  25%|██▌       | 4/16 [00:01<00:03,  3.60it/s]Epoch 1, Tomo tomo_6f83d4, Batch 5/16, Loss: 0.9994\nTraining tomo tomo_6f83d4:  31%|███▏      | 5/16 [00:01<00:03,  3.62it/s]Epoch 1, Tomo tomo_6f83d4, Batch 6/16, Loss: 0.9996\nTraining tomo tomo_6f83d4:  38%|███▊      | 6/16 [00:01<00:02,  3.60it/s]Epoch 1, Tomo tomo_6f83d4, Batch 7/16, Loss: 0.9986\nTraining tomo tomo_6f83d4:  44%|████▍     | 7/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_6f83d4, Batch 8/16, Loss: 0.9910\nTraining tomo tomo_6f83d4:  50%|█████     | 8/16 [00:02<00:02,  3.59it/s]Epoch 1, Tomo tomo_6f83d4, Batch 9/16, Loss: 0.9952\nTraining tomo tomo_6f83d4:  56%|█████▋    | 9/16 [00:02<00:01,  3.60it/s]Epoch 1, Tomo tomo_6f83d4, Batch 10/16, Loss: 0.9924\nTraining tomo tomo_6f83d4:  62%|██████▎   | 10/16 [00:02<00:01,  3.61it/s]Epoch 1, Tomo tomo_6f83d4, Batch 11/16, Loss: 0.9928\nTraining tomo tomo_6f83d4:  69%|██████▉   | 11/16 [00:03<00:01,  3.64it/s]Epoch 1, Tomo tomo_6f83d4, Batch 12/16, Loss: 0.9958\nTraining tomo tomo_6f83d4:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_6f83d4, Batch 13/16, Loss: 0.9936\nTraining tomo tomo_6f83d4:  81%|████████▏ | 13/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_6f83d4, Batch 14/16, Loss: 0.9947\nTraining tomo tomo_6f83d4:  88%|████████▊ | 14/16 [00:03<00:00,  3.64it/s]Epoch 1, Tomo tomo_6f83d4, Batch 15/16, Loss: 0.9886\nTraining tomo tomo_6f83d4:  94%|█████████▍| 15/16 [00:04<00:00,  3.66it/s]Epoch 1, Tomo tomo_6f83d4, Batch 16/16, Loss: 0.9928\nTraining tomo tomo_6f83d4: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\nEpoch 1, Tomo tomo_6f83d4 completed, Average Loss: 0.9956\nCompleted training on tomogram tomo_6f83d4, Loss: 0.9956\nProcessed 34/287 tomograms\nTraining on tomogram tomo_210371\nTraining tomo tomo_210371:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_210371, Batch 1/16, Loss: 0.9888\nTomo tomo_210371, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -7.5228/114.1939\nLoss: 0.9888\nGPU Memory: 0.17 GB\nTraining tomo tomo_210371:   6%|▋         | 1/16 [00:00<00:04,  3.56it/s]Epoch 1, Tomo tomo_210371, Batch 2/16, Loss: 1.0000\nTomo tomo_210371, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/0.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -7.1659/141.5478\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_210371:  12%|█▎        | 2/16 [00:00<00:03,  3.55it/s]Epoch 1, Tomo tomo_210371, Batch 3/16, Loss: 0.9918\nTraining tomo tomo_210371:  19%|█▉        | 3/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_210371, Batch 4/16, Loss: 0.9960\nTraining tomo tomo_210371:  25%|██▌       | 4/16 [00:01<00:03,  3.58it/s]Epoch 1, Tomo tomo_210371, Batch 5/16, Loss: 0.9957\nTraining tomo tomo_210371:  31%|███▏      | 5/16 [00:01<00:02,  3.67it/s]Epoch 1, Tomo tomo_210371, Batch 6/16, Loss: 0.9917\nTraining tomo tomo_210371:  38%|███▊      | 6/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_210371, Batch 7/16, Loss: 0.9907\nTraining tomo tomo_210371:  44%|████▍     | 7/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_210371, Batch 8/16, Loss: 0.9978\nTraining tomo tomo_210371:  50%|█████     | 8/16 [00:02<00:02,  3.61it/s]Epoch 1, Tomo tomo_210371, Batch 9/16, Loss: 0.9902\nTraining tomo tomo_210371:  56%|█████▋    | 9/16 [00:02<00:01,  3.64it/s]Epoch 1, Tomo tomo_210371, Batch 10/16, Loss: 0.9869\nTraining tomo tomo_210371:  62%|██████▎   | 10/16 [00:02<00:01,  3.65it/s]Epoch 1, Tomo tomo_210371, Batch 11/16, Loss: 0.9885\nTraining tomo tomo_210371:  69%|██████▉   | 11/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_210371, Batch 12/16, Loss: 0.9950\nTraining tomo tomo_210371:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_210371, Batch 13/16, Loss: 0.9901\nTraining tomo tomo_210371:  81%|████████▏ | 13/16 [00:03<00:00,  3.65it/s]Epoch 1, Tomo tomo_210371, Batch 14/16, Loss: 0.9984\nTraining tomo tomo_210371:  88%|████████▊ | 14/16 [00:03<00:00,  3.65it/s]Epoch 1, Tomo tomo_210371, Batch 15/16, Loss: 0.9883\nTraining tomo tomo_210371:  94%|█████████▍| 15/16 [00:04<00:00,  3.65it/s]Epoch 1, Tomo tomo_210371, Batch 16/16, Loss: 0.9905\nTraining tomo tomo_210371: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_210371 completed, Average Loss: 0.9925\nCompleted training on tomogram tomo_210371, Loss: 0.9925\nProcessed 35/287 tomograms\nCleaning memory for batch: ['tomo_a8073d', 'tomo_82d780', 'tomo_fc5ae4', 'tomo_6f83d4', 'tomo_210371']\nProcessing batch of tomograms: ['tomo_85fa87', 'tomo_c4a4bb', 'tomo_a37a5c', 'tomo_ff505c', 'tomo_57592d']\nTomogram tomo_85fa87 has 1 motors\nDownloading data for tomogram tomo_85fa87\nPrepared patches for tomogram tomo_85fa87\nTomogram tomo_c4a4bb has 1 motors\nDownloading data for tomogram tomo_c4a4bb\nPrepared patches for tomogram tomo_c4a4bb\nTomogram tomo_a37a5c has 1 motors\nDownloading data for tomogram tomo_a37a5c\nPrepared patches for tomogram tomo_a37a5c\nTomogram tomo_ff505c has 1 motors\nDownloading data for tomogram tomo_ff505c\nPrepared patches for tomogram tomo_ff505c\nTomogram tomo_57592d has 1 motors\nDownloading data for tomogram tomo_57592d\nPrepared patches for tomogram tomo_57592d\nTraining on tomogram tomo_85fa87\nTraining tomo tomo_85fa87:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_85fa87, Batch 1/16, Loss: 0.9935\nTomo tomo_85fa87, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -7.0759/168.8730\nLoss: 0.9935\nGPU Memory: 0.17 GB\nTraining tomo tomo_85fa87:   6%|▋         | 1/16 [00:00<00:04,  3.27it/s]Epoch 1, Tomo tomo_85fa87, Batch 2/16, Loss: 0.9914\nTomo tomo_85fa87, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -6.2987/138.0242\nLoss: 0.9914\nGPU Memory: 0.17 GB\nTraining tomo tomo_85fa87:  12%|█▎        | 2/16 [00:00<00:04,  3.41it/s]Epoch 1, Tomo tomo_85fa87, Batch 3/16, Loss: 0.9955\nTraining tomo tomo_85fa87:  19%|█▉        | 3/16 [00:00<00:03,  3.49it/s]Epoch 1, Tomo tomo_85fa87, Batch 4/16, Loss: 0.9847\nTraining tomo tomo_85fa87:  25%|██▌       | 4/16 [00:01<00:03,  3.55it/s]Epoch 1, Tomo tomo_85fa87, Batch 5/16, Loss: 0.9870\nTraining tomo tomo_85fa87:  31%|███▏      | 5/16 [00:01<00:03,  3.57it/s]Epoch 1, Tomo tomo_85fa87, Batch 6/16, Loss: 0.9908\nTraining tomo tomo_85fa87:  38%|███▊      | 6/16 [00:01<00:02,  3.59it/s]Epoch 1, Tomo tomo_85fa87, Batch 7/16, Loss: 0.9816\nTraining tomo tomo_85fa87:  44%|████▍     | 7/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_85fa87, Batch 8/16, Loss: 0.9888\nTraining tomo tomo_85fa87:  50%|█████     | 8/16 [00:02<00:02,  3.62it/s]Epoch 1, Tomo tomo_85fa87, Batch 9/16, Loss: 0.9956\nTraining tomo tomo_85fa87:  56%|█████▋    | 9/16 [00:02<00:01,  3.65it/s]Epoch 1, Tomo tomo_85fa87, Batch 10/16, Loss: 0.9920\nTraining tomo tomo_85fa87:  62%|██████▎   | 10/16 [00:02<00:01,  3.68it/s]Epoch 1, Tomo tomo_85fa87, Batch 11/16, Loss: 0.9847\nTraining tomo tomo_85fa87:  69%|██████▉   | 11/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_85fa87, Batch 12/16, Loss: 0.9929\nTraining tomo tomo_85fa87:  75%|███████▌  | 12/16 [00:03<00:01,  3.70it/s]Epoch 1, Tomo tomo_85fa87, Batch 13/16, Loss: 0.9963\nTraining tomo tomo_85fa87:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_85fa87, Batch 14/16, Loss: 1.0000\nTraining tomo tomo_85fa87:  88%|████████▊ | 14/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_85fa87, Batch 15/16, Loss: 0.9964\nTraining tomo tomo_85fa87:  94%|█████████▍| 15/16 [00:04<00:00,  3.72it/s]Epoch 1, Tomo tomo_85fa87, Batch 16/16, Loss: 1.0000\nTraining tomo tomo_85fa87: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\nEpoch 1, Tomo tomo_85fa87 completed, Average Loss: 0.9919\nCompleted training on tomogram tomo_85fa87, Loss: 0.9919\nProcessed 36/287 tomograms\nTraining on tomogram tomo_c4a4bb\nTraining tomo tomo_c4a4bb:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 1/16, Loss: 0.9935\nTomo tomo_c4a4bb, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -10.2584/108.0326\nLoss: 0.9935\nGPU Memory: 0.17 GB\nTraining tomo tomo_c4a4bb:   6%|▋         | 1/16 [00:00<00:04,  3.59it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 2/16, Loss: 0.9978\nTomo tomo_c4a4bb, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -11.7826/118.8179\nLoss: 0.9978\nGPU Memory: 0.17 GB\nTraining tomo tomo_c4a4bb:  12%|█▎        | 2/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 3/16, Loss: 0.9971\nTraining tomo tomo_c4a4bb:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 4/16, Loss: 0.9944\nTraining tomo tomo_c4a4bb:  25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 5/16, Loss: 0.9902\nTraining tomo tomo_c4a4bb:  31%|███▏      | 5/16 [00:01<00:03,  3.63it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 6/16, Loss: 0.9976\nTraining tomo tomo_c4a4bb:  38%|███▊      | 6/16 [00:01<00:02,  3.54it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 7/16, Loss: 0.9902\nTraining tomo tomo_c4a4bb:  44%|████▍     | 7/16 [00:01<00:02,  3.51it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 8/16, Loss: 0.9972\nTraining tomo tomo_c4a4bb:  50%|█████     | 8/16 [00:02<00:02,  3.49it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 9/16, Loss: 0.9934\nTraining tomo tomo_c4a4bb:  56%|█████▋    | 9/16 [00:02<00:01,  3.56it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 10/16, Loss: 0.9933\nTraining tomo tomo_c4a4bb:  62%|██████▎   | 10/16 [00:02<00:01,  3.60it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 11/16, Loss: 0.9928\nTraining tomo tomo_c4a4bb:  69%|██████▉   | 11/16 [00:03<00:01,  3.64it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 12/16, Loss: 0.9959\nTraining tomo tomo_c4a4bb:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 13/16, Loss: 0.9937\nTraining tomo tomo_c4a4bb:  81%|████████▏ | 13/16 [00:03<00:00,  3.64it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 14/16, Loss: 0.9961\nTraining tomo tomo_c4a4bb:  88%|████████▊ | 14/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 15/16, Loss: 0.9916\nTraining tomo tomo_c4a4bb:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_c4a4bb, Batch 16/16, Loss: 0.9924\nTraining tomo tomo_c4a4bb: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\nEpoch 1, Tomo tomo_c4a4bb completed, Average Loss: 0.9942\nCompleted training on tomogram tomo_c4a4bb, Loss: 0.9942\nProcessed 37/287 tomograms\nTraining on tomogram tomo_a37a5c\nTraining tomo tomo_a37a5c:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_a37a5c, Batch 1/16, Loss: 0.9955\nTomo tomo_a37a5c, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -15.2772/132.5829\nLoss: 0.9955\nGPU Memory: 0.17 GB\nTraining tomo tomo_a37a5c:   6%|▋         | 1/16 [00:00<00:04,  3.51it/s]Epoch 1, Tomo tomo_a37a5c, Batch 2/16, Loss: 1.0000\nTomo tomo_a37a5c, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -15.0969/151.5730\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_a37a5c:  12%|█▎        | 2/16 [00:00<00:03,  3.51it/s]Epoch 1, Tomo tomo_a37a5c, Batch 3/16, Loss: 0.9816\nTraining tomo tomo_a37a5c:  19%|█▉        | 3/16 [00:00<00:03,  3.49it/s]Epoch 1, Tomo tomo_a37a5c, Batch 4/16, Loss: 0.9968\nTraining tomo tomo_a37a5c:  25%|██▌       | 4/16 [00:01<00:03,  3.59it/s]Epoch 1, Tomo tomo_a37a5c, Batch 5/16, Loss: 0.9904\nTraining tomo tomo_a37a5c:  31%|███▏      | 5/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_a37a5c, Batch 6/16, Loss: 0.9864\nTraining tomo tomo_a37a5c:  38%|███▊      | 6/16 [00:01<00:02,  3.71it/s]Epoch 1, Tomo tomo_a37a5c, Batch 7/16, Loss: 0.9878\nTraining tomo tomo_a37a5c:  44%|████▍     | 7/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_a37a5c, Batch 8/16, Loss: 0.9950\nTraining tomo tomo_a37a5c:  50%|█████     | 8/16 [00:02<00:02,  3.71it/s]Epoch 1, Tomo tomo_a37a5c, Batch 9/16, Loss: 0.9857\nTraining tomo tomo_a37a5c:  56%|█████▋    | 9/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_a37a5c, Batch 10/16, Loss: 0.9952\nTraining tomo tomo_a37a5c:  62%|██████▎   | 10/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_a37a5c, Batch 11/16, Loss: 0.9863\nTraining tomo tomo_a37a5c:  69%|██████▉   | 11/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_a37a5c, Batch 12/16, Loss: 0.9950\nTraining tomo tomo_a37a5c:  75%|███████▌  | 12/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_a37a5c, Batch 13/16, Loss: 0.9855\nTraining tomo tomo_a37a5c:  81%|████████▏ | 13/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_a37a5c, Batch 14/16, Loss: 0.9844\nTraining tomo tomo_a37a5c:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_a37a5c, Batch 15/16, Loss: 0.9943\nTraining tomo tomo_a37a5c:  94%|█████████▍| 15/16 [00:04<00:00,  3.70it/s]Epoch 1, Tomo tomo_a37a5c, Batch 16/16, Loss: 1.0000\nTraining tomo tomo_a37a5c: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\nEpoch 1, Tomo tomo_a37a5c completed, Average Loss: 0.9913\nCompleted training on tomogram tomo_a37a5c, Loss: 0.9913\nProcessed 38/287 tomograms\nTraining on tomogram tomo_ff505c\nTraining tomo tomo_ff505c:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_ff505c, Batch 1/16, Loss: 0.9854\nTomo tomo_ff505c, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -18.3758/286.7339\nLoss: 0.9854\nGPU Memory: 0.17 GB\nTraining tomo tomo_ff505c:   6%|▋         | 1/16 [00:00<00:04,  3.55it/s]Epoch 1, Tomo tomo_ff505c, Batch 2/16, Loss: 0.9899\nTomo tomo_ff505c, Batch 1 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -14.1990/178.3079\nLoss: 0.9899\nGPU Memory: 0.17 GB\nTraining tomo tomo_ff505c:  12%|█▎        | 2/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_ff505c, Batch 3/16, Loss: 0.9893\nTraining tomo tomo_ff505c:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_ff505c, Batch 4/16, Loss: 0.9899\nTraining tomo tomo_ff505c:  25%|██▌       | 4/16 [00:01<00:03,  3.62it/s]Epoch 1, Tomo tomo_ff505c, Batch 5/16, Loss: 0.9960\nTraining tomo tomo_ff505c:  31%|███▏      | 5/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_ff505c, Batch 6/16, Loss: 0.9909\nTraining tomo tomo_ff505c:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_ff505c, Batch 7/16, Loss: 0.9907\nTraining tomo tomo_ff505c:  44%|████▍     | 7/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_ff505c, Batch 8/16, Loss: 0.9952\nTraining tomo tomo_ff505c:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_ff505c, Batch 9/16, Loss: 0.9948\nTraining tomo tomo_ff505c:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_ff505c, Batch 10/16, Loss: 0.9853\nTraining tomo tomo_ff505c:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_ff505c, Batch 11/16, Loss: 0.9893\nTraining tomo tomo_ff505c:  69%|██████▉   | 11/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_ff505c, Batch 12/16, Loss: 0.9843\nTraining tomo tomo_ff505c:  75%|███████▌  | 12/16 [00:03<00:01,  3.70it/s]Epoch 1, Tomo tomo_ff505c, Batch 13/16, Loss: 1.0000\nTraining tomo tomo_ff505c:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_ff505c, Batch 14/16, Loss: 0.9899\nTraining tomo tomo_ff505c:  88%|████████▊ | 14/16 [00:03<00:00,  3.69it/s]Epoch 1, Tomo tomo_ff505c, Batch 15/16, Loss: 0.9861\nTraining tomo tomo_ff505c:  94%|█████████▍| 15/16 [00:04<00:00,  3.69it/s]Epoch 1, Tomo tomo_ff505c, Batch 16/16, Loss: 0.9891\nTraining tomo tomo_ff505c: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_ff505c completed, Average Loss: 0.9904\nCompleted training on tomogram tomo_ff505c, Loss: 0.9904\nProcessed 39/287 tomograms\nTraining on tomogram tomo_57592d\nTraining tomo tomo_57592d:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_57592d, Batch 1/16, Loss: 0.9950\nTomo tomo_57592d, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -12.8690/173.3736\nLoss: 0.9950\nGPU Memory: 0.17 GB\nTraining tomo tomo_57592d:   6%|▋         | 1/16 [00:00<00:04,  3.60it/s]Epoch 1, Tomo tomo_57592d, Batch 2/16, Loss: 0.9897\nTomo tomo_57592d, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -13.7406/138.1102\nLoss: 0.9897\nGPU Memory: 0.17 GB\nTraining tomo tomo_57592d:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_57592d, Batch 3/16, Loss: 0.9891\nTraining tomo tomo_57592d:  19%|█▉        | 3/16 [00:00<00:03,  3.56it/s]Epoch 1, Tomo tomo_57592d, Batch 4/16, Loss: 0.9844\nTraining tomo tomo_57592d:  25%|██▌       | 4/16 [00:01<00:03,  3.64it/s]Epoch 1, Tomo tomo_57592d, Batch 5/16, Loss: 0.9886\nTraining tomo tomo_57592d:  31%|███▏      | 5/16 [00:01<00:02,  3.72it/s]Epoch 1, Tomo tomo_57592d, Batch 6/16, Loss: 0.9949\nTraining tomo tomo_57592d:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_57592d, Batch 7/16, Loss: 0.9880\nTraining tomo tomo_57592d:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_57592d, Batch 8/16, Loss: 0.9758\nTraining tomo tomo_57592d:  50%|█████     | 8/16 [00:02<00:02,  3.69it/s]Epoch 1, Tomo tomo_57592d, Batch 9/16, Loss: 0.9939\nTraining tomo tomo_57592d:  56%|█████▋    | 9/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_57592d, Batch 10/16, Loss: 0.9874\nTraining tomo tomo_57592d:  62%|██████▎   | 10/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_57592d, Batch 11/16, Loss: 0.9872\nTraining tomo tomo_57592d:  69%|██████▉   | 11/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_57592d, Batch 12/16, Loss: 0.9935\nTraining tomo tomo_57592d:  75%|███████▌  | 12/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_57592d, Batch 13/16, Loss: 0.9809\nTraining tomo tomo_57592d:  81%|████████▏ | 13/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_57592d, Batch 14/16, Loss: 0.9869\nTraining tomo tomo_57592d:  88%|████████▊ | 14/16 [00:03<00:00,  3.67it/s]Epoch 1, Tomo tomo_57592d, Batch 15/16, Loss: 0.9869\nTraining tomo tomo_57592d:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_57592d, Batch 16/16, Loss: 0.9868\nTraining tomo tomo_57592d: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\nEpoch 1, Tomo tomo_57592d completed, Average Loss: 0.9880\nCompleted training on tomogram tomo_57592d, Loss: 0.9880\nProcessed 40/287 tomograms\nCleaning memory for batch: ['tomo_85fa87', 'tomo_c4a4bb', 'tomo_a37a5c', 'tomo_ff505c', 'tomo_57592d']\nProcessing batch of tomograms: ['tomo_9fc2b6', 'tomo_c84b8e', 'tomo_b50c0f', 'tomo_ba37ec', 'tomo_ede779']\nTomogram tomo_9fc2b6 has 1 motors\nDownloading data for tomogram tomo_9fc2b6\nPrepared patches for tomogram tomo_9fc2b6\nTomogram tomo_c84b8e has 2 motors\nDownloading data for tomogram tomo_c84b8e\nPrepared patches for tomogram tomo_c84b8e\nTomogram tomo_b50c0f has 1 motors\nDownloading data for tomogram tomo_b50c0f\nPrepared patches for tomogram tomo_b50c0f\nTomogram tomo_ba37ec has 1 motors\nDownloading data for tomogram tomo_ba37ec\nPrepared patches for tomogram tomo_ba37ec\nTomogram tomo_ede779 has 1 motors\nDownloading data for tomogram tomo_ede779\nPrepared patches for tomogram tomo_ede779\nTraining on tomogram tomo_9fc2b6\nTraining tomo tomo_9fc2b6:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 1/16, Loss: 0.9952\nTomo tomo_9fc2b6, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -8.6494/292.8793\nLoss: 0.9952\nGPU Memory: 0.17 GB\nTraining tomo tomo_9fc2b6:   6%|▋         | 1/16 [00:00<00:04,  3.39it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 2/16, Loss: 0.9981\nTomo tomo_9fc2b6, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -9.0422/213.5459\nLoss: 0.9981\nGPU Memory: 0.17 GB\nTraining tomo tomo_9fc2b6:  12%|█▎        | 2/16 [00:00<00:04,  3.43it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 3/16, Loss: 0.9945\nTraining tomo tomo_9fc2b6:  19%|█▉        | 3/16 [00:00<00:03,  3.47it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 4/16, Loss: 0.9950\nTraining tomo tomo_9fc2b6:  25%|██▌       | 4/16 [00:01<00:03,  3.57it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 5/16, Loss: 0.9954\nTraining tomo tomo_9fc2b6:  31%|███▏      | 5/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 6/16, Loss: 0.9909\nTraining tomo tomo_9fc2b6:  38%|███▊      | 6/16 [00:01<00:02,  3.66it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 7/16, Loss: 0.9870\nTraining tomo tomo_9fc2b6:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 8/16, Loss: 0.9938\nTraining tomo tomo_9fc2b6:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 9/16, Loss: 0.9917\nTraining tomo tomo_9fc2b6:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 10/16, Loss: 0.9908\nTraining tomo tomo_9fc2b6:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 11/16, Loss: 0.9951\nTraining tomo tomo_9fc2b6:  69%|██████▉   | 11/16 [00:03<00:01,  3.71it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 12/16, Loss: 0.9948\nTraining tomo tomo_9fc2b6:  75%|███████▌  | 12/16 [00:03<00:01,  3.70it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 13/16, Loss: 0.9999\nTraining tomo tomo_9fc2b6:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 14/16, Loss: 0.9899\nTraining tomo tomo_9fc2b6:  88%|████████▊ | 14/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 15/16, Loss: 0.9947\nTraining tomo tomo_9fc2b6:  94%|█████████▍| 15/16 [00:04<00:00,  3.67it/s]Epoch 1, Tomo tomo_9fc2b6, Batch 16/16, Loss: 0.9931\nTraining tomo tomo_9fc2b6: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_9fc2b6 completed, Average Loss: 0.9938\nCompleted training on tomogram tomo_9fc2b6, Loss: 0.9938\nProcessed 41/287 tomograms\nTraining on tomogram tomo_c84b8e\nTraining tomo tomo_c84b8e:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_c84b8e, Batch 1/16, Loss: 0.9999\nTomo tomo_c84b8e, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -13.1905/165.4474\nLoss: 0.9999\nGPU Memory: 0.17 GB\nTraining tomo tomo_c84b8e:   6%|▋         | 1/16 [00:00<00:04,  3.58it/s]Epoch 1, Tomo tomo_c84b8e, Batch 2/16, Loss: 0.9982\nTomo tomo_c84b8e, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -13.8643/135.8519\nLoss: 0.9982\nGPU Memory: 0.17 GB\nTraining tomo tomo_c84b8e:  12%|█▎        | 2/16 [00:00<00:03,  3.50it/s]Epoch 1, Tomo tomo_c84b8e, Batch 3/16, Loss: 0.9936\nTraining tomo tomo_c84b8e:  19%|█▉        | 3/16 [00:00<00:03,  3.52it/s]Epoch 1, Tomo tomo_c84b8e, Batch 4/16, Loss: 0.9927\nTraining tomo tomo_c84b8e:  25%|██▌       | 4/16 [00:01<00:03,  3.52it/s]Epoch 1, Tomo tomo_c84b8e, Batch 5/16, Loss: 0.9944\nTraining tomo tomo_c84b8e:  31%|███▏      | 5/16 [00:01<00:03,  3.60it/s]Epoch 1, Tomo tomo_c84b8e, Batch 6/16, Loss: 0.9929\nTraining tomo tomo_c84b8e:  38%|███▊      | 6/16 [00:01<00:02,  3.53it/s]Epoch 1, Tomo tomo_c84b8e, Batch 7/16, Loss: 0.9962\nTraining tomo tomo_c84b8e:  44%|████▍     | 7/16 [00:01<00:02,  3.55it/s]Epoch 1, Tomo tomo_c84b8e, Batch 8/16, Loss: 0.9884\nTraining tomo tomo_c84b8e:  50%|█████     | 8/16 [00:02<00:02,  3.60it/s]Epoch 1, Tomo tomo_c84b8e, Batch 9/16, Loss: 0.9963\nTraining tomo tomo_c84b8e:  56%|█████▋    | 9/16 [00:02<00:01,  3.61it/s]Epoch 1, Tomo tomo_c84b8e, Batch 10/16, Loss: 0.9930\nTraining tomo tomo_c84b8e:  62%|██████▎   | 10/16 [00:02<00:01,  3.62it/s]Epoch 1, Tomo tomo_c84b8e, Batch 11/16, Loss: 0.9921\nTraining tomo tomo_c84b8e:  69%|██████▉   | 11/16 [00:03<00:01,  3.63it/s]Epoch 1, Tomo tomo_c84b8e, Batch 12/16, Loss: 0.9882\nTraining tomo tomo_c84b8e:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_c84b8e, Batch 13/16, Loss: 0.9955\nTraining tomo tomo_c84b8e:  81%|████████▏ | 13/16 [00:03<00:00,  3.68it/s]Epoch 1, Tomo tomo_c84b8e, Batch 14/16, Loss: 0.9904\nTraining tomo tomo_c84b8e:  88%|████████▊ | 14/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_c84b8e, Batch 15/16, Loss: 0.9978\nTraining tomo tomo_c84b8e:  94%|█████████▍| 15/16 [00:04<00:00,  3.68it/s]Epoch 1, Tomo tomo_c84b8e, Batch 16/16, Loss: 0.9999\nTraining tomo tomo_c84b8e: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\nEpoch 1, Tomo tomo_c84b8e completed, Average Loss: 0.9943\nCompleted training on tomogram tomo_c84b8e, Loss: 0.9943\nProcessed 42/287 tomograms\nTraining on tomogram tomo_b50c0f\nTraining tomo tomo_b50c0f:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_b50c0f, Batch 1/16, Loss: 1.0000\nTomo tomo_b50c0f, Batch 0 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -17.8986/170.8720\nLoss: 1.0000\nGPU Memory: 0.17 GB\nTraining tomo tomo_b50c0f:   6%|▋         | 1/16 [00:00<00:04,  3.50it/s]Epoch 1, Tomo tomo_b50c0f, Batch 2/16, Loss: 0.9999\nTomo tomo_b50c0f, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -18.8826/175.4001\nLoss: 0.9999\nGPU Memory: 0.17 GB\nTraining tomo tomo_b50c0f:  12%|█▎        | 2/16 [00:00<00:03,  3.52it/s]Epoch 1, Tomo tomo_b50c0f, Batch 3/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  19%|█▉        | 3/16 [00:00<00:03,  3.45it/s]Epoch 1, Tomo tomo_b50c0f, Batch 4/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  25%|██▌       | 4/16 [00:01<00:03,  3.50it/s]Epoch 1, Tomo tomo_b50c0f, Batch 5/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  31%|███▏      | 5/16 [00:01<00:03,  3.57it/s]Epoch 1, Tomo tomo_b50c0f, Batch 6/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  38%|███▊      | 6/16 [00:01<00:02,  3.61it/s]Epoch 1, Tomo tomo_b50c0f, Batch 7/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  44%|████▍     | 7/16 [00:01<00:02,  3.64it/s]Epoch 1, Tomo tomo_b50c0f, Batch 8/16, Loss: 0.9952\nTraining tomo tomo_b50c0f:  50%|█████     | 8/16 [00:02<00:02,  3.65it/s]Epoch 1, Tomo tomo_b50c0f, Batch 9/16, Loss: 0.9998\nTraining tomo tomo_b50c0f:  56%|█████▋    | 9/16 [00:02<00:01,  3.69it/s]Epoch 1, Tomo tomo_b50c0f, Batch 10/16, Loss: 0.9997\nTraining tomo tomo_b50c0f:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_b50c0f, Batch 11/16, Loss: 0.9949\nTraining tomo tomo_b50c0f:  69%|██████▉   | 11/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_b50c0f, Batch 12/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  75%|███████▌  | 12/16 [00:03<00:01,  3.69it/s]Epoch 1, Tomo tomo_b50c0f, Batch 13/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  81%|████████▏ | 13/16 [00:03<00:00,  3.71it/s]Epoch 1, Tomo tomo_b50c0f, Batch 14/16, Loss: 0.9964\nTraining tomo tomo_b50c0f:  88%|████████▊ | 14/16 [00:03<00:00,  3.70it/s]Epoch 1, Tomo tomo_b50c0f, Batch 15/16, Loss: 0.9999\nTraining tomo tomo_b50c0f:  94%|█████████▍| 15/16 [00:04<00:00,  3.71it/s]Epoch 1, Tomo tomo_b50c0f, Batch 16/16, Loss: 0.9999\nTraining tomo tomo_b50c0f: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_b50c0f completed, Average Loss: 0.9991\nCompleted training on tomogram tomo_b50c0f, Loss: 0.9991\nProcessed 43/287 tomograms\nTraining on tomogram tomo_ba37ec\nTraining tomo tomo_ba37ec:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_ba37ec, Batch 1/16, Loss: 0.9999\nTomo tomo_ba37ec, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -15.5818/127.9548\nLoss: 0.9999\nGPU Memory: 0.17 GB\nTraining tomo tomo_ba37ec:   6%|▋         | 1/16 [00:00<00:04,  3.61it/s]Epoch 1, Tomo tomo_ba37ec, Batch 2/16, Loss: 0.9999\nTomo tomo_ba37ec, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -13.3889/129.7870\nLoss: 0.9999\nGPU Memory: 0.17 GB\nTraining tomo tomo_ba37ec:  12%|█▎        | 2/16 [00:00<00:03,  3.53it/s]Epoch 1, Tomo tomo_ba37ec, Batch 3/16, Loss: 0.9941\nTraining tomo tomo_ba37ec:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_ba37ec, Batch 4/16, Loss: 0.9950\nTraining tomo tomo_ba37ec:  25%|██▌       | 4/16 [00:01<00:03,  3.65it/s]Epoch 1, Tomo tomo_ba37ec, Batch 5/16, Loss: 0.9958\nTraining tomo tomo_ba37ec:  31%|███▏      | 5/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_ba37ec, Batch 6/16, Loss: 1.0000\nTraining tomo tomo_ba37ec:  38%|███▊      | 6/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_ba37ec, Batch 7/16, Loss: 0.9850\nTraining tomo tomo_ba37ec:  44%|████▍     | 7/16 [00:01<00:02,  3.68it/s]Epoch 1, Tomo tomo_ba37ec, Batch 8/16, Loss: 0.9916\nTraining tomo tomo_ba37ec:  50%|█████     | 8/16 [00:02<00:02,  3.68it/s]Epoch 1, Tomo tomo_ba37ec, Batch 9/16, Loss: 0.9964\nTraining tomo tomo_ba37ec:  56%|█████▋    | 9/16 [00:02<00:01,  3.70it/s]Epoch 1, Tomo tomo_ba37ec, Batch 10/16, Loss: 0.9915\nTraining tomo tomo_ba37ec:  62%|██████▎   | 10/16 [00:02<00:01,  3.71it/s]Epoch 1, Tomo tomo_ba37ec, Batch 11/16, Loss: 1.0000\nTraining tomo tomo_ba37ec:  69%|██████▉   | 11/16 [00:02<00:01,  3.72it/s]Epoch 1, Tomo tomo_ba37ec, Batch 12/16, Loss: 0.9996\nTraining tomo tomo_ba37ec:  75%|███████▌  | 12/16 [00:03<00:01,  3.72it/s]Epoch 1, Tomo tomo_ba37ec, Batch 13/16, Loss: 0.9931\nTraining tomo tomo_ba37ec:  81%|████████▏ | 13/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_ba37ec, Batch 14/16, Loss: 0.9944\nTraining tomo tomo_ba37ec:  88%|████████▊ | 14/16 [00:03<00:00,  3.72it/s]Epoch 1, Tomo tomo_ba37ec, Batch 15/16, Loss: 0.9903\nTraining tomo tomo_ba37ec:  94%|█████████▍| 15/16 [00:04<00:00,  3.65it/s]Epoch 1, Tomo tomo_ba37ec, Batch 16/16, Loss: 0.9923\nTraining tomo tomo_ba37ec: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\nEpoch 1, Tomo tomo_ba37ec completed, Average Loss: 0.9949\nCompleted training on tomogram tomo_ba37ec, Loss: 0.9949\nProcessed 44/287 tomograms\nTraining on tomogram tomo_ede779\nTraining tomo tomo_ede779:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 1, Tomo tomo_ede779, Batch 1/16, Loss: 0.9919\nTomo tomo_ede779, Batch 0 load time: 0.05s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -18.1093/188.2979\nLoss: 0.9919\nGPU Memory: 0.17 GB\nTraining tomo tomo_ede779:   6%|▋         | 1/16 [00:00<00:04,  3.60it/s]Epoch 1, Tomo tomo_ede779, Batch 2/16, Loss: 0.9985\nTomo tomo_ede779, Batch 1 load time: 0.06s\nInputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nTargets shape: torch.Size([4, 1, 128, 128, 128]), min/max: 0.0000/1.0000\nOutputs shape: torch.Size([4, 1, 128, 128, 128]), min/max: -18.3614/197.5052\nLoss: 0.9985\nGPU Memory: 0.17 GB\nTraining tomo tomo_ede779:  12%|█▎        | 2/16 [00:00<00:03,  3.54it/s]Epoch 1, Tomo tomo_ede779, Batch 3/16, Loss: 0.9954\nTraining tomo tomo_ede779:  19%|█▉        | 3/16 [00:00<00:03,  3.59it/s]Epoch 1, Tomo tomo_ede779, Batch 4/16, Loss: 1.0000\nTraining tomo tomo_ede779:  25%|██▌       | 4/16 [00:01<00:03,  3.66it/s]Epoch 1, Tomo tomo_ede779, Batch 5/16, Loss: 0.9969\nTraining tomo tomo_ede779:  31%|███▏      | 5/16 [00:01<00:02,  3.73it/s]Epoch 1, Tomo tomo_ede779, Batch 6/16, Loss: 0.9945\nTraining tomo tomo_ede779:  38%|███▊      | 6/16 [00:01<00:02,  3.70it/s]Epoch 1, Tomo tomo_ede779, Batch 7/16, Loss: 0.9966\nTraining tomo tomo_ede779:  44%|████▍     | 7/16 [00:01<00:02,  3.69it/s]Epoch 1, Tomo tomo_ede779, Batch 8/16, Loss: 0.9917\nTraining tomo tomo_ede779:  50%|█████     | 8/16 [00:02<00:02,  3.64it/s]Epoch 1, Tomo tomo_ede779, Batch 9/16, Loss: 0.9917\nTraining tomo tomo_ede779:  56%|█████▋    | 9/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_ede779, Batch 10/16, Loss: 0.9985\nTraining tomo tomo_ede779:  62%|██████▎   | 10/16 [00:02<00:01,  3.67it/s]Epoch 1, Tomo tomo_ede779, Batch 11/16, Loss: 0.9937\nTraining tomo tomo_ede779:  69%|██████▉   | 11/16 [00:03<00:01,  3.67it/s]Epoch 1, Tomo tomo_ede779, Batch 12/16, Loss: 0.9866\nTraining tomo tomo_ede779:  75%|███████▌  | 12/16 [00:03<00:01,  3.66it/s]Epoch 1, Tomo tomo_ede779, Batch 13/16, Loss: 0.9818\nTraining tomo tomo_ede779:  81%|████████▏ | 13/16 [00:03<00:00,  3.66it/s]Epoch 1, Tomo tomo_ede779, Batch 14/16, Loss: 0.9854\nTraining tomo tomo_ede779:  88%|████████▊ | 14/16 [00:03<00:00,  3.65it/s]Epoch 1, Tomo tomo_ede779, Batch 15/16, Loss: 0.9945\nTraining tomo tomo_ede779:  94%|█████████▍| 15/16 [00:04<00:00,  3.66it/s]Epoch 1, Tomo tomo_ede779, Batch 16/16, Loss: 0.9887\nTraining tomo tomo_ede779: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\nEpoch 1, Tomo tomo_ede779 completed, Average Loss: 0.9929\nCompleted training on tomogram tomo_ede779, Loss: 0.9929\nProcessed 45/287 tomograms\nCleaning memory for batch: ['tomo_9fc2b6', 'tomo_c84b8e', 'tomo_b50c0f', 'tomo_ba37ec', 'tomo_ede779']\nProcessing batch of tomograms: ['tomo_8e90f9', 'tomo_651ecd', 'tomo_9722d1', 'tomo_7b1ee3', 'tomo_033ebe']\nTomogram tomo_8e90f9 has 1 motors\nDownloading data for tomogram tomo_8e90f9\nPrepared patches for tomogram tomo_8e90f9\nTomogram tomo_651ecd has 1 motors\nDownloading data for tomogram tomo_651ecd\nPrepared patches for tomogram tomo_651ecd\nTomogram tomo_9722d1 has 2 motors\nDownloading data for tomogram tomo_9722d1\nPrepared patches for tomogram tomo_9722d1\nTomogram tomo_7b1ee3 has 1 motors\nDownloading data for tomogram tomo_7b1ee3\nPrepared patches for tomogram tomo_7b1ee3\nTomogram tomo_033ebe has 1 motors\nDownloading data for tomogram tomo_033ebe\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 1 Hyperparameter Tuning (Peak Detection Threshold)\n# 2 Predict on the validation set,\n# 3 Extract motor locations using peak detection\n# 4 Tune the threshold to maximize the Fβ-score (β=2).\n# 5 Using competition's metric\n\n# Metric implementation (from provided notebook)\ndef distance_metric(solution, submission, thresh_ratio, min_radius):\n    coordinate_cols = ['Motor axis 0', 'Motor axis 1', 'Motor axis 2']\n    label_tensor = solution[coordinate_cols].values.reshape(len(solution), -1, len(coordinate_cols))\n    predicted_tensor = submission[coordinate_cols].values.reshape(len(submission), -1, len(coordinate_cols))\n    solution['distance'] = np.linalg.norm(label_tensor - predicted_tensor, axis=2).min(axis=1)\n    solution['thresholds'] = solution['Voxel spacing'].apply(lambda x: (min_radius * thresh_ratio) / x)\n    solution['predictions'] = submission['Has motor'].values\n    solution.loc[(solution['distance'] > solution['thresholds']) & (solution['Has motor'] == 1) & (submission['Has motor'] == 1), 'predictions'] = 0\n    return solution['predictions'].values\n\ndef score(solution, submission, min_radius, beta):\n    solution = solution.sort_values('tomo_id').reset_index(drop=True)\n    submission = submission.sort_values('tomo_id').reset_index(drop=True)\n    if not solution['tomo_id'].eq(submission['tomo_id']).all():\n        raise ValueError('Submitted tomo_id values do not match')\n    submission['Has motor'] = 1\n    select = (submission[['Motor axis 0', 'Motor axis 1', 'Motor axis 2']] == -1).any(axis='columns')\n    submission.loc[select, 'Has motor'] = 0\n    predictions = distance_metric(solution, submission, thresh_ratio=1.0, min_radius=min_radius)\n    return sklearn.metrics.fbeta_score(solution['Has motor'].values, predictions, beta=beta)\n\n# Predict on full volume with sliding window\ndef predict_full_volume(model, volume, patch_size=(128, 128, 128), stride=64):\n    model.eval()\n    volume = volume.to(device)\n    z_size, y_size, x_size = volume.shape[2:]\n    pz, py, px = patch_size\n    output = torch.zeros_like(volume)\n    counts = torch.zeros_like(volume)\n    \n    with torch.no_grad():\n        for z in range(0, z_size, stride):\n            for y in range(0, y_size, stride):\n                for x in range(0, x_size, stride):\n                    z_end, y_end, x_end = z+pz, y+py, x+px\n                    patch = volume[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)]\n                    pad_z, pad_y, pad_x = max(0, z_end-z_size), max(0, y_end-y_size), max(0, x_end-x_size)\n                    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n                        patch = torch.nn.functional.pad(patch, (0, pad_x, 0, pad_y, 0, pad_z))\n                    out_patch = torch.sigmoid(model(patch))\n                    output[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)] += out_patch[:, :, :pz-pad_z, :py-pad_y, :px-pad_x]\n                    counts[:, :, z:min(z_end, z_size), y:min(y_end, y_size), x:min(x_end, x_size)] += 1\n    output = output / (counts + 1e-8)\n    return output.cpu().numpy()\n\n# Extract motor location from predicted mask\ndef extract_motor_location(mask, threshold):\n    mask = mask.squeeze()\n    if mask.max() < threshold:\n        return -1, -1, -1, 0  # No motor\n    # Find the strongest peak\n    z, y, x = np.unravel_index(np.argmax(mask), mask.shape)\n    # Refine with center of mass for sub-voxel accuracy\n    region = mask[max(0, z-5):z+6, max(0, y-5):y+6, max(0, x-5):x+6]\n    if region.size == 0:\n        return z, y, x, 1\n    z_offset, y_offset, x_offset = center_of_mass(region)\n    z, y, x = z + z_offset - 5, y + y_offset - 5, x + x_offset - 5\n    return z, y, x, 1\n\n# Tune peak detection threshold\ndef tune_threshold(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df, thresholds=np.linspace(0.1, 0.9, 9)):\n    best_model_path = \"/kaggle/working/best_model.pth\"\n    if os.path.exists(best_model_path):\n        model.load_state_dict(torch.load(best_model_path, weights_only=True))\n        print(\"Loaded best_model.pth\", flush=True)\n        logger.info(\"Loaded best_model.pth\")\n    else:\n        print(\"Warning: best_model.pth not found, using current model state\", flush=True)\n        logger.warning(\"best_model.pth not found, using current model state\")\n    model.eval()\n    best_threshold = 0.5\n    best_fbeta = 0.0\n    # Store thresholds and Fβ Scrores \n    thresholds_list = []\n    fbeta_scores = []\n    \n    for threshold in thresholds:\n        predictions = []\n        for tomo_id in tqdm(val_ids, desc=f\"Tuning threshold {threshold:.2f}\"):\n            dataset = TomogramDataset(tomo_id, gcs_preprocessed_path, local_dir, mode=\"val\")  # Fixed typo\n            dataset.load()\n            volume, _ = dataset[0]\n            pred_mask = predict_full_volume(model, volume)\n            z, y, x, has_motor = extract_motor_location(pred_mask, threshold)\n            \n            # Visualize prediction for the first tomogram\n            if len(predictions) == 0 and tomo_id == val_ids[0]:\n                mask_path = os.path.join(local_dir, f\"{tomo_id}_mask.npy\")\n                if not os.path.exists(mask_path):\n                    _, mask_path = download_npy_and_mask(tomo_id, gcs_preprocessed_path, gcs_precomputed_path, \"train\", local_dir)\n                gt_mask = np.load(mask_path)\n                slice_idx = volume.shape[2] // 2\n                tomo_slice = volume[0, 0, slice_idx, :, :].numpy()\n                pred_slice = pred_mask[0, 0, slice_idx, :, :]\n                gt_slice = gt_mask[slice_idx, :, :]\n                gt_row = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n                gt_z, gt_y, gt_x = gt_row[\"Motor axis 0\"], gt_row[\"Motor axis 1\"], gt_row[\"Motor axis 2\"]\n\n                plt.figure(figsize=(15, 5))\n                plt.subplot(1, 3, 1)\n                plt.imshow(tomo_slice, cmap=\"gray\")\n                plt.title(f\"Tomogram Slice (z={slice_idx})\")\n                plt.axis(\"off\")\n                plt.subplot(1, 3, 2)\n                plt.imshow(pred_slice, cmap=\"hot\")\n                if has_motor:\n                    plt.scatter(pred_x, pred_y, c=\"blue\", marker=\"x\", s=100, label=\"Predicted\")\n                if gt_z != -1:\n                    plt.scatter(gt_x, gt_y, c=\"green\", marker=\"o\", s=100, label=\"Ground Truth\")\n                plt.title(\"Predicted Mask with Motor Locations\")\n                plt.legend()\n                plt.axis(\"off\")\n                plt.subplot(1, 3, 3)\n                plt.imshow(gt_slice, cmap=\"hot\")\n                if gt_z != -1:\n                    plt.scatter(gt_x, gt_y, c=\"green\", marker=\"o\", s=100, label=\"Ground Truth\")\n                plt.title(\"Ground Truth Mask\")\n                plt.legend()\n                plt.axis(\"off\")\n                plt.tight_layout()\n                plt.show()\n                os.remove(mask_path)  # Clean up\n                break  # Visualize only one tomogram\n            predictions.append({\"tomo_id\": tomo_id, \"Motor axis 0\": z, \"Motor axis 1\": y, \"Motor axis 2\": x, \"Has motor\": has_motor})\n            dataset.clear()\n        \n        # Create submission DataFrame\n        submission_df = pd.DataFrame(predictions)\n        # Create solution DataFrame\n        solution_data = []\n        for tomo_id in val_ids:\n            tomo_labels = labels_df[labels_df[\"tomo_id\"] == tomo_id].iloc[0]\n            solution_data.append({\n                \"tomo_id\": tomo_id,\n                \"Motor axis 0\": tomo_labels[\"Motor axis 0\"],\n                \"Motor axis 1\": tomo_labels[\"Motor axis 1\"],\n                \"Motor axis 2\": tomo_labels[\"Motor axis 2\"],\n                \"Voxel spacing\": tomo_labels[\"Voxel spacing\"],\n                \"Has motor\": 1 if tomo_labels[\"Number of motors\"] > 0 else 0\n            })\n        solution_df = pd.DataFrame(solution_data)\n        fbeta = score(solution_df, submission_df, min_radius=1000, beta=2) # Compute Fβ-score\n        print(f\"Threshold {threshold:.2f}, Fβ-score: {fbeta:.4f}\", flush=True)\n        logger.info(f\"Threshold {threshold:.2f}, Fβ-score: {fbeta:.4f}\")\n        thresholds_list.append(threshold)  # Append for plotting \n        fbeta_scores.append(fbeta)\n        # WANDB Log Fβ-score for this threshold\n        #wandb.log({\"threshold\": threshold,\"fbeta_score\": fbeta})\n        \n        if fbeta > best_fbeta:\n            best_fbeta = fbeta\n            best_threshold = threshold\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds_list, fbeta_scores, marker=\"o\")\n    plt.xlabel(\"Threshold\")\n    plt.ylabel(\"Fβ-score (β=2)\")\n    plt.title(\"Fβ-score vs. Peak Detection Threshold\")\n    plt.grid(True)\n    plt.show()\n    plt.close()\n    print(f\"Best threshold: {best_threshold:.2f}, Best Fβ-score: {best_fbeta:.4f}\", flush=True)\n    logger.info(f\"Best threshold: {best_threshold:.2f}, Best Fβ-score: {best_fbeta:.4f}\")\n    #wandb.log({\"best_threshold\": best_threshold, \"best_fbeta_score\": best_fbeta})\n    return best_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:53:53.179248Z","iopub.execute_input":"2025-04-23T15:53:53.179791Z","iopub.status.idle":"2025-04-23T15:53:53.204335Z","shell.execute_reply.started":"2025-04-23T15:53:53.179769Z","shell.execute_reply":"2025-04-23T15:53:53.203277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test prediction\ndef predict_test(model, test_ids, gcs_preprocessed_path, local_dir, threshold):\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    predictions = []\n    for tomo_id in tqdm(test_ids, desc=\"Predicting on test set\"):\n        dataset = TomogramDataset(tomo_id, gcs_preprocessed_path, local_dir, mode=\"test\")\n        dataset.load()\n        volume, _ = dataset[0]\n        pred_mask = predict_full_volume(model, volume)\n        z, y, x, has_motor = extract_motor_location(pred_mask, threshold)\n        predictions.append({\"tomo_id\": tomo_id, \"Motor axis 0\": z, \"Motor axis 1\": y, \"Motor axis 2\": x})\n        dataset.clear()\n    return pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:53:56.60014Z","iopub.execute_input":"2025-04-23T15:53:56.600829Z","iopub.status.idle":"2025-04-23T15:53:56.609594Z","shell.execute_reply.started":"2025-04-23T15:53:56.600793Z","shell.execute_reply":"2025-04-23T15:53:56.60889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run pipeline\nprint(\"Starting hyperparameter tuning...\")\nlogger.info(\"Starting hyperparameter tuning...\")\nbest_threshold = tune_threshold(model, val_ids, gcs_preprocessed_path, gcs_precomputed_path, local_dir, labels_df)\nprint(\"Generating test predictions...\")\nlogger.info(\"Generating test predictions...\")\nsubmission_df = predict_test(model, test_ids, gcs_preprocessed_path, local_dir, best_threshold)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created: submission.csv\")\nlogger.info(\"Submission file created: submission.csv\")\n# Finish the wandb run\n#wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:58:07.054805Z","iopub.execute_input":"2025-04-23T15:58:07.055117Z","iopub.status.idle":"2025-04-23T15:59:37.74346Z","shell.execute_reply.started":"2025-04-23T15:58:07.055097Z","shell.execute_reply":"2025-04-23T15:59:37.742022Z"}},"outputs":[],"execution_count":null}]}